Okay, Good morning class. We're going to continue our discussion of vector differential equations. And today's lecture is going to focus on the forced response or the sinusoidal steady-state two vector differential equations. And it's going to lead into what we call AC analysis. Ac stands for alternating current. It's gonna give us a perspective on basically if you're only interested in knowing what the sinusoidal steady-state is, then there's a shortcut way of finding the answer. And that's the goal today is to motivate you of how you get to this shortcut. And then you might be wondering, why do we have this whole lecture on this long way of doing it. The long way of doing it as basically to show you that, to motivate why the shortcut way works. I've had students who've taken this course in the past come to me and say I didn't really understand AC analysis. It just seemed to come out of nowhere. Didn't seem connected to the rest of the class. But hopefully through today's lecture, you're going to realize that it's actually very much connected. It's more or less an extension of what we're learning. So let me remind you of the solution of the vector differential equation that we studied last lecture. That was the, we focused on the homogeneous solution. And then starting from there, we'll go to the force solution and the steady-state solution. And then at the end we'll introduce these new concepts like impedance and AC circuits. And probably the examples we will have to do next time depends on how quickly you go today. Let me just remind you where we were. Last lecture. Remember last lecture, we looked at an example circuit. This is called a low-pass filter. And you can see, I just wanted to come up with something for now. Don't worry about its functionality is just another circuit. But I wanted an example that involve more than a couple inductors and capacitors. So this one has four state variables, right? The voltage across the capacitors, we define two states and the current through the two inductors in the next two states. So we have four states. You might be wondering, is there another way of defining the state variables? And there is, in fact, you can also define the state variables as the charge on the capacitor and the magnetic flux through the inductors. So that's an equivalent way of defining the state. But usually it's more convenient to talk about the voltage across the capacitors and the current through the inductors. And kinda to motivate this concept of state, we imagined that any given time, if we freeze the circuit and redraw the schematic with the state variables represented as independent sources. They're not really independent sources right there. Voltages on capacitors and inductors. So I'll call them pseudo independent sources. And over here you can see that we've mapped the inductor into its state representation, which is the current through the inductor. And we've mapped the capacitor to its state representation, which is the voltage across the capacitor. Doing this allows us to basically determine everything about the circuit in terms of the state variables, right? If we think of the state variables as independent sources, then we can solve this. This is just a DC circuit. We can solve it at any given time, right? If we freeze time, we can find all the voltages and currents. And it's gonna be a linear superposition of the actual sources and the pseudo sources which are our state variables. So knowing the state allows us to completely know everything about the circuit any given time. If we can know how the state of the system evolves through time, right? That led to our vector differential equation. Then we can predict the future state of the circuit from the past states. Okay? Now last lecture we went through and we actually derived with the vector differential equation x dot or me, use the notation d by d t. D by d t of x is equal to AX plus B S. So this is our vector of sources. This is the independent sources. And of course this is our state variable. Okay? Last lecture, we came up with a systematic way of coming up with a, as I mentioned, a lot of times, if it's a simple enough circuit, you can just by inspection, do KVL, KCL and find a. But as the circuit becomes more complicated, now you have also a generalized procedure for finding a which is based on repeated application of Norton's theorem and Thevenin theorem. If you need a little bit of review on that, I typed up some notes. You can look at those. They're on they're posted on Ed. And content staff is going to also translate that into the format that we use in this class. So if you want to get an early peek at it, it's up there on line. So now you should be very comfortable taking any circuit and representing it in this more mathematical form, which is this vector differential equation. In this particular example, I put in some numbers for the inductors and capacitors. And then I just asked what are the eigenvalues of this matrix for these values? So let's have a look. If you look at these eigenvalues, there's a couple of interesting things to note. These eigenvalues are real. Basically we have four distinct eigenvalues. Which means without a doubt, we have four linearly independent eigenvectors, right? If the eigenvalues are not distinct, then we're not sure. But for now, don't worry about that for most cases of interest here you're going to end up with four distinct eigenvalues. It's only kind of like edge cases that involve repeated eigenvalues, in which case you have to go and figure out if there are actually four distinct eigenvectors are linearly independent eigenvectors. So here we have a real eigenvalue, we have another real eigenvalue, and then we have two complex eigenvalues. And in fact, if you look more carefully, you'll see that their real parts are the same and their imaginary parts are the opposite. So they're actually complex conjugate, conjugate. Another interesting thing about these eigenvalues that you'll note is their real part is always coming out negative. You can do lots of examples and you always see that as long as these L's and C's are positive and the R's are positive, then we're going to end up with negative eigenvalues. So you might suspect live, this is what is the physical significance of that? And we'll come back to that question as well. The imaginary parts though it can be plus or minus. Yeah, question. What are the carrots? Yeah, this is just, sorry, yeah, this is Mathematica. Mathematica has really ugly notation, but this just means ten raised to the power of minus nine. Okay? So let's plot some of these solutions. So here I'm just plotting state zero. So this is X1. So this is the voltage across the capacitor. And this is x2, which is the voltage across the second capacitor. So the circuit looks like this. We have a capacitor, C1, C2. And then in this simulation or this calculation, what I did was I set the initial state at zero equal to 1000 transpose. So this is the initial state vector. In other words, what I did was I charge up this capacitor to 1 v. And then I let the circle, the circuit settle, right? Homogeneous response. So there's no force response here. So if we look at the first state, it starts at 1 v. Alright, that's a good sign. The first time I wrote this code it was not a 1 v, so I knew there was an error. So it's a good debugging. So it starts at 1 v and it decays, right? And we can kinda see how there's also a couple of resistors here. Forgot to draw this. We call RL this we're calling R S, the source resistance and the load resistance. So we can kind of imagine that this discharge here can decay through these different paths. In fact, we can say that the charge on capacitor one can either flow into ground through R S, or it can flow into the load through RL, right? Once it flows into the loader, into the source, it's gone. Because those are lost the elements. Or it can flow into capacitor C2 and charge up C2. In doing so, in charging up C2, it has to go through an inductor. And we know that inductors have momentum that's like that waterwheel. So that inductor is also going to store some energy. So the energy of the capacitor is going to get stored and C2 and L1 and L2, but also partially lost through these load resistors. And we can see this is actually happening. If we look at the second state, it starts at zero and it rises up to about half a volt. How's that happening? Well, that's the charge from capacitor one leaving and charging of capacitor to eventually though we see that both capacitor is discharged to ground, right? Both of these, well discharge to ground because there is no DC source. So in steady-state, both capacitors see resistors across them and so they're going to discharge to ground. So physically this checks out. This is what we expect. The interesting part is because of the inductor, the capacitor C1 and C2 not only get charged, are not only positively charged at most of the time, but for a brief duration, we can see the first couple of cycles of their discharge. They actually get charged negatively, right? That's because of the inductor. The inductor is saying, Don't stop the current flow, right? And it'll basically force, it'll not only discharge the capacitor, but it'll start charging it negatively. Charging it negatively, it has to give up some of its own energy. And so its magnetic flux starts to collapse. And that's why I can't keep the current going negative all the time, right, for it can only do it for so long. So by this point, the inductor has given up, its given up all, almost all of its energy. And now the current starts to swing back in the other direction, right? So now it gets to charge positively again. So the charge sloshes back and forth between these capacitors. Because the inductor is there. Without the inductor would just be boring. It would just all go away. Okay? Any questions about this? Now if we go back to the numbers, remember that the solution to the homogeneous equation looks like this. So I'll call this solution q of t. And the eigenspace domain. In the eigenspace domain, the solution is basically Q zero. This is the initial conditions. E to the lambda t. Okay? What do I mean by this? Well, let me actually back up. And say for each state variable, let's say. The kth state variable, right? This could be the voltage on capacitor to, for instance. Then we have some initial condition, e to the Lambda k t. Alright? Remember, the whole point of diagonally diagonalizing the system wants to take this vector differential equation and turn it into a bunch of independent first-order differential equations. And so we've seen that problem 1 million times by now, we know how to solve it. It's just this exponential decay. And the exponential decay factor depends on lambda k. So each state variable is decaying from its initial value with at a rate of lambda k. Lambda k In general, we find is complex, right? So let's write it as sigma k plus j omega k. So we just split it into real imaginary parts, are doing nothing fancy here. So again, let me erase this. We don't really need the vector for now. We're just looking at one particular state. So that one particular state is going to respond as follows. And we've seen this before, right? So if we write this in terms of real and imaginary parts, this is cosine omega k t and the imaginary part is sine omega k t. Remember that we found that for every state k. Well, we haven't really proven this, but what we found, we found earlier was just coincidentally, every time we found a complex lambda k, We found its complex conjugate. I don't have to prove it to you. I can give you a physical argument that they have to come in complex conjugate pairs. Why is that true? Why do the Lambda k has to be occur in complex conjugate pairs? Just appeal to your physical intuition. You could do a mathematical proof. Honestly, I don't know how to do the mathematical proof. I'd have to think about it, but physically I know it's true. I don't even need to bother to go to the math. Why is that? Yeah. Say that louder. Yes. So that tells us that there are complex, right? But why, why did they have to occur in complex conjugate pairs? So what I mean is that when you list the eigenvalues, if you find a complex one, you're going to find its complex conjugate pair. So let's say this is lambda k. Notice that lambda k plus one is lambda k complex conjugate. What I'm claiming is that all the eigenvalues, if they're complex, have to occur in complex conjugate pairs. Yeah. Okay, Let me just stop you there. Solving the quadratic is only true for a second-order system, right? This is an nth order system, right? We know that basically these eigenvalues are the solution to a characteristic polynomial, right? A11 minus lambda one, right? Do the expansion. That determinant is an nth order polynomial, right? So you're going to solve by fundamental theorem of algebra. We know that an nth order polynomial has n solutions, right? But I'm claiming that when you solve those solutions, the solutions are going to be complex conjugate pairs. Now the mathematically inclined can go from there. But I want a physical explanation, Yeah. Okay, energy conservation know, maybe have to think about it. But what I'm trying to get at is mathematically, you can go ask your buddies in the math department, they'll tell you, oh yeah, if you have constant coefficient equations, yeah, the solutions will always come in complex conjugate pairs and you can prove that. But physically, I wanted somebody to just say, Look, I charged up the capacitor to 1 v. And now I'm going to watch it discharge, right? And it's going to oscillate and discharge at the rate of these eigenvalues. But I can always go into the lab and measure that voltage. It's always a real quantity, right? It's not a complex quantity. So that means that if you have an e to the j omega, like this, there's gotta be another, let's call this tilde D, Another, call it j naught k. That is the exact complex conjugate. So it has the real part. So that when I add these two together, their imaginary parts cancel out. Or perhaps, you know, and so I end up with a real decaying exponential, right? So that's why we have complex conjugate pairs. Because we initialize the system with real voltages, not complex voltages. And so if we watch it decay, it should decay all the season. Ours are real numbers. So we end up with a polynomial with real coefficients. Okay? So that's, that's the explanation for why at the end of the day, we get real waveforms, right? These are real waveforms, not complex waveforms. But if we look at any one particular state, like again, this is the decay of one state. We can say that it's real and imaginary parts are doing this decay that we saw earlier. Where the decay rate is determined by this factor and the oscillation frequency is determined by omega k. So the frequency at which it oscillates, we'll call that the natural frequency of this lambda is omega k is the imaginary part and the decay rate is the real part. So what if sigma k, We're a positive number? That is that a physical reasonable solution? I see some nods. Yeah. Yeah. Your responses no, because it blows up to infinity, right? We know exponential with a positive coefficient is going to grow. And why is that not a physically reasonable solution? We can't have we can't have infinite voltages there. Well actually, you know, eventually things will break down, but the solution growing is not necessarily unphysical. You know, you can't, you can't just say, oh, we can't have the solution growing. I might say, well maybe the solution grows for awhile and then some physical breakdown happens, right? But what? Why do you know that it can't grow? Yes. We can't gain energy. There's no power supply exactly. That goes back to what someone was saying earlier. That yes, we know the amplitude of the rate at which the state decays means that if it's decaying, it's losing energy. But if I were to grow, it means that it's gaining energy, right? That doesn't make sense. There's no voltage sources connected here. All the inductors, resistors, everything is real. Real resistors dissipate energy. Positive capacitors can store energy. Positive inductors can store energy. So, yeah, physically we'd expect that all the, the imaginary, excuse me, all the real parts should be real. And indeed we see that in this particular example, all of them are real. So we could basically make a statement that the system is stable if the real part of the lambda k is less than zero. Well, technically we can make an argument about what happens if lambda k is equal to zero. So what happens if lambda k is equal to zero? Yeah. It just keeps oscillating. Yeah, So if there's no loss in the circuit, right? If you had an ideal pendulum or an ideal LC circuit, it would keep oscillating and it would never lose energy. Okay, so now we'd like to talk about the forced response. And let me actually just remind you how we got. Again, this is good reminder. So from the solution, let's call this the homogeneous response. We can form a vector. I'm gonna do this for the homogeneous response because I want to do the same thing for the force response. So what we found was the homogeneous solution. The kth solution is equal to two q at zero, e to the lambda k t. Now, because I have n states, let me collect them all together and formed the q vector. So q of t. Let's call this the initial state vector. And how do I write this? Well, let me use some shorthand notation like that. What does this mean? This is this is E that you know and love. This is Euler's constant, right? This is a matrix. So we're saying take e and raise it to a matrix power. That's silly. It makes no sense. Actually, you can make sense of it, but let's not get there just yet for now. It's just shorthand notation. So what does this notation mean? We're going to introduce the notation that e to the Lambda is equal to element-wise e to the lambda1 t, either lambda2 t, and so on, E to the lambda n t. And we're only going to define this for now for a diagonal matrix. Sorry, there's no t here. So let me just fix that. Either lambda1, lambda2, either lambda n. So basically lambda is a diagonal matrix. And when we raise e to the lambda power, just think of it as a notation. Take, because that'll basically exactly form a vector version of this equation. We want to take this e to the Lambda k t in form of a nice vector version. Because this is a diagonal matrix, all we need is either race though diagonal power. We don't need any off-diagonal elements. So that's okay, we can do that. So think of this as notation. Notice that we're not raising e to the zeroth power because that would be one. So that would put 1s everywhere here. So we're not doing element-wise exponentiation. This is really some kind of new operation that we haven't introduced yet. For now, we're saying that if you take, if we're defining, don't worry about the math for now. We're defining this matrix exponential such that if you give it a diagonal matrix, then you form a new matrix where the diagonals are just e raised to the diagonal power, okay? And everything else is going to remain zero. So you end up with another diagonal matrix. So the output is also a diagonal matrix. That's the key. So with this notation, I can basically write this kinda nice solution in vector form. And now I need to go back to my state-space, right? Because this is in the eigenspace. So in the, in the state-space, x of t is simply q times little q of t. Remember this is the eigenvector matrix. It's a matrix where the row and the columns are the eigenvectors of our matrix a. So remember Q inverse takes us from the state-space to the eigenspace. And therefore, the inverse of Q inverse, which is just Q, brings us back. So let me write this as follows. So let me multiply here. When I say that x of t is q times q of t. And so that's q of q of zero, e to the lambda t. This is all now a nice vector. And of course q zero itself is given to us. Q is Q inverse times X. So this is the, this is the true initial state. And this is our, again transformation matrix that takes us from the state-space into the eigenspace. So if we substitute that here, we get Q inverse x0e to the lambda t. Actually, this doesn't look right. This is not quite right. There should not be canceling out. What did I do wrong here? Yeah, let me hold onto this idea. This definitely should not be canceling out. Let me go back here. The solution looks right. Q is Q inverse times x. Yeah, let me come back to why this is not true. Okay. So the main thing I want to take away from here is this notation. And now we want to deal with the steady-state solution. So let, let's, let's remember what our steady-state solution is. So we have, again, let me just quickly remind you of where we're at. So we have this vector differential equation. But now we want to include the source. So here I have BS, BS is no BS, it's actually our sources. This is a vector of sources. And basically if you think back to our original differential equation, what we're doing is we're writing the evolution of every state as a linear combination of other state variables plus the actual independent sources. So this matrix B tells you, for the ice state, right, than the ith row of this matrix B tells you what linear combination of sources are involved in the evolution of the state. So let's go back to our example to see how that played out in our example. In our example. Notice that the vector, the vector b here is simply 0010 because there's only one voltage source VS, and it's only the third equation where it came into play. So when we looked at the first two equations, so when we looked at the states 1.2, they didn't depend on the source. The evolution of state one depends on the current of inductor one and the current of inductor to not the source. The evolution of the second state depends on the current of the second inductor. And the current that flows into RL, which is actually the voltage across, it's the second state, Vc2 over R. Again, it doesn't depend on the state. On the other hand, when we looked at the third state, which is this current. Now we can see, oh yeah, this current definitely depends on the source, right? Because this current depends on the voltage here. That's why there's a one for the third equation. So that's all we're doing here is we're basically summarizing and vector form. Not only the evolution of the states that in terms of the other state variables and the initial conditions. But now we're actually also including the source values. So how do we solve this equation? Same exact procedure. Let's multiply it by Q inverse. And let's call this, this whole thing. Let's call this B S tilde. In other words, this takes our source voltages, transforms them into the state-space, from the state-space into the eigenspace. So it's basically a linear combinations of the voltage, independent voltage and current sources in our circuit. We can change the order of operation here. We did that last time. And we can basically say that x is q times Q, where Q is the eigenspace. And this, we already wrote it in terms of the eigenspace, right? And this is also q inverse x takes us into the eigenspace. Think I multiply. Let me multiply by Q, not Q inverse. So this a. Let me just multiply, let me expand it. And let me multiply it by Q inverse. Now qn versus correct. And here's another Q inverse, BBS, and we'll call this B S tilde. This we'll call q, which is basically the reason we call it Q. We know that Q inverse is the transformation matrix that takes us from state-space to eigenspace. And so then we get our differential equations just like before. The only difference is that we have an extra term here now on the right-hand side. Right? And this is the voltage sources transformed into the eigenspace. So how do we solve this? Well, every row of this equation is very familiar. That's right. For instance, one of the rows down. So we have this equation, BPSK. You guys have met this equation before, right? We can rewrite this as d q k d T minus Lambda KQ key is BPSK. Let's quickly solve it again. If you don't remember the solution. Remember the idea with the integrating factor was to make that left-hand side look like the derivative of a product and then it expanded into the product rule. So what we did, the trick that we used was we multiplied both sides by e to the Lambda k, e to the minus lambda k t. And then multiplying this out, we have e to the minus lambda k t DQ DT minus lambda k, e to the minus Lambda KT q k. And if you think about it, this looks a lot like the product rule, right? Because this looks like the derivative of this. And this of course is the derivative of this. So this is simply q k of t e to the minus lambda k t derivative is equal to k e to the minus lambda k t. So nothing new here. This is exactly what we did before. And we integrated both sides. And we have a solution here, let's say from some initial time to some time t. And of course we pick up, when we do the integral, we pick up a constant. We'll call that Q, K, K constant by this notation. And it also needs to get multiplied. So when we integrate this and we multiply everything out, let's go to the next page here. So that means that the Q k of t is equal to e to the Lambda k t integral B S k in the eigenspace domain, e to the Lambda k, s. This is a function of s Now, because it's our dummy variable integration plus e to the Lambda k, t k, k. So this is the general solution. We can think of this as the general solution. We can think of this as the solution introduction to zero input. In other words, if PSK where equal to zero, that would be the only solution. And we can think of this as the kind of the force solution for zero initial conditions. So the left-hand side is basically what happens if you initialize the circuit with zero state, zero initial conditions, but you drive it with the source. That's what you get. And the second term is a solution if you turn off all the sources, initialize it and then watch it evolve, right? That's the homogeneous solution that we've been looking at. And in general, of course, we sum them together. That's the total solution. And the sum together has to satisfy the actual initial conditions. Yes. Why are there exponential decay as well? There's exponential decays, they have to be right, because if lambda K had a positive real part, then we get back to the growing exponentials. Right? We need exponential decay, is exponential decay or the physical solutions. For pulse responses, what we find is you also have an exponential decay, right? So for a pulse response, it's gonna do something like this, right? Or maybe it's gonna do something like that. But it's still an exponential decay, right? It's just flipped around. Okay, So what we'd like to do is try to write this using that kind of vector notation that we introduced. So what I'm going to do is write this q of t As again e to the lambda t. This is again that funny notation. And over here let's call this. I want to keep it consistent with my notes here. Let's call this Q k of zero. So this is going to be Q of zero. So this is the initial state solution, changing the order, right? That's the second term. Now for the first term, I'm going to also write e to the lambda t integral B S k. I'm going to write BPSK also as a vector now. And I'm going to write it as Q inverse B S. Technically there's a BS, has a matrix in it, but let's just absorb the matrix and into the vector itself. And then we have another matrix exponential here, e to the minus lambda S ds. So again, it's just notation for now. It just happens to be very clever notation that works out and can be generalized. But let's look at every term to make sure that it makes sense. So let's look at the first row of this. The first row is saying either the lambda one t q zero, or the first element of the vector q, right? Okay, that makes sense, right? Second element also makes sense. It's e to the lambda2 times q2. So that term is just that notation should be pretty familiar. Let's look at the second one. Again, if we look at the first row, this exponential is simply e to the lambda1 t. This is just B1 moved into the eigenspace through Q inverse, right? So it's p1 tilde. And this is also e to the minus lambda one S. So you can see that row by row, these two are identical. And we just have this nice vector notation that makes our life easier. Okay. Let me now any questions. It's pretty clear. Yeah. In the back. Yeah, the capital B I just absorbed for because I just got lazy. So if you want, you can put in a capital B everywhere you see a B. But yeah, you're right. I mean, I'm lazy. So what I'll do is I'll introduce a double tilde notation. Double tilde means that b has been absorbed. And that way I don't have to change anything. Okay. So this is sink. Well, I don't want to put a single till here because that will confuse things. Okay? So let's now, if there are no more questions, let's move this into our state-space. Remember to move it to state-space, all we have to do is multiply by q, the matrix q. So then we have q e lambda t. And this is Q inverse x of zero plus e lambda t q integral q inverse B S E minus lambda S. Yes. So we have this interesting term here. And I'm gonna move this Q inverse out to match. So let me just rewrite this as we change the order of integration with the change and matrix product. So this term becomes q e to the lambda t q inverse. Now, integration of the vector B S lambda S ds. So everything now is in terms of state variables, right? We got rid of all the eigenstates because Q is gone. And so let me now just write this all as one thing. So what we have is x of t is equal to e to the, basically Q, either lambda t Q inverse times x zero plus the integral or before the integral, we also have a queue, either lambda t q inverse integral of bs e to the lambda S ds. I'm very tempted to call this something. Okay? If you think about the non vector differential equation, what would that be? Right? What is the non, What do I mean by that? Well, if you just had dx, d t is equal to a of X plus b. What's the solution? What would this q e lambda t q inverse B. Just be a, right? And be e to the a t would be the solution e to the minus a t, let's say. So why don't we just for fun, this, don't take this too seriously yet. Maybe later on you can come back. Let's call this term e to the a t. Then if we did that, notation wise things look very clean. It's almost too good to be true. It looks like the exact same solution that we found before. Right? If you go back and look at the non vector version of this, this was exactly what we found for a simple ordinary differential equation, right? Like an RC circuit or an RL circuit. This was a solution. Now we're finding that just for like some notation trickery, we can make this vector differential equation solution look the same. So this is a temptation. But it turns out that this temptation is actually correct. That this is how you could define. We can define e to the a t as q, e to the lambda t q inverse. And the reason that we can do this is actually through, basically through a Taylor series expansion. We can say that this is one plus x over one factorial plus x squared over two factorial, and so on. And we could actually put in the matrix in here and generate a squared, a cubed. And we could go and do all this. And that's for now. We're not going to go down that path, but it turns out to be very fruitful path. So this kind of bit of notation trickery is actually true. It works. For now. We're just going to leave things in this form. We're not going to go there. But if you're interested, there's a really cool YouTube video. This is what's his name? Three blue, one brown, that guy. He does really great videos. He has a really nice video that explains this. So I encourage you guys to watch that. Okay, so now what we're going to do, that's a little bit of a detour, but I think it's an interesting detour to take. Let me also quickly talk about the DC solution. I skipped that slide. The DC solution is pretty simple, right? So the reason the DC solution is simple is we have this differential equation. Ken, I'll skip the matrix B for now. It's simple, simple notation. And we're interested in the steady-state. Steady-state means things are no longer changing. We're going to wait a long time. There are sources, but they are DC sources, so they don't change in time. So if the sources are not changing in time and you wait a really long time, what can you say about x dot zero, right? The state variables have to go to a constant value. They don't have to go to zero. In general, they won't go to zero because we have voltage sources and current sources. But they can't change. If we wait a very long time, they're going to settle to a constant value. And then if you want to be convinced of that, if we go and look at this general solution, the answer is right here. The first term varies in time, right? But we argued that the real part of every eigenvalue is negative. So if we wait a very long time, what happens to e to the lambda t? What do you guys think? Should go to zero? Because all the eigenvalues have a negative real part. So I won't belabor this point. We can basically say that this term is actually zero. So for our DC steady-state solution, we have this equation, which we all know how to solve. There's no derivatives involved. We can say that the solution is equal to minus a inverse B S. And to make someone in the back happy that actually there's a little tilt in here. So that's actually pretty easy. In fact, you don't really have to do this. Never compute the DC solution this way because there's a much easier way, right? What's the shortcut way we learned? Yeah, exactly. We redraw the circuit. Again, if we look at this state representation, right? And for instance, this circuit here we know in steady-state, this is constant, potentially even zero. But it doesn't matter as long as it's constant c d v d t is equal to zero. If CDVDT is zero, it has zero current. Zero current. It's an open circuit. In steady-state, all capacitors look like open circuits. Inductors. We have L times DIDT in steady-state, i is a constant, so DIDT is zero. So if v is equal to zero, the inductor looks like a short-circuit. So to find the steady-state solution, just redraw the circuit without the inductors and capacitors. And most of the time it's just trivial. You can just by inspection say, Oh, that's the solution right here is look at it. Or you might have to do a little bit of algebra. But don't go through this general procedure of finding the matrix a inverting it. No, don't do that. Only do that if the circuit is really hard, right? It involves like dozens of elements and there's no way you can just look at it and see what the solution is. Okay? But if your computer, write this is you tell the computer, hey, the DC solution is this. Computers happy. Okay, so that's the DC solution. Now we get to the interesting bit, which is the sinusoidal steady-state solution. So we're going to assume that we excite the circuit with a sinusoidal response. And just to give you a little bit of motivation for why this is important, sinusoids are very fundamental. We use them all over the place. In my world, I do RF circuits, radio-frequency circuits like wireless LAN, your cellular phone, and so on. It turns out that the signal that you transmit from your antenna is a sinusoid. Now you might say, well, sinusoid contains no information, it's just a tone. But in fact, like as an example, let's use your WiFi signal. Let's say you're transmitting at 5 ghz, or 5.4 ghz. 5.4 ghz means that you're sending a tone that's going up and down 5.4 billion times a second. So that tone is being transmitted. And the information that is the ones and zeros right, of the video that you're watching can be something as simple as modulating the amplitude of that. So you send a tone that strong, then you send the tone that's weak, then another strong tone and a weak tone. So that's a 1a010. Or you could flip the phase, or you can change the frequency. So you can do lots of things to impart information on that. But the point is that the rate at which you impart information is much slower than the sinusoid. So you're not sending, or you probably want to send 5 billion bits a second. But you can add 5 ghz and 5 ghz, you can send hundreds of megabits per second. So that means that over the course of a bit period, you're not changing the tone. And the bit period could be hundreds of cycles. So your circuit look like they're being excited with. Basically pure tones for very long durations. By long we mean many, many cycles of the RF signal. So that's a very common example that of course I care about. You might be unmusical person and you might say, well, okay, Basically music is just a bunch of tones, right? Every time you strike a note, that's a particular frequency. And the way you make that tone habits characteristic sound as you add harmonics to that frequency, like overtones. And you shape the waveform, basically your shape, how it grows, how it sustains, how it releases, right? That waveform shape determines the sound if it's a piano string or a guitar string. But essentially for many cycles, it's also just a bunch of tones. And by superposition, we can say a bunch of tones is just one tone. We can analyze a circuit for one tone, and then we can sum up all the tones together later. And of course, by extension, our voices are just musical instruments, right? My voice, I have a vocal string. It's not really a string, but it could oscillate at particular frequencies. We call those frequencies vowels, right? My local system can also generate noise. So if I say, if I want to say, haha, that's mostly noise, I'm just pushing air through my pipes. But if I say right, That's actually a very well-defined frequency. So in essence, speech is just noise plus Tones. Noise actually ends up looking like a bunch of tones being added together. And then in general, Fourier series and Fourier analysis and Fourier transforms let you take any signal waveform and represented as a bunch of different tones or frequencies. So if we understand how certain any given frequency goes through a circuit and comes out the other end. Then we can synthesize basically the response to any signal. So it's a very powerful tool. Okay, so let's look at the complex exponential response. Before I do that though, I want to take a step back because I know some of you got really upset last time when I just started talking about e to the j omega t. Ok. Because it was a shock clay out. Where did this thing come from, right? Most of you are very familiar with sines and cosines, right? You wouldn't be here if you weren't familiar with sines and cosines, you've seen it in high school, you've seen it several times. But the key observation, and hopefully you guys have mostly made this observation by now, is it sines and cosines are merely shadows of the complex exponential, right? By shadows, I mean, of course the real and imaginary part, right? So this is cosine and this is sine. Now, sometimes when you look at the shadow of something, it looks really complicated. Until you see the actual thing itself, you'd say, Oh, it's very simple. Imagine like the three-dimensional object. Projecting the shadow might not really be able to tell what the three-dimensional object is, right? It might be like some distorted spheroid ellipsoid, right? But then when you see the three-dimensional objects at all, it's just a sphere and the light was coming in at a different angle. Well, sine and cosine are like that too. If you think about at sine and cosine are kinda complicated, right there. You don't know what actually I want to calculate what value it's taking on any given point. You have to do some calculations, right? You know, some special angles. I know 30 degrees, 45 degrees is root 2/260 degrees half, you know those. But in general, it's a complicated waveform. But now if you look at the actual parent waveform, the parent waveform is very simple. It's just a circle, right? So don't be intimidated by the complex exponential. First of all, it has the word complex in it. So that's scary. No, it should really be called the simple exponential, right? Because It's just a circle. It's not doing anything ugly, right? It's just going if you haven't imaginary argument. Yes, just going in circles. If you make the argument and real, let's say we start here. And we have a real part. It can spiral into the origin, right? Because the magnitude is getting smaller and smaller and smaller. So this is e to the Sigma, let's say minus sigma t, e to the j Omega t. So it's still spinning around the circle at the same rate. But its amplitude is getting smaller because of that decay factor. So that corresponds to spiraling towards the origin. Of course, if, if it were, if it weren't negative, than it would spiral out of the origin, right? It would blow up. So complex exponentials really they should be your new best friends because they really help you out. They make problems much easier. So let's find the response again, q k of t for a complex exponential. So we have either lambda k, t, Q of zero for kth row. And here we're going to put in. My complex exponential, I'll call this. I'm going to abuse notation again. This is BPSK, is Q inverse b s. And we're going to now say that let's say BS is just a bunch of complex exponentials. Again. Now I've introduced this hat notation. I'll explain this in a second. What is this hat? Well, the reason why I need this hat is that in the real space, I've got voltage sources. Let's just make it simple. Let's say I just have one source, either a current source or voltage source. Now that one source, I'm writing the solution in the eigenspace gets multiplied by q inverse matrix. So now in the eigenspace, that voltage source looks like a linear combination of voltage sources, right? So in general, if I have more than just one voltage source, I wouldn't need to do this. I would just say, Okay, it's bs e to the j Omega t. But if I have multiple voltage sources, then I have to acknowledge that I'm taking Q inverse times a bunch of voltage sources and summing them together. So what does that do? Let's look at that over here. Let's say that you have B1 e to the j omega t. And we'll give it a phase, B to E to the J omega T and a phase and so on. The key is that they're all at the same frequency. So all my sources have to be at the same frequency. If there are different frequencies, I'll use superposition. And later on, I'll basically solve the problem separately. For now, let's say we just collect all our sources that are at some frequency omega. Their phases could be different. That's why here we have a phase term. And their amplitudes could be different, right? So the in the state representation, what we're saying is that you're gonna get a linear combination of these sources, right? And the linear combination of resources means we need to sum up all these exponentials. Well, that sounds difficult if you think about summing sines and cosines. But because we have this new friend, the complex exponential, we're going to ask for help. So the first thing a complex exponential does is it says, Aha, if you have sums, you can turn them into products, right? And now we can basically pull out e to the j omega t and have B1, B1 plus B2 e to the j F12, and so on. Then we just say Look, everything in the parentheses is just another complex number. Let's call this a hat or p-hat, right? Nothing special about that. It's just a sum of a bunch of complex numbers. And so this tells us that when you sum up all these complex numbers, you get another complex number. You might remember the animation I showed you several lectures ago where we solve this graphically. We saw that if you have a number, I give a vector going around the circle. And then you add another vector on top of a different phase. It changes the phase of the complex exponential, but it still goes around the circle at the same rate. So this is again, another complex exponential. And this has a magnitude and a phase. So I'll write the magnitude like that, and I'll write the face like that. In other words, now you're not going around the unit circle, but you're going around a circle with radius b hat S K, right? Absolute value. And you're not starting on the x-axis like e to the j omega t does. You're starting off the axis by the phase of this constant. So adding a bunch of exponentials together, it can change the phase, it can change the magnitude, but it doesn't change the actual time dependence, which is the e to the j omega t. That's the key idea. Any questions? This is a really important idea, is very simple, but it's going to basically permeate throughout the rest of our lecture. Yeah. Yeah, so how did I do this part? This part? Yes. So any complex remember any complex number again, this is a good you guys should review your complex numbers. I can take any complex number here that has a real and imaginary part, right? Call this x, y. Z is equal to x plus j y. I can imagine that this number is actually some are e to the j theta, where r is x squared plus y squared square root. And theta is arc tangent of y over x. So this is just converting this, summing all these together, right? You can use, I didn't do the long hand write each term here, is real and imaginary, right? You can use Euler's identity and say this is B1 cosine, or let me do that. So this is B1 cosine phi one plus B1 j sine phi B1 plus B2 cosine Fe2 plus j B2 sine phi2, right? And so on. So collect all the real and imaginary parts together. Call that x plus j y. X is the sum of all the real parts. Why is the sum of all the imaginary parts? And then write this as r e to the j theta. Okay? Yeah, this is the angle of, this is that Theta term. So this is like if I do angle of that, this means it's the, lets, let me just use z as, as a consistency. So I'm saying the angle of z, That's what this notation is, is simply arc tangent, imaginary part of z over real part of z. This is because this is theta, right? There's the polar notation. Yeah, I think some of you for sure need to review your complex analysis and it'll pay off because you're gonna use it a lot in other courses. Yeah. No, it's not non-linear. So I think if I got your question right, is what if you have multiple omega, right? Yeah. So each omega, you can analyze the problem for frequency one, write omega one, and you get a solution. Then you analyze a problem for omega2, you get another solution. You can add them together by superposition. Okay, so let me now go ahead and do the deed, which is to plug in. By the way, for our example system, if you go look at the notes that I gave you using the equations that we just derived. We can basically look at the force response where we hook up a voltage source. So let me just draw the picture for, to remind you guys. So now I have a capacitor here just like before. Except now I have a voltage source V. This V is 1 v. And to Drew, to draw these, I set x to zero equal to the zero vector. To all capacitors are discharged. All inductors are carrying zero current. And then I excite the circuit with the source, which is 1 v. And you can see that the force solution actually does something quite interesting. The first state, this is the first capacitor, C1. It just basically it starts at zero, it gets charged up to 1 v. That's the steady-state solution, right? Again, we can double-check that the steady-state solution makes sense. Because in steady-state, we have the L should be short-circuits. And the capacitor is go to open circuits. So here we can see that in steady-state this voltage here, both voltages should go to the same value. They should go to voltage divider not equal to v S. So actually this is a question mark. This should be going to RL over RL plus RS. So something is wrong. I'll have to go look at my code. But assuming we not worry about that minor detail for now. The interesting thing again here is that the force solution for the second one. Oh, now I remember why. Okay, Yeah, So actually you can see that state two starts at 1 v. So when I ran this, just to make things interesting, this is the initial state. The initial state is that the second capacitor is actually charged. And initially it starts to discharge through these RL and RS is. But then it gets charged backup because of our voltage source. So this is just an application of what we learned. Again, I think there's a typo somewhere in here. I'll have to come back. Let me now just finished the sinusoidal response because it's important. Let's plug in. Let's just look at the forced response. So why am I looking at the force responsible in steady-state? Well, actually, and so forth. Let me just say steady-state response. In steady-state, this is equal to zero. So we're just left with the first term. So we have e to the Lambda k t. And we did this integral before. You guys may remember that we just plug-in now, Q inverse b hat e to the j omega t ds. Now the matrix part just comes out. We can change the order. And this is our vector here. And now we have e to the j Omega minus lambda k t DS. And if we do this integral, we have e to the Lambda k t q inverse B S E to the J Omega minus lambda k T divided by j Omega minus lambda k t evaluated from minus infinity to this as S naught. So when I put this in the integral, it turns into S. That's the dummy variable of integration. Now I need to evaluate this over these two limits. The first limit is easy. It just gives us e to the j omega. Yeah, This is, there's something wrong here. Okay? I think it's correct now. So when I plug in t, Obviously this just turns into S, turns into t. That's, that's easy. There's no t here. On the other hand, when I put in minus infinity, we need to be a little bit careful when we put in minus infinity. Well, notice that the first term is just the imaginary part that's going, that's oscillating. The second part is e to the minus lambda k times t. So it turns into, if I look at the limiting case, this is e to the j Omega minus lambda k T over J Omega minus lambda k minus now e to the j omega t, e to the plus lambda k t. Same denominator, where t is approaching infinity. So if I approach infinity, you can see that the first term, this term here, it just oscillates. The second term because it turned into a plus sign. As long as the real part of that has a negative part will decay to zero. So again, the stability criteria that we defined earlier is very important. We can say that this term is zero. By stability. Real part of lambda k has to be less than zero. If that's true for all of our eigenvalues, then we just end up with that first term. And this is just this part, right? Factoring this out. So let me write that steady-state solution. Probably the most important equation that we've derived. Just put it on a blank page here. We basically have found that the steady-state solution is equal to. So in steady-state be k hat j Omega minus lambda k e to the j omega t. And of course x k. We can, we can collect all the states together and say that the state is just going to be, let me not do that actually because then the notation gets a little bit ugly. So then I can say that X k is just going to be q times, well, x is q times q. Which just implies it's a linear combination of these various queues. And the most important point now is to say that any given x k of t is just going to be the sum over all eigenvalues. Some constant h k times e to the j omega t. So how did I get to here? Well, notice that this is just a complex number, right? The numerator P-hat was a complex number. The denominator j Omega minus lambda is just another complex number. It's a ratio of complex numbers. So I can write this as maybe GK e to the j omega t. So this is just some complex number. And we know that that's just one eigenstate, right? The state solution, the state-space solution is a linear combination of the eigenstate solutions. But each one is a complex exponential and some complex coefficient. So when I sum all of these things together, I'm just going to get some number h. So this is the key idea. I'll review it again next lecture. But just to take away right now, is that what we find that in sinusoidal steady-state, all state variables become complex exponentials multiplied by a constant. And that constant is just a complex number. It has magnitude it has faced. That's the key idea. In sinusoidal steady-state, no matter what. 