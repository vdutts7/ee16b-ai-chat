Can people hear me? Okay, So welcome to the last class before spring break. So yeah, make sure that you rest up. Because you have energy for the homestretch. Know, about a month ago. Okay. So let's get started. Last time we did Gram-Schmidt orthonormalization of linearly independent vectors. Today, the goal is to do a recap. I always do recaps. But we are going to now generalize the Gram-Schmidt procedure. It's not really a generalization. But we're going to deal with the, when the vectors are not independent. If you have a dependent vectors, what to do. And we're gonna revisit BIBO stability that we have seen earlier. And in light of asking the question, what happens if our system matrix is not diagonalizable? We consider the case when matrix was diagonalizable, but what if it isn't? Okay? So that's the agenda. It looks like we're back to the old attendance tissue. Right. So I guess you guys don't want the bonus points it seems you do, but you have to entice your, your buddies to show up. Otherwise, you won't get it. Last time. It was a bit better. I thought we're going in the right direction, but I guess people said, yes, so far off the threshold might as well stay home, maybe be lower the threshold a tiny bit. We'll see. But is it because people have already in Spring Break mod anything is possible, right? Like I was saying, you know, every day There's an excuse for not showing up. Okay. Anyway, you guys are the good ones. So let's. Let's carry on. So I'm going to go through the recap of the orthogonal rotation briskly because we did it last time. But then we will slow down and do the the new material. We'll write it. In fact, I'm very tempted to use the board. I'm not sure whether it's all pretty. I can't write like this colors and for one I don't have the colors, but we'll see. Okay, recap. Orthogonal basis and Gram-Schmidt. We know that. Let me take my pencil out. Okay? Whoops. Okay, So we saw that if you are column vectors of your matrix are called orthonormal. If their dot product is zero when they are distinct and dot product with itself is one. That means this unit length. An example is shown here. That tool in our QI is the blue one, q, j is the green one they make. They are perpendicular to each other, so their dot product is zero and their unit length. And the key idea is that they satisfy the following property. That if you write Q transpose, so you write the rules of q if q1qt Q k transpose, and you multiply it by Q on the right. So Q, Q transpose times Q is going to be this matrix, which by the definition of orthonormality, all the off-diagonal terms are going to be zeros because your dot producting with the other guys and on the diagonal terms your dot producting with yourself. So it's gonna be one. So it's just identity. Identity is a k by k for when Q has k columns. Now if Q is square, then you have a full matrix, a square matrix. Then because Q transpose Q is I and Q has full rank Q transpose Q inverse. So finding the inverse as a really easy for orthonormal matrices, right? That was the lesson we learned last time. Then we went through the Gram-Schmidt orthogonal algorithm. The procedure, you will be given n vectors which are for the moment linearly independent as one through n. This is all stuff we did last time. We want to convert it to a set of q i's, which are orthogonal to one another and of unit length. That's exactly what is shown here. And we want to respect the fact that again, the motivation for this, we did with these drones example just as a, as a, as a setting is that these vectors come flying in one-by-one, S1, S2, S3. And as soon as they come in, you normalize the orthonormalize them with, with respect to u, make them unit length and orthogonal to all the other vectors that came in earlier. Okay, so that's the procedure. So span of this one is span of q, one, span of S1, S2, span of Q1, Q2, and S1 through S N as well. So you want to make sure you respect the span of the vectors that came in. Now, if you consider the linearly independent set, here's the Gram-Schmidt algorithm. First, s1 comes in, and that's the simplest one. You just normalize it by its length and you generate one. That's the first vector. And indeed you check that span of one is the same as the span of S1, which is obviously true. And you put it in the box. So in my box I'm going to accumulate all the orthonormal vectors that the Gram-Schmidt producers incomes S2. And we have to check which part of S2 has already been captured by one, the one direction. I don't need that one. That's exactly shown in the picture here. Here comes the S2, which is making an angle with S1. So this is Q1 normalized version. So you project S2 and S1 direction or the one direction. And that gives me beta Q1. And that part I don't want, because I've already seen it before. What's the new stuff that came in E2? The orthogonal complement of the projection onto the previous set. And that's exactly E2, which is S2 minus beta one. And beta by our projection formula, is just the inner product of S2, which Q1 divided by Q1 squared norm times Q1. Now, if Q1 is already in unit norm than the denominator vanishes, you don't need it. So you get S2 inner product Q1 and Q2, Q1, this is the Beta Q1, and this is E2. But I need to be normalized, so a normalize it before I leave. So E2 may not be unit norm if it isn't, do it. To clear. Check and you can check that indeed Q1Q2 inner product of that. And you can check that indeed it's orthonormal. But I'm not gonna go through, we thought through the geometry, but you can also do it with algebra, okay? Okay, Now comes the next step, S1, S2, S3. And what do you do? Well, now you have to go to 3D since there are three vectors. And here's the picture. So again, before we do that, I, I put Q1 and Q2 into my box saying I'm done with two of them. Now comes the third one. Here's S3. So you should visualize this as being on the plane, off the floor or the bot, say on the plane of this. And S3 is the 3D vector sticking out. And what is the new component that S3 brings to the table? It's the part that has not been seen by S1 and S2 are Q1 and Q2, right? Because they span the same space. So Q1 and Q2 are spanning the plane of this floor. So S3 come sticking out. So project S3 onto the floor. And I don't want that one because that's already been taken care of. The new part is just black line here, is this arrow here. Because that's the part that the z-direction. So if you imagine that S1 and S2 are on the x, y plane, and S3 is an arbitrary 3D vector. It has to be on the 3D because it's independent, can't be only on the floor. So you projected onto the floor, strip that away. All that's left if the Z dimension, right, that's exactly what we're showing here. So on how do you find what to strip out? You project S2 onto Q1 and Q2 and remove them because Q1 and Q2 orthogonal, you just project S3 onto one, has three onto Q2. Remove their contributions. Because projecting onto the x-y plane, it's like projecting onto the plane and y plane and adding them up because x and y are orthogonal, right? It's just like if you have a 3D coordinates. In 3D coordinates, if you have x, y, z zero as your point and you want to find the projection onto the x y plane. What do you do that strip the zero and the y zero. What's left is 00zz z zero is gonna be the complement to the plane. So that's okay. So then that generated E3. But you need to normalize it before you leave. That's exactly what you do here. And the span of this check is the same as the span of that. And you're good to go. You can check that indeed you are orthogonal to the previous two vectors, and you did that by construction. But you can also check it algebraically. Let you do that. So that's all Gram-Schmidt this. And you can keep doing this for three, but you can imagine doing it for any number like thousand, 10,000, million doesn't matter. But the key is that now you have an orthonormal basis. So let's write that down. This is Hou N is orthonormal abbreviation. So it's an orthonormal basis for the column space of a. And we saw that having orthonormality is useful for doing things like least-squares and other stuff, right? It's always good to be orthonormal if you can, if you can do it, if it's not too painful. And by the way, you don't need to know this. But these orthogonality of these vectors we have studying it for real space is R1, R2, R3, and so on. But you can also do it for so-called abstract spaces. You study this later if you go on to take other classes. So you can do it for abstract spaces. They're things like Kalman filters will become very easy to understand. It's exactly Kalman filtering is actually Gram-Schmidt. So think of your mind. Remember this when you learn Kalman filters later down the line, okay? Alright. So this is Q1, Q2, Q3. And lets me show you that. I'm I'm hoping this is going to work. There's no audio. So here's a animation of the Gram-Schmidt. So V1 is first normalized to get Q1. Then V2 comes in. That's another vector, 310. Take the projection of v2 onto Q1 and strip it from V2 to get z2. They call it zeta, we call it E, doesn't matter. And then you normalize it to get Q2. Then you get v3, which has some other vector, 01 minus one. And this example, take the projection of v3 onto Q3. And then onto Q2 and subtract out the projections, strip it. And here's your orthonormal basis in 3D, normalized z3 to get Q3 and you're done. So that's a 3D example. Okay. So this was from spring 22, 60, 16 be I don't know if your discussion section showed this or not. Might have, right? At at least one discussion section may have shown it. I thought this would be a nice visual tool to understand, but in case you want to get it. Okay? What happens if I can't, let me start writing now? Maybe we'll go through a little more and then I start writing, okay, what if S1, S2, and the set of vectors you're given is not independent. E.g. S1, S2, S3. Let's take n equal to three and suppose that the second vector is just two times the first vector s1. So what is my orthonormal set I'm going to produce as the Gram-Schmidt output. Step one is business as usual. The first vector, you normalize it, and that's step one. Step two, you are supposed to do what? You're going to strip the projection of F onto in the x1 direction, remove it and deal with whatever is leftover. That's the new dimension, right? That's a cartographer complement of the stuff that you already saw. Well, when you do that under normal conditions, has two would make an angle. S2 would make an angle with S1 or Q1. This angle, you would remove this contribution and be left with e to right. That's what you would have done. But what happens here? Well, the picture looks like this. S2 is aligned with Q1. It's just twice Q1 or Q2 twice S1. But doesn't matter if some, some scaled version of one. What is the projection of S2 on the one direction? Still? You've got all of it. So what is leftover? Nothing. That's exactly what we are verifying in this equation here. E2, which is the difference between S2 and the projection onto one. S2. Of course, we know the projection formula. Inner product S2 with q one times q one when q one is unit length. So this is just, this has two inner product of one is just magnitude of S2. Because Q1 is unit length s Q2, Q1 is just as two because it's the same thing. In this case it was two and S two minus two is zero. I mean, that's just algebraically verifying this obvious fact that if you are aligned with the previous vector, you have nothing new to contribute. Useless. What should we do? Nothing. You should do nothing. Meaning you ignore the presence of S2 because he is just redundant, is not bringing in anything that you can use to build your setup. Is that clear? Okay. So basically you ignore S2. Don't add a Q2 corresponding to as too, because q2 is the same as one. Okay? So what is the Gram-Schmidt for building a basis, an orthonormal basis I should add. So what should we do? So suppose we have a bunch of vectors coming in and you're going to see why this is needed in a few minutes as the lecture goes on. So you have a bunch of vectors, X1 through as they're coming in. And sorry, s1 through S k and k is less than n. And you want to build out an orthonormal basis for R n. Meaning you want vectors that are perpendicular to each other and unit length. If I have k of them and k is less than n, then I don't have a basis. Why? Yeah. Yeah, exactly. You cannot span all of our n. If you only have k vectors, right? At best you can expand our k and k is less than n, so you're not going to completely cover our n as needed to span R n, right? So that's, that's not possible. But suppose I tell you that I have these vectors here, please complete the orthonormal basis for R n. How do you do this? And we'll see we need to do this when we are talking about upper triangular realisation later, okay? So remember that this is just a tool we need to learn and it's very, very straightforward because what do you do? What do you think you should do? So I gave you, Let's say I gave you n is 100, and I gave you K is 30. Okay? So I have three vectors. They may or may not be independent, right? You know, at least one of them is independent because it's not all zeros. So you have at least one component that you can deal with. And I don't know anything about the other 29 because I have 30 of them. But I want you to complete it, to make it 100. I want an orthonormal basis for our hundred, but starting at S1. So S1 has to be the anchor. Okay. The order matters. We stopped. The order matters. So how do you how do you think we should proceed? Yeah. Exactly. Now. Very yeah. Exactly. It is what he says, which is that we want to add. So because I don't know how many orthonormal basis vectors or how many independent basis vectors are in the, in the example I said that collection of 30 of them, I'm going to add an extra hundred, okay? Because I want a basis for 100. I'm going to add an extra hundred known independent vectors. What is unknown independent vectors in R 100? The simplest one you can think of the standard basis, right? The standard basis, we know not only independent but Stephen orthogonal, but we don't care about that. We only care that it's independent. I just need to have enough independent vectors in my collection so that after Gram-Schmidt, I will be left with exactly 100. But starting at as well. That clear. So that's what we're gonna do. Yeah, yeah, exactly on the right track. So can we create an orthonormal basis? That's exactly what we just discussed. Can we create an orthonormal basis for s1 through SKA? So this is exactly what we just talked through a minute ago. We add to my set from the back, okay. Always, because I want to respect the order of S1 through SK, I want to build out and complement, connect or complete the basis set with the k vectors that have already been given to me. So if k is three and they're all independent, then they're all bringing in new information. And I will get Q1 through Q k, which are going to be legit. Now I want to complete the basis and go from k plus one onwards. I'm going to add E1 through EN. Of course, I know that I'm not going to have all independent here. Because how many, how many? What is the collection of a set size n plus k? You know, what is the maximum number we will have is n. So clearly you have k redundant here. But I'm going to respect the order of the one through k and then I'm going to add one-by-one. Okay? So how do you do it exactly using the Gram-Schmidt? Anytime a vector comes in and it has already been seen before, because you already covered it. Chuck it I don't need it. Get the next one, and then keep going until you complete your set. The clear. Okay. That's exactly what we do here. As he suggested, E1, you start with y1 with a one in the first direct coordinate. Ei is one and the height coordinate, and you start building it out. So this is just a Venn diagram, if you will. So this is the, this is the basis, sorry, these are the k independent vectors that were given to me as s1 through SK. I want to complete n orthonormal vectors in this space. And then I keep building them out. So basically, you can imagine that E1 comes in, E2 and E3 comes in and so on. And I keep checking if these independent vectors have already been seen before or not. If it is, you're done. Otherwise you keep going. Okay. So you do Gram-Schmidt in this order. You first start with S1. We've discussed all this maximum number is n, So Q1 to Q2, and it's guaranteed that all other vectors are linear combinations. Exactly what we discussed. But it's important that the first vector is a scalar multiple of S one, because we want to start with that vector in mind. Okay, Here's an example. Let's say S1, S2, and S2 is 40, right? Then first thing you do is you normalize the first vector. So one is S1 divided by the norm of S1, which is equal to a half. Because S1 has norm two times zero, which is just 10. So it's just the 10 vector. One is the same as in this case y1, but we didn't know that. I made the example, so it's easy to follow. I don't want to do math. So we're going to call R2 the, the projection of the complex, the orthogonal complement of S Q2 to Q1, of the orthogonal complement of S to the direction orthogonal to one, which is what we have done earlier. So that's two minus S2 comma q1q1, right? So what is this has two is 40 minus 40 comma 10 inner product, the following the maths. And this is obvious, right? You agree you're going to get zero because what is the vector for zero? What's the projection of the vector 40 in the direction that is orthogonal to the 10 direction. Nothing because four zeros aligned with one-zero with a scale factor of four. So that's exactly why R2 is going to give me nothing. That means our algorithm says, jacket, he brings nothing new. So let's add our basis set. In this case it's two-dimensions, so it's 10.01. So incomes 10110 had nothing. You already been covered. So this is a case where E1 is also a garbage. He doesn't bring anything. So you got to e2. E2 is not only are independent but even orthogonal. So E2 minus e to 1101 box has done Q1Q2. A very simple example, but it shows you how you by systematically bringing in these elementary basis or standard basis vectors, you are guaranteed to have an orthonormal set when you leave. Okay? Are we good? Alright, now comes the painful part where I have to tell the TAs how many people showed up. You have to arrange your fellow peers and buddies to show up if you want extra credit. But I'm a little disappointed. Today is even less than Tuesday. Tuesday was a rainy day and people came, I guess they were expecting everybody to show up. That's kind of, I don't know what any suggestions on how to motivate other than giving only you guys extra credit, that's not allowed. That that would be what did that The last about a one-fourth, I think last time, I don't, I'm not sure 25% is something we should strive for. But that's kind of been, I guess 50 per cent was maybe wishful thinking somewhere 25-50. Do you think that would help? I think people gave up. They weren't people. Okay. So what if I made it 33? Anyway? I don't want to treat you guys like kindergarten, right? This is like giving you lollipops for showing up at my door. Alright, so let's go back and now we just start writing. I don't know, I'm gonna consider using the board from next time that will that will also deter people from because you can I don't know. I think the board shows up pretty well in the in the recording too. Unfortunately. I think we should have no recordings. That that's the best policy. I mean, I may entertain that, but my TA's might kill me. So alright, so let's, okay, so we're gonna start writing now. Let's revisit BIBO stability. By the way, if I'm going too slowly or too quickly, flag me. Okay. I think there was some ad posts that I was going too fast, but I was just reviewing last time. So for reviews, I'm gonna go first. I'm not going to, because you didn't, if you miss the previous lectures, That's not on me. You have to, you have to show up like leaves, they have to keep up. So BIBO stability is if you had x of k plus one equals X of k plus u of k. General. These are all vectors. This is the most painful part of 16 be putting those vectors. Because apparently that's a tradition when you go to higher classes, nobody uses vectors. But we want to hold your hands in this class so that you realize that there is a vector. So this is diagonalizable. If what? When, when is this matrix diagonalizable or not on a full rank? That just means it's all full rank matrices. We got married, I guess you're right, but more explicitly, we, we've looked at this. When will a be diagonalizable? Yes, linearly independent eigenvectors, not quite full rank, but indeed, the eigenvector should all be linearly independent, right? That one is diagonalizable. And whenever stable, yeah, yeah, all the eigenvalues have magnitude less than one, exactly. If all evils of a magnitude less than one. So when we introduced this instance, I assume for the moment that a was diagonalizable. That's how we started our treatment of the whole subject. I'm sure you would have been. In fact, I'm surprised. Weren't any curious customers asking what if it is not diagonalizable? What if you don't have against you or you just do what you need to know. You don't go beyond what you need to know, but indeed, from a scholarly standpoint, you don't want to know. Hey, okay, you told me when this is diagonalizable, what happens when it isn't? What gifts, what are you gonna do? Because that's what we're going to visit now. So in fact, it's not a knock on you. Okay. I would probably be in the same boat. This is hard enough. I don't want to comfort her. Special cases which are very hard. Question. What if a is not? So let's do an example of exactly such a case. So here's an example. So here's my a matrix. It's got lambda 10 lambda as the two-by-two entries. What are the eigenvalues of a plus or minus h4s plus the diagonal entries for upper triangular matrices. So I see that you can take the determinant of a lambda minus i and you'll get exactly lambda. Lambda, right? Has two eigenvalues, but they are repeats. What are the eigenvectors? Are not independent. Because you have, you have a degenerate case here. What is the eigenvector? Well, you guys know how to, given an item value in order to find the eigenvector, right, from your linear algebra background. So AB equals lambda v. So if you plug this in, you have lambda 10 lambda times V1, V2. The other components of v is equal to lambda times V1, V2, right? And you solve, you will get lambda v1 plus v2 equals lambda v2, and then lambda v2 equals lambda v2. Well, that second equation is really informative. So you can see that you have only one degree of freedom, right? V2 can be anything. Rather V2 has to be zero in this case. So you can solve for this. You can, from here you see that lambda v1 knew I made a mistake here. First equation as lambda v1 plus v2 is lambda v1, which means v2 has to be zero. And v one can be anything. So that means the eigenvectors are aligned in say, some alpha times 10. The always the 10 direction, both of them. So you have two-by-two matrix that only one eigenvector. That means the, it's, it's kind of the degenerate setting, which is the case we are interested in now. So you see here that you have, you don't have two independent eigenvectors. So what should we do with regard to understanding BIBO stability and so on? And in general, what do you do? What, what do we do with matrices that are not diagonalizable? Because when they are diagonalizable, you have this beautiful eigenbasis. You decouple all your vector equations and two scalars and solve for them one by one, right? Turns out, what is the next best thing to having a diagonalizable matrix? Nvt, next best thing. Somebody had a company called next best thing to do. I don't know whether they were successful. You are triangular. Try Yang. The next best thing to having a diagonalizable matrix is an upper triangular matrix. What is an upper triangular matrix? It's what the name suggests. So if this is my matrix, then this part is non-zero stuff. We'll call it stuff. And this is all zeros. You guys have dealt with upper triangular matrices, right? Gaussian elimination, I don't remember what. But generally on the diagonal and to the right is all non-zero stuff. And below the diagonal, it's all zeros. It's called upper triangular. Okay, so that's the meaning of upper triangular. Okay? So we see that if you have a system like dx d t lambda 110 lambda two times x of t. Then we are in, let's say that we want to test for BIBO stability of a system whose a matrix is given by this, which is clearly not diagonalizable if lambda one equals lambda two, right, as a general case. But let's say that this is a general upper triangular system of matrices, square matrix, which is upper triangular. Then there is a fairly straightforward way of showing whether the eigenvalue test that we did applies here too. Why? Anyone? What do you see about the upper triangular system? Yeah, exactly. So he said, You can do the last equation first because that one has, has no interference from the other guys, right? It's all zeros to the left of lambda2. In general, even in an upper triangular system, even if this was n by n, the last row is gonna be all zeros. And the only non-zero entry is gonna be on the diagonal, right? So that guy is completely decoupled from the rest. That for upper triangular does. So you can ask, so dx2 d t equals lambda two times x2 of t, then you can ask, if you are only come to a minute, you're only stable if what? Lambda two has to be less than one in magnitude. Now you know that. Now you can go to the first equation and plugging the fact that if you are, if lambda two is less than one, then I'll have Lambda dx1 d t equals lambda one x one of t plus x2 of t. Now, x2 of t is just some general input now, which we know in the Bible system the input has to be bounded. If the input is bounded, you test whether the output is bounded, right? That's what BIBO stability means. So if the input is bounded, which it will be if x2 is table. X2 is not blowing up, then clearly X2 is within bounds. And therefore, you can just check whether lambda1 is less than one in magnitude. And if it is, the whole system is stable. So even though I was not diagonalizable, I was able to work my way through. And in general, in an n by n system, you start with the last guy and then you plug into the previous guy. So you can see here that the only way you will be stable is if all the lambdas are going to be less than one, exactly like before. That's what he had to check earlier too. Okay. Yeah. Question. Yeah. You can do similar stuff. Yeah. There you would start from the top rather than the bottom. Upper triangular is usually the convention is you could, either triangular would, would work. But it turns out that now comes the main result. So we will visit this more in a little later. But the key is that we can, I'm just going to you first solve for lambda two and then solve for lambda one k. So we'll just write in courts will visit this later after we visit the upper triangular treatment. So you can do dx2, d t equals lambda two x2 of t. And you check this. Then you will have dx1dt equals lambda one, x1 of t plus x2 of t. And then because x2 is because X2 you can certify has been stable. You can add this as a general input. And then the Bible condition, you will see that only need to check is if lambda one is less than one, it's just a high-level overview of what's coming, just a preview. So bottom line question, which is the main result for today, is question is how can we convert a matrix? General matrix? I should say a square matrix. Just to be precise. Not any ordinary tree into UT is upper triangular. I'm going to keep, we're going to keep using upper triangular. I could have used you with a Delta for triangle, but we won't be cute. Let's call it GOT, this means upper triangle form. Using our favorite trick, change of basis. One thing 16 B will teach you it is all about change of basis. So how many times did we see change of basis? We're feeling at least twice. First we saw it. For diagonalizing, you need to have a change of basis into what, which basis from elementary to eigenbasis. And then we saw it last time. In fact, it's in your homework. What other basis changed it? Have you seen yeah. Yes. Ccf, right. We want to put the basis into CCF form. You guys haven't done your homework. If you did, you would know. Okay. And here you're going to know by tomorrow since it's due. Alright, so this is similar to diagonalizing thesis or a CCF. Producing or generating basis that we have seen earlier. Okay? So question. Um, if m is not diagonalizable, can we find a u such that u inverse m u is equal to t? So this is the notation. So we did, we found you is gonna be our basis. And of course, what is T mean? This means upper triangular. Something that upper triangular symbol is gonna be a pizza slice, which is sliced on the top and nothing in the bottom right, somebody has taken away the pizza slice at the bottom. Alright. Where are you? Going to be? U1, U2, u, n. And this is going to be an orthonormal basis set. That's why we are studying orthonormal basis. So we want you to be, in the case of eigenvectors, that was not possible because eigenvectors need not be orthogonal to one another. So I think in the CCF T If it was, but it doesn't matter. So we want to do this. And I wouldn't be here if the answer was yes. So what am I saying? Sum square matrices are diagonalizable because they are eigenbasis as complete, right? Is full rank. But every single square matrix is upper diagonalizable. It's a very powerful statement. Some eigen sum square matrices can be diagonalized, but every single square matrix can be upper triangular. Okay, That's, that's the statement. So the answer is yes. Any square matrix can be upper triangular. Upper triangular, right? I don't want to write the whole thing. Okay, So this is a pretty, a pretty broad claim. And for the rest of this lecture, we are going to deal with what is probably the most mathy part of 16. Okay? So if you are lost, don't worry. I'm not going to test you on the math part. But I think it is really useful for you to know why this statement is true, just from a scholarly standpoint. Why is it that every matrix can be upper triangular, every square matrix can be upper triangular. You have people studied induction. How many? Half? Most of you, okay. But some of you haven't read who hasn't? I don't expect you don't, you don't need to know induction for this course. So, but anyway, the proof is by induction, but let's not worry about that. I'm going to try to give some intuition, but the proof is hard. Even fine. I had to go through to three times the cost can also new to this material cannot controls person. So I will tell you what I learned along the way. So this is why this course is going to be what I learned yesterday. I will tell you tomorrow. And if there are any holes, please point out because I'm sure there will be. So imagine if you had to be me, right. You had to learn on Wednesday and then present on Thursday. Well, that's the situation I'm in. But we'll start with the simplest case. M is a one-by-one matrix. Can't get any simpler than that because zero by zero if no good. So m is just little m. Letting you guys, scalars and vectors and matrices. A matrix is a big guys. So let's build some intuition. So it's an one-by-one matrix, upper triangular. The scalar, of course, it is whatever you want it to be. Diagonal, it's upper triangular, upper triangular, and lower triangular. Anything you want, right? It's a joke. So it's obviously upper triangular. Right? Now. Let's do the, the two-by-two case, slightly less trivial. So you have M, you will have M11, M12, M21, M22, right? And we're going to assume mostly for convenience, that M has all real eigenvalues. For convenience only you can deal with a complex case, but let's not get cluttered. And you want to find u. That is going to upper triangular exam in the sense that you inverse MU is upper triangular, right? That's the claim that we want to do. So what do we want? I don t know. We need to we need to find the entries of view. I look to you for insights. What shall we do? Okay? I think it's just complicated. I'm not I'm not I'm not saying you're not on the right direction, but it's way too complicated for the question I was asking. How would I had in mind? What did we do when we did die in front of me? Try to diagonalize. What, what was our diagonalizing? What are the columns of the matrix that diagonalized my, my system, my matrix eigenvectors. So that's the first idea I should try. Let's use yeah. Question. Answer. Yeah, you guys are getting ahead of me. That's exactly right. First thing, I'm going to try it. Let's try an eigenvector. Because an eigenvector worked well for me earlier, right? When I did the diagonal system. So let's do that. So try eigenvector of m. Maybe. Okay, let's see what happens. So we know that you have to have. So see that V1 is one eigenvector of N. You know, you need to have at least one non-trivial eigenvector because m is a non-zero matrix, right? But you know, at least one eigenvector is good to go. So m v1 equals lambda one v1. So we'll try to populate the first column of U with V1. And we know that V1 is in R2 because we are in the two-by-two case. And we will assume without loss of generality, you know what the acronym for without loss of generality. W log. Yes. This means without loss of generality. Don't ask me what that means. It just means that it doesn't doesn't hurt me to do this because the general case is no different. Okay? That's what it means. So assume without loss of generality that V1 has unit norm. Because if it wasn't, you could make it so by scaling it, right, so it's no big deal. I see no eigenvectors are more about the direction they point in. And you can make, you can scale them any which way. They are still associated with the eigenvector, eigenvalues. Associated with that eigenvector. You can make them, stretch them or shrink them if the thing. Okay. So that's just now. Okay. That's a try. So I have V1 in the first step. What should I put for the second entry, second column of U. What she said? What did she say? Because remember, we want orthonormal you, right? That's the goal we are trying to see. But if I did the second eigenvector, we know that eigenvectors in general are not orthogonal to each other, right? There eigenvectors, but what do you want them to be orthogonal to have wishing for too much? Yeah. Exactly. Why did we go through that procedure where we added E1, E2, E3, E4. And then we said, oh, let's find, let's complete the set, right? And orthonormal set. That's exactly what we will do. We'll use Gram-Schmidt now, but we will anchor on V1. That's why I said the order matters. And Gram-Schmidt, we know V1 is our gut instinct tells us d1 is a good choice for the first column, right? Because it worked for us well before. What should I put in the second one? Find another vector which is orthogonal to v1, but which is a basis for R2. How do you do that? Exactly the procedure that clear. Okay, So let's do that. Second step is to build out the orthonormal basis using Gram-Schmidt as we saw earlier. So then let's say that R1 is the vector. That completes the Gram-Schmidt procedure with V1 as the first or hanker vector. So what do we do? We know that V1 and R1 form an orthonormal basis, right? It's what we did. Alright? So because of all this means is that V1. This, by the way, this is another notation that I typically use. Whenever you see V1 with the signal R1, that means V1 and R1 are perpendicular to each other. Or orthonormal kids is another way of saying that. And our V1 equal to zero. Okay? Let's check if you've got lucky with our choice of view. So let's try doing u transpose, or let's call it in general, U inverse m. And we know that u is orthonormal, right? Let's call that U is an orthonormal basis by the Graham Smith to construction. Okay? So let's try u inverse MU, which because u is orthonormal, is the same as U transpose m mu, right at the beginning we saw that orthonormal square matrices. If you are orthonormal, the inverse is the transpose. So we know that Q transpose MU. So what is u transpose? U is v1 transpose, v1 transpose U transpose. And then you have m, you is V1 and R1 and columns, right? Going systematically step-by-step. Alright, let's kind of do some algebra here, just doing two-by-two matrix multiplication. The simplest kind of non-trivial matrix multiplication we can do our best to do scalars, but unfortunately we are in 2-by-2 land. So what is the first entry? So we can see here that you will have, let's take em. Let's first multiply M by V1 and R1. So we can do it as v1 transpose. I'm going to skip one step, okay? So you can see that the, this is gonna be multiplying three two-by-two matrices. What is the answer going to be another two-by-two matrix, right? Two-by-two multiplied by a two-by-two is two-by-two and so on. They have one two-by-two matrix. The top-left corner is gonna be. So you can imagine, you can, if you want. I can suck this M here. So I didn't change anything, right? I just brought in m, m times v1 and M times r. Vr1 is gonna be the same as m times V1 R1. Now, it's exactly like these are columns, these are rows. So you do row vector multiplication, row column multiplication. So v1 transpose MV1. Then we have v1 transpose m, car one. Then we have one transpose M, V1, and R1 transpose m or one. Everybody with me. All I did was follow the rules of matrix multiplication. Nothing more. Okay? Now let's see if good stuff happens. That's the hope. Keeping my fingers crossed. Okay. What is the first entry? Well, what do we need? Okay. What do we need for this to be upper triangular? This is a two-by-two matrix. When will this be upper triangular? Yes. When bottom left has to be zero. So you want R1 transpose MV1, you hope will be zero because then you're done. Because my two-by-two matrix is upper triangular, That's what we need. So check bad writing. R1 transpose m v1, which is the bottom left entry of my two-by-two. What is MV1? What is v1? V1 was the eigenvector, one eigenvector for m. What is the eigenvector equation? M1v1 is what? Lambda1, v1 lambda one is the first eigenvalue. This is same as hormone transpose will put lambda one, which is a scalar upfront, times v1. What is R1 transpose v1? Zero because R1 and B1 orthogonal by construction of the Gram-Schmidt. This is just zero. Yay. Unfortunately, that's not the end of the proof. Because it turns out the two-by-two case is very specialties where it misleads you into thinking that procedure will always work. There's actually more work to be done. But so far so good, right? You have to be an optimist. So you know what a definition of an optimistic, there's a joke. In that space here. It's apparently it's somebody who when he's falling from the 50th floor of a building at the 25th floor, he says So far so good. So we are kind of like that. Alright, so given that we have checked and not only that, we also get a bonus, he says v1 transpose m v1, the top-left corner is also, again, let's use the Eigenvalue relation of MV1 being lambda one and lambda one to the front. So sorry, v1 transpose v1. V1 transpose v1 is just one because its unit norm. So this is just lambda, right? So in my two-by-two case, so you inverse m mu is going to be equal to lambda. Have something else here, I have something else here, and zero, which means this is upper triangular. So we're going to donate. And in general, because it's a two-by-two system, I'm going to put 2's everywhere. Okay? So if you go back here, 2222, okay. So we are in great shape. Well, if you're on this good shape, Let's. Okay, Before we do that, I can either actually, let's continue. Let's build out. We will come back to. That's another concept that I want to bring. True. But let's build up from the two-by-two case. The next case has three-by-three. I mean, we're not going to inch our way forward. Two-by-two, three-by-three, four-by-four, and so on. But you want to see a pattern establishing, right? For induction, you need a pattern emerging general. So if you put those patterns, so again, we will do inverse MU equals t, right? With three to index the fact that it's a three-by-three system that we are examining. So U3, let's follow the same prescription as we did earlier, which is, Let's plunk in one of the first column. And we had to fill in the other two columns because the three-by-three case, right? So first point is that m v1 equals lambda one v1, just as for the two-by-two case. And V1 is an eigenvector of m. Second point is just as we did earlier, use V1 to start the Gram-Schmidt procedure as before, to complete an orthonormal basis for R3. R3 now. Okay? So we will get, after we have finished Gram-Schmidt, you will get three vectors which are all orthogonal to each other. And unit norm V1 we started with as the anchor. And then you'd have an R1 which is orthogonal to v1 and R2, which is orthogonal to both. Just generalizing the two-by-two case, right? In this worked in the two-by-two. So we hope it's going to work in the three by three. Okay? So now let's, for convenience sake. Otherwise we'd carry along too much notation. We're going to define R to be the other two vectors that we appended to V1 to complete. So what is the size of our three by 23 by two? So note, one is three by one or two is three by one. So you stack them together, it's three by two, right? Okay. So now let's try U3. What did we say? It'll be v1. And our heart is not three by two. So I can remember the dimensions have to match. Hard is just a shorthand for R1 and R2. So it's three by two. This is three by one. And this is three by three as you need to be. Because you the three-dimensional basis. So it's not hard to show that this is orthonormal rect clearly this is orthonormal. So U3 has an orthonormal matrix because it comprises a new one, R1 and R2, which are all by Gram-Schmidt, an orthonormal system. What does that mean? Well, if you consider one transpose r times m times this is what we need to do, right? This is our u inverse M U, U inverse Q transpose because it's orthonormal. So I just, I'm gonna put transpose. So continuing through. So this is also U3, so this is equal to v1 transpose. R transpose m v1. Just cutting through. So again, let's write this out. By now. We should be experts and writing this matrix. So even though r is a block, um, it's, it's, it's kinda a vector. You can treat it as a scalar for the purposes of matrix multiplication. You will see that then you have blocks. The matrix multiplication rules don't change if you consider blocks instead of scalar numbers. But you should verify this. So what does the top-left entry is going to be v1 transpose m, v1. Then you have v1 transpose m. Then I have r transpose m, v1, and I have r transpose m, right? You can think of art as a scalar even though it isn't. That's why we had the shorthand notation. It obeys all the matrix multiplication rules that you are aware of. Okay? What do we know about v1 transpose m v1, exactly like before. That must be Lambda one. Because m v1 is lambda, lambda one v1, d1 has unit norm. This guy is v1 transpose m bar. That's gonna be one by two, right? The other two entries in the first row. What about r transpose MV1? So what is R transpose m v1. Mv1 is lambda one v1 because it's D1 is an eigenvector the same as lambda one times r transpose v1 transpose v1 zero, because R transpose is just harder, one transpose, a2 transpose, right? And these R's are orthogonal to the b. Bye, bye Gram-Schmidt. So this is going to be zero. Which means this is also zero. Because R transpose Nv is two by one. What is leftover? Then? Let's make some room here. This is r transpose m bar. What is the size of r transpose m, r two-by-two, right? Because the whole thing of three-by-three, you have 33 rows, three columns, and alternatives, MR. So in general, it's going to have, what do we need for this three-by-three matrix to be upper triangular? We are half in good shape because I have zeros on the first column, right? What else do I need? Two-by-two matrix R transpose. Mr. also has to be upper triangular, meaning the bottom entry here. This guy has to be at zero. Only then will this three-by-three matrix B also upper triangular? But who says it has to be Who told you that r transpose m r because upper triangular is an upper triangular matrix, fan fails or method failed, but we can still salvage it. And to do that, we have to wait till after spring break. I'm trying building up the story here. Okay, so, but first we note that R transpose M par need Hartree and post MR to also be upper triangular. Upper triangular. But we have made some progress in that we have reduced the problem of upper triangular rising a three-by-three matrix to the problem of upper triangular arising a two-by-two matrix. So it should have instincts telling you that there is some hope for some kind of recursion or induction, and we can then clear it out. But then our U3 choice clearly failed because but it was kind of almost there. You just need to tweak it a bit. And that tweaking we will reserve for after you come back haul refreshed. Okay. Because I don't want to start that topic not and there are a lot more room to cover. So make sure that you enjoy your spring break week, rest up, sleep, go on vacation, go to the beach, or whether it's really bad here, you'd have to go elsewhere if you want to go to the beach. But come back refreshed and then we'll go you'll hit it. 