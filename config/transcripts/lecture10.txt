Okay. Good morning class. Let's get started. I wanted to review where we were at last lecture and put things in context. So as you may recall last lecture, we ended on the sinusoidal steady-state response. The forced response. For RC circuits or RLC circuits are pretty much any type of circuit that you can dream of. We wanted to find out if you excite it with a sinusoid, what is the sinusoidal response? In steady-state? What we found was that the sinusoidal steady-state response is also sinusoid in general. And what's interesting about that is that we found that these circuits, they might say they, they like sinusoids. If you put a sinusoid in sinusoid out, the only thing they can do to that sinusoid is either delay or advance its phase relative to the actual signal. And they may, that may sound unphysical when they said advance the phase, but we'll talk about that a little bit later. They can change the amplitude. So in steady state, if you look at the response of any circuit, voltage current anywhere in the circuit, if you look at it and compare it to the source, it's going to look exactly the same like the source sinusoidal, but it'll be phase offset from it and its amplitude will be different. Okay. And how did we get that result? Well, let me, let me take a step back and kind of just put everything in perspective because there's a lot of machinery involved on how we got to here. So let me just review very quickly it kinda pictorially how we got to that conclusion. Well, we said is you've got a circuit inside, you've got sources, dependent sources, inductors, capacitors, and so on. So this is any generic circuit made up of linear elements. So we can have LRC, we can have dependent sources. And notice I'm taking the independent sources out because I'm going to hook up my independent sources on the outside. So this will be our source voltage or current. This is our input signal. And then we're going to define the output signal to be either the current or the voltage. Somewhere in the circuit doesn't matter where the, for instance, I might hook up a resistor here and say, What's this output current? Or equivalently, what's the output voltage across this resistor? So this is what we're after. Now the way we solve this problem was we converted this into a vector differential equation. Right? And the way that we get this vector differential equation as we defined states, right? X is equal to R state. It's basically the voltages on the capacitors, the current through the inductors. And to find the matrix a, we actually came up with a procedure where we were able to, for every state variable, inductor or current, we were able to kind of translate it into a circuit that looks maybe something like this, where this is some V Thevenin k, This is our Thevenin K. And let's say this is l k and this is CJ, R Thevenin j, V Thevenin j, and so on. So for every state variable, we could write an equation like this, where this V Thevenin is a sum of all the state variables. In other words, this equivalent circuit here is coupled because each term like this is x k, right? This is x k basically. And this is x j, x j here is a voltage, x k is a current. But when we write these equations, if we write l k d x d t, It's going to involve a sum of all state variables. So these are coupled equations. Okay? Likewise over here, we can write this as CJ dx j d t. This current is also going to be, I could, I should put in some coefficients here, a sum over all state variables. Okay? So this is how we form the vector differential equation. Any questions on this step? This is almost like a mechanical step we have to go through to translate this big circuit into kind of a nice state vector representation. Good question. Dependent sources. Dependent sources are not state variables. Dependent sources are just like resistors because their output is instantaneous function of some other voltage or current in the circuit. So the only thing that you need to keep track of our voltages on capacitors and inductors. Okay, So, so where do we go from here? Well, the next step that we took was we diagonalized our system. And we diagonalize this system by writing this in terms of we use a similar matrix, right? We just translate a, we diagonalize a factor a, let's say into these terms. And then we define a variable q, which is in the eigenspace. So x is the state-space, Q is the state-space in the eigenbasis. Okay? And sometimes you'll hear me calling these cute K modes of the circuit. Now this is quite interesting because clearly this is much easier problem to solve because each line, each row of this is just a simple first-order differential equation with constant coefficient, right? In fact, you might be tempted to say, you know, every row of this circuit looks something like this. This is the equivalent circuit of each row. Because we know a first-order constant coefficient differential equation is in fact, this is a circuit representation of that. But there's no reason to actually draw this circuit because this is digital physics behind this, right? This is just, we can think of it this way. Keep in mind that these elements have nothing to do with physical reality. They're not. You can't say, Oh yeah, that's an RC time constant. So just because we have first-order equations here, it doesn't mean that there are circuits behind this. The circuits behind this are actually superpositions of modes, superpositions of the Eigen solutions in our original state-space. So I just wanted to draw this to make sure that you guys understand this is not necessary. Because we can just solve the problem directly. So this is just a reminder that we've seen this problem before. And we know how to solve this problem. The way we solved it was we use an integrating factor so that each mode solution is simply related to There's a homogeneous solution which evolves like either lambda t, lambda Katie, let me just write this as a little lambda so that we can remember that this is just one of the particular modes that we're looking at is just one exponential. It's not a vector or matrix or anything. This is e to the Lambda k t integral from minus infinity to t, e to the minus lambda k s BPSK of S ds. So this is the general solution. And so what we did last lecture is we first show that our source is always if we start out with a complex exponential. Okay? This is a kind of a mathematical idea, right? Let's just use a complex exponential and put it in and see what the solution is. And we can always get a sinusoidal solution by looking at real and imaginary parts of this. Or if you like, taking the sum of this with its complex conjugate, we can get a cosine, for instance, two times cosine. So if this is the form of our source, then V of S of t is the same source in the eigenbasis. And what we argued last time is the sum of any number of complex exponentials is another complex exponential. We call that v hat is a J omega t, right? And this is simply algebra, right? Just to remind you how simple this is, the idea is if I take p1 e to the j omega t plus B2 e to the j omega t, right? This is the sum of two complex exponentials. Let's call this one has a phase shift respect the first one. We can always absorbed the phase shift term into the complex coefficient, factor out the complex exponential. And we'll just call this B hat. And I'm doing this several times because this is very important operation that we're going to use a lot in AC circuits. So taking the face part which is not varying with time and just absorbing it into that complex coefficient. So then with our source, even in the eigenspace domain, looking like a complex exponential, we can just plug in a complex exponential here and see what happens. And that's what we did last time. So last time what we found was that Q k. So this is more or less where we stopped last time. The force solution looks like either lambda k t b k hat. Then we had two terms, e to the j Omega minus lambda k t. Actually, yeah, divided by j Omega minus lambda k. So we don't need this. The other term we showed that goes to zero. And if you look at this thing, you will notice that. Several factors cancel out, right? You can see that this first factor here will cancel out with over here. And what you're left with is where we started today's lecture, which was this is the response. Each mode looks like a complex exponential because this is just a constant. It's not a function of time. Right? So b hat is just a complex number. The denominator j Omega minus lambda k is also a complex number. It's a function of frequency, tells you the circuit behaves differently, not based on the sinusoidal, but based on its frequency. So it depends on the free response of the circuit, depends on the frequency. And we're going to just call this guy h hat of k e to the j omega t. And maybe I'll even emphasize that this is a function of frequency by putting in J omega here. Because notice that the frequency independence has this j Omega term, y j omega naught omega. Yeah, call it omega for now if you'd like, but you'll see that j omega is convenient later on. So any questions about this? Okay. I'll review hopefully. Now that another key step. So this is how the eigenstate, this is how things evolve in the Eigen space, right? Well, how about in the real space? Well, in the real space, we know that x k of t, the forced response, is basically a linear combination of q case. So what's the linear combination of? The solution is going to look like another complex exponential, right? So we'll call this H of j Omega k e to the j omega t. And each one of these H of Omega t, Since we're taking a linear combination, this would be Alpha-1 k j Omega minus lambda1 plus alpha2 k j Omega minus lambda two, and so on. All times e to the j omega t. And we're just going to call this whole thing h of k j omega t. Now, the solution is always going to look like this, right? Because each fraction here is representing the responsive one of the eigenmodes, right? And in general, the voltages and currents in the circuit are linear combinations of these eigenmodes. Where did these alphas come from? Anybody? Yeah. Which matrix? Q exactly. Remember q takes us from the eigenspace back to the state-space. And so these alphas are just related to the cues. It's known, it's a known quantity. So those are just constants. So all the time dependence is here. And of course, the rate at which it rotates around the circle is also determined by this e to the j Omega t. But the frequency dependence, the amplitude and phase shift is here. So this is a function of frequency. Okay? You'll notice that there's some really interesting frequencies. What are some interesting frequencies that you might be interested in knowing what happens? Just look at that expression. Zero, okay, Yeah, That's what happens at DC. But we already know what happens at DC, right? That's really easy. You don't need any of these equations to analyze DC. Yeah. Yeah, Good question. What if one of these terms is zero on the denominator, right? Is there a frequency omega? One of these terms goes to zero, in which case we get an infinite response. So it turns out that if your circuit has any loss in it, That's it'll never happen because all your lambdas will be complex and j omega is imaginary. So there always will be a residual real part. But those are still interesting frequencies because at those frequencies, the imaginary part cancels out. And you get this kind of real response, which is large. And that's actually called resonance. So we'll come back and discuss how those modes are actually the resonant modes of the system. And what happens when you excite a circuit in one of those resonant modes. I guess before I leave this because it's just here, It's just so tempting not to do it. Let me now write this in a different form which you're going to meet again next week. So this, That's just too tempting not to do this. You can write this as a numerator times the denominator. Where the numerator, I'll just write it in this form as a product like that. And the denominator, you can already see as a product like this. So we're going to meet this. This is called the transfer function in the frequency domain. So this H is called the transfer function. And you can see that a transfer function is characterized by the disease and lambdas. And later on we'll call these Lambdas pulse. And z is we're going to call zeros. So why do we call these poles and zeros will zeros is obvious. If you plug in a frequency j omega equal to z1 or z2 or any of those z's, the transfer function collapses to zero, right? So those frequencies don't propagate in your circuit. Very interesting is that going to notch, it doesn't let that circuit, the frequency go through the denominator. The reason we call them poles is just, we just realized if you plug in a frequency equal to one of those lambdas, the transfer function gets big. So if you think of the transfer function is a tent at certain points, it's going to point up, that's where the poles of the ten tar, right? Think of circuits tenths. So that's the terminology is very interesting. So it turns out that a linear system is completely characterized by its poles and zeros. And so we're going to develop techniques to quickly calculate these poles and zeros without having to do all this work. And also how to quickly sketch the transfer function. So we can very quickly figure out how a circuit behaves. So that's all preview of what's coming. Any questions so far. So, yeah. It is possible to get imagined response. Imagine response just means that there's a phase shift of 90 degrees between the source and the steady-state voltage or current. And you'll see that very clearly soon. Hopefully in today's lecture, I think there's another question in the back. Okay, maybe I answered that question already. Okay. Let me go back to this example circuit. This is our working example. Interestingly enough, at last lecture, I don't know if you guys noticed or not, but when I was showing you that circuit, I realize there's a bug in my solution. And the reason there's a bug in it is that the steady-state response was not matching what I expected. The steady-state response of this circuit. If I put an, a DC voltage here of let's say 1 v, then in steady-state, these are shorts and these are opens. So this should be half a volt. If our l is equal to RS, It's a voltage divider. And all the plots I showed you, we're going to 1 v. So I couldn't figure out what, what's wrong in lecture. But when I went home, I looked at it. I realize there is a factor of one over L, I think missing here something. So the matrix was wrong. So I fixed that, replotted it. And now things look good. So this is basically, if you put an, a DC voltage, actually I just put an a DC current in this example. So initialize the inductor with some current and then looked at the output waveforms, and now it's doing the right things. Let's look at the sinusoidal response of this circuit. I want it to kind of unpack this expression. This is exactly what I wrote earlier, right? This is our Q matrix. So this takes us back into, so let's work backwards. This is the integration that we're doing. This is r e to the minus lambda sigma. This QI is our q inverse, this is our B, S is our source. This is Q, which brings us back into the real space. So here is, here's the queue bringing us back into real space. This is again e to the lambda t. This is QI, Q inverse. And there's a q in here. And this is being integrated over Sigma. So this is again, BS is a constant and we get those solutions. This is the homogeneous solution. Now that the DC solution, basically if I integrate here and take the limit. As time goes to infinity, notice the DC solution now makes sense. Half a volt, half a volt. There is some numerical noise in here, so you can see that it's almost half a volt with close to zero imaginary part. So that's just numerical noise. But now the DC solution makes sense both voltage V1 and V2, the voltage across the capacitors are half a volt. And then also remember last time I mentioned that the DC solution you can get at several ways. And in fact, just doing this kind of saved me last time because, you know, the best way to get the DC solution is to basically L's go to short, C's go to Open. And then just, it's obvious. Usually. Or you might have to do a little bit of analysis. But if your computer, you can also say that the DC steady-state solution is when this is the zero vector. In other words, there's no changes in the state that d by d t term goes to zero. In which case the steady-state solution is minus a inverse B S. And this is what I calculate over here. And indeed it matches half a volt, half a volt. Okay, so that's fixing what we did last lecture. Now here's the new bit. We're going to drive the circuit now with a source. Okay? And notice that there is a omega one here. Omega one is a frequency. This is 100 mhz. So 100 mhz. Of course, as you know, it's 100 million times a second. So that vector is rotating 100 million times a second. And we put it into our circuit. And we say, well, what does the output look like? So this is the, I'm going to call this the AC response. Sinusoidal waveforms are often called AC waveforms because they're alternating current, legal positive, they go negative. So the current is going one direction, then it goes the other direction. So the AC response, as you would expect, is a sinusoid. So here, here's what it looks like. You'll notice that in the beginning it's doing something a little bit different. Right? This is not sinusoidal here. This is the homogeneous response. This is the response to the initial conditions that the circuit has. But the steady-state. When you plot this in steady-state, look, it looks sinusoidal. Just as you would expect. I should have drawn the original source cosine next to it. But I think I'll just ask you to use your mental magic power to do that. Let's, let's do that. My drawings are going to be terrible. So cosine is going to do this. No, no, no. You get the point right. But the point I want to make is the cosine actually has a different phase relationship than the sign. So this is the source. How can I tell will look, this starts out at zero and then it rises, whereas the cosine starts at peak and then it comes down. So if you were to mentally superimpose the cosine on top of this waveform, you would see that indeed there is a phase shift. There's also an amplitude shift because the cosine is coming in at 1 v. This is coming in around half a volt. Okay? So, okay, this is kinda boring. You put it in a sinusoid, you get a sinusoid out. Big deal. Let's do another experiment. Let's now increase the frequency. So here, all I've done is increase the frequency to three times 100 mhz. That's not, that's not a big deal, right? We went from 100 mhz to 300 mhz. But look at the circuit. The circuit is behaving completely differently. Let's zoom in. First of all, there's the homogeneous response, also called the transient response or the natural response, right? Guys are probably going crazy with the notation. People just can't make up their minds on what to call this stuff. The transient response is really interesting. There's actually multiple frequencies present, as you can tell. It's not, the frequencies are not related to the frequencies of the source. So this is the source frequency, just 300 mhz. What frequency does this look like? Does it look like 300 mhz. It's much slower, right? It looks if you're really good at eyeballing things, you might guess how that looks like 100 mhz. Where did 100 mhz come from? It's a question. It doesn't matter. It's cosine, but we call sines cosines and complex exponentials are all sinusoidal because they all have the same character. But yeah, good question. Yeah, so here the input is a cosine. You're right. What do you guys think? Why is that 100 mhz? There's no 100 mhz anywhere in the source. 100 mhz magically pop up here. Any guesses? Yeah. What's that? The inductor. Okay. You're onto something. It's related to the circuits own properties. So you might be wondering, what the heck is this circuit? How did I even design it? Yeah, very good. It's the resonant frequency of the circuit itself. The circuit itself likes to resonate at around 100 mhz. So every time you excited, it's transient response, it's natural response is going to be at 100 mhz, right? It's a lot like, like a window, right? Like let's say you have a window that vibrates and produces a sound. And someone steps across the room and I live in an old house. So you're walking across the room, the ground shakes than the window shakes. You always hear the same frequency. It doesn't matter like the frequency that you hear. Let's say it's a kilohertz, has nothing to do with your stepping, right? You're stepping is very slow frequency where you're stepping in at a few hertz, right? Or fractions of a hertz. But the window naturally likes to resonate at say, kilohertz or something. Same thing is happening here, this circuit. Remember when we looked at its eigenvalues, the eigenvalues were all close to 100 mhz. Let's actually go back and do that again. Here they are. These are the eigenvalues of a matrix. This is lambda one, lambda two, lambda three, lambda four. And again, remember, these are each eigenvalue has a real part and an imaginary part. Notice the imaginary part, you know, is around ten to the eight. So I just divide all the imaginary parts by 100 mhz times two Pi. What do I get? I get 114, 15 mhz twice. And then over here I get 45 mhz. So in fact, what we're seeing is a superposition of 45 mhz and 115 mhz together. That's the natural response. Yeah. Yeah, so good question. So 45 mhz, so the sign, if you think about e to the j omega t, that goes around the circle like this, right? Or Friday, maybe I should point, goes around the circle like this. Negative just goes around in the other direction. And taking sums of these gives you real cosine and sine, right? So for the argument here doesn't really matter if it's positive frequencies and negative frequencies. They both just determine the speed at which you go around the circle and the direction you go around the circle. So this is interesting. This transient response will always be there. Here's a really cool part of this, the steady-state response. It's tiny, right? I had to zoom in over here. Notice that this is five millivolts. The steady-state response of this circuit is, it doesn't like 300 mhz, right? It's basically attenuating the hell out of 300 mhz. This is quite an amazing right? You put in 100 mhz, the circuits really happy. It gives you half a volt output. We give it 300 mhz and we get five millivolts output. That's a huge difference, right? So how would we predict this? Well, we could take that H of j Omega and we can plot its magnitude. And we could compare H of j Omega three, right? Or let's say it's alcohol three omega two, omega, where this is 100 mhz. So this magnitude difference should apparently be something like five millivolts over half a volt, right? Or about a factor of 100. So this circuit is attenuating. This frequency. By a factor of 100 compared to omega naught. In fact, if we made a plot of this, it would look something like this. This. Just ignore the red plot, just look at this, this plot here. This x-axis is frequency, so here's 100 mhz. So this is telling us that this circuit up to 100 millions and log scale. So that's why it's zero dB. Zero dB means one. This circuit is allowing everything to pass more or less up to 100 mhz. Frequencies up to 100 mhz pass through unattenuated. But once you hit 100 mhz is like a waterfall. It doesn't like higher frequencies, right? It's attenuating him. In fact, if you look over here, 300 mhz, we're getting an attenuation of 36 dB. What does these DBs mean? How do we translate those? We'll talk about that when we talk about how we generate these plots. But the answer to how did I even design this circuit is I just went through this website and I said, Hey, I want a low-pass filter. I want its cut-off frequency to be 100 mhz. So here's the cutoff frequency. And I said I want fourth-order. What is fourth-order mean? It means for state variables, right? Two inductors, two capacitors. And here it is. It said, Well, here's your circuit. That's your filter. It's called a filter. And these are the sizes of the capacitors and inductors. If you go look in the Mathematica script that I have, these are the values that I use to generate all those plots. So clearly this is very useful and it's something that we'd like to do very quickly. We'd like to understand how this filter works. Maybe with a few lines of algebra, not like pages of algebra. We don't wanna do that calculation over and over again. There's no reason to do that calculation over and over again if we can find a shortcut way of doing it. So that's what today's lecture is all about. Today's lecture is about AC analysis. Or how do we calculate the sinusoidal steady-state response directly from the circuit? Our goal is directly calculate the sinusoidal steady-state response. We want to avoid this vector differential equation. To be honest with you guys my whole career and circuits 30 years. I think I've written that vector differential equations two or three times. One of them was for this class. And the other time was like ten years ago. I was analyzing a problem and I found a nice way to actually analyze it. It turned out to be very useful because I was thinking of things in time domain and frequency domain. But as engineers, you're going to become very adept at thinking in terms of the frequency domain and not the time domain. So this is why all of my work is in the frequency domain. So what does that mean? The order here is a little bit weird. I wonder why I was thinking, let's look at a capacitor. The memory that these elements like capacitors and inductors are the bad guys. They make US forces to write differential equations. But the observation is that look in sinusoidal steady-state, both the current through a capacitor and the voltage across the capacitor are just going to be sinusoidal. Agreed? Right? That's, that's where our analysis said. Write that whole long analysis with the vector differential equation and superposition and e to the lambda t and this and that. At the end of the day, what we got was, okay, all the state variables look like this. I see is a and V, c are constants. They are complex constants, but they're constant. So if we think about the relationship between IC and d, b, c, d t in steady-state, if we plug in either the j omega t I c d by d t e to the j omega t vC. Well, in steady-state, VC is just a constant, it comes out. And the derivative of an exponential is another exponential times j Omega. Which tells us that IC and VCE are not independent of each other. They're actually related. We know that e to the j omega t is never zero, so we can divide it out. Alright, let me actually do that over here. So we have that IC on the left-hand side is equal to c times j Omega times VC, both times j omega t. So this e to the j omega t is I could given we know that's gonna be the time domain response for every state variable. It doesn't give us any new information. In fact, you can see it just cancels out of the equation. This is the interesting part. Capacitor in steady-state and sinusoidal steady-state looks like a complex resistor. Because remember for a resistor, we know that I is equal to G times V. For a capacitor, we have IC is equal to j omega C times V. So you can think of this as like the Ohm's Law for a capacitor in steady state. The only difference is that instead of a real conductance G, we have an imaginary conductance j omega c. Ok? So there's a word for this complex resistor. We call this complex resistor and impedance, which I'm sure you've heard before, but now you know where it comes from. Okay. So, you know, all that stuff seem complicated, but at the end of the day, the results are very simple. For a capacitor, you have an IV relationship. That was the problem, right? When we're setting up our differential equations, we were forced to go to differential when we did KCL and KVL, excuse me, we were forced to write differential equations because of the capacitor. But now we're saying, well, if you have capacitors and sinusoidal steady-state, don't write a differential equation. Just think of them as resistors, right? Because they have an IV relationship and instantaneous IV relationship. Essentially there's no memory involved. The memory is captured in this j Omega term for sinusoidal steady-state. So this is going to take a little time to sink in. We can do the same thing with an inductor. I won't bore you guys and do it, but I'll just give you the result. So for an inductor, we know that the voltage across the inductor is L, the L D T in sinusoidal steady state. This translates into j Omega L IL. So again, this is a complex resistor. It's an impedance. So we can say, we'll call it z instead of r. Z L is the impedance of an inductor, is j omega L. The zc is the impedance of a capacitor. It's one over j Omega C. Okay. Yeah, question. Why does it only hold in steady-state? Yeah, Good question. So if you're not in steady-state, this is the relationship, right? So imagine like this, IL were pulse waveform, right? Like it's zero and then it goes up to some high value, then it goes to zero again. Then it goes to some high value. Then in that case, the voltage would actually jump up to really high values and it will come back, right? So in general, the voltage waveform is not going to be the same as the current waveform, right? So again, the example that I was just saying, let's say that you have an inductor and you have some time varying current that does something like this. Very steep slope here. So the voltage would go wish, right? It would go to some huge value because the current has a very big slope. Now, this looks like if some squarish waveform, this looks like some spiky wave form. They're not related to each other, right? So in general, the voltage and current will not track each other with just a phase shift. But with sinusoidal waveforms, they look exactly the same. They have the same sinusoidal form. They're just phase shifted by 90 degrees. That's where the j, that's what the J does. Okay. So what do I mean by that? So in time, so let's, let's go back in time. Not the movie back in time. But Let's say we have the voltage across the capacitor. We found that as one over j omega C times IC. So what if we want to represent this in time? Will in time. We could write this as some amplitude VC. I'm going to take the amplitude and phase part off here. I know there's a j omega t multiplying both sides. Okay? So let's say you're just concerned about the voltage. You want to just write the voltage in terms of the current. So I want to write v of t. Okay? So I'm gonna put a special equal sign because we haven't really made it equal yet. Let me take this one over j omega t and just write it as minus j one over omega C e to the j, the j Omega t. And of course this is e to the minus pi over two times j. So I can just sweep it all under the rug. And say, Look, this is some e to the j c prime one over omega C e to the j omega t. Now if you want to know what the voltage is in time, that means that we can say that the voltage in time, let's say we're interested in a cosine response. It's one over omega C, cosine omega t plus phi prime. This was the prime they've got dropped. Okay? So how did this all translate? We can see that if the current had some phase shift VC, the action of the capacitor was to add or subtract 90 degrees phase shift from that to the voltage waveform is equal to some sinusoidal cosine omega t plus some phase, which is the phase of the current -90 degrees. And the amplitude gets modified by one over omega C. So that's how we translate from this kind of sinusoidal steady-state form into a time domain form. Yeah, exactly. The impedance represents a 90 degrees. So now I'm going to hit you with guys with some terminology which kinda not a fan of phasors. People talk about phasors. What is a phasor? Well, what I just called i-hat and v hat, these are phasors. So you might ask, well, what is a phasor? My response is, it's a complex number. Why do we call them phasors? Well, they capture the amplitude and phase of a sinusoidal steady-state waveform. So if I give you v of t, Let's say with some amplitude v zero cosine omega t. Then I want to write this in a phasor form. So I'm going to translate this into V hat. What is v hat gonna be? It's gonna have, well, let me also add a face here just to be as general as possible. V-hat. To do that, I'm going to write v e to the j omega t plus phi. And then I'm going to write v0e to the j phi e to the j Omega t. So the v hat part, this is the v hat part. V hat has an amplitude and a face, and it's attached to a complex exponential. That's the parent. Like if you're talking about a phasor in isolation, it's fine, but it's meaningless. It really represents the amplitude. It's a shorthand way of instead of writing this whole long thing, V zero cosine omega t plus phi. We write v-hat. And v hat has amplitude v zero and has a Phase II. It's a complex number with amplitude and phase. That's what a phasor is. Okay. And why is it important? Why do we use it? The reason that we've kinda like phasors is that this always cancels out. Whenever we do sinusoidal steady-state analysis, that term is going to cancel out. So let me show you how that happens. We talked about impedance. So we can actually even think of impedance. Well, I won't say that. So we know that impedance is just a complex number. There's another term I need to throw out u, which is reactants. Reactants means if z is purely imaginary. Then we say z is reactive. In general, I can write z is equal to x plus j b expression. You'll be sorry, G plus j X. This is the conductance. So a resistor is also an impedance. It just has zero imaginary part, right? So not g, sorry, are right and not conductance, but resistance. Can tell I didn't get much sleep last night. That's the resistive part, and this is the reactive part, reactants. So in general, an impedance is a complex quantity. It has a real part and an imaginary part. The real part we call resistance. The imaginary part we call reactants. Y is equal to g plus j b. Y of course is just one over z. Inverse of a complex number, you get a new real and imaginary part. G is the conductance. That's not new. We've seen that before. This is called the susceptible. This is just notation. Nothing new here. Here. It's kinda like doctors when they talk, you can understand what they're saying. But if you could actually translate what they're saying, he's just saying, Oh yeah, his blood pressure is high and you had to get a heart attack. But they might say something like, oh, Elevated BP, tacky arrhythmia. And what else would they say? They would say and I don't know. You get my point. So this is engineer's talking, oh yeah, what's the impedance? Yeah. What's the reactants? What's the susceptible? They're just saying really ordinary thinks, Yeah. Why is just like a conductance is one over z. So basically kind of Ohm's law. V is equal to IR. Now for impedances we have v is equal to I times z. We can also write Ohm's law as I is equal to g v. Here we could write I is equal to Y v. So this is the sinusoidal steady-state equivalent. I think you can kind of get a sense of where we're going now, which is that in sinusoidal steady-state, things get really simple. Again. Ohm's law is again valid for every element, not just your resistors, capacitors and inductors and follow something like Ohm's Law. Yeah. Yeah, Z is the impedance and this is the admittance. Okay? So now we're going to solve any general circuit without the vector differential equation. So let's imagine that you're writing systems of equations, right? Usually we do. We'd like, let's say we're doing nodal analysis. This is C1, this is C2, L1, R1, and so on. So let's do nodal analysis here. So under nodal analysis, we'll call this node j. I have VJ. Let's say this is a grounded capacitor, d by d t times c two in this case, that's the current going this direction, plus VJ minus Vk, right? Let's say this is node k over R1. Plus, let's say I have this capacitor, C1 d by d t V j minus V m, some other nord node m here, plus this inductor current. The point is that what we're going to end up with for every node, we're going to end up with terms that either look like this or kind of like the terms that we've already met. Or if you want, let's also include an inductor current. Well, how do we include an inductor current? Call this L j. Well, the L j's copy and paste this one. The L j's are integrals of voltages. Okay? So basically, this is why we had the right differential equations and we spent so much time trying to simplify these differential equations. But now in sinusoidal steady-state, we know that v k has some amplitude Vk, vk e to the j omega t. So does v j and so on. So every term in here is a complex exponential. So we're gonna get CK j j Omega V minus V J plus V minus V J over K. J plus integral is gonna give us one over j Omega L. This I should call JL Vk minus VJ. And everything is multiplied by e to the j omega t. So looking at this equation, I need to solve this equation. I'm going to get k of these equations for every node, right? This is node k. I'm going to repeat this process for every node in the circuit until I have a full rank matrix. And then I can just solve this with ordinary linear algebra. I don't need any differential equations because this thing can just be divided out, right? This term is common. It's never zero. You divide it out. What you're left with. Well, this is just a simple, right? This is just algebra. It's just complex numbers. Other than the complex numbers, it looks like a resistive circuit, right? Of course you can have sources. Two sources are also, let me add the source just to be this is the sum of the sources coming in. But the sources are also complex exponentials. So that cancels out. So at the end of the day, we can see that in sinusoidal steady-state, our circuits can be reduced to just basically simple algebraic equations. We don't have any differential equations anymore. Yeah, I'll repeat your question. Does this only work with sinusoids? Yes. Only with sinusoids does a circuit turn into this simple representation? It has to do with some interesting mathematical theory. It turns out that linear circuits can be viewed. If we look at the transfer function of a linear circuit, you can think of the whole thing as an operator. And it turns out that sinusoidal signals are Eigen functions of that operator. So only in the eigenfunction domain do differential equations turn into algebraic equations. So it's a lot like what we learned with vector differential equations. But now it's happening in the time domain. That's something that we don't cover. But it's an interesting realization that yeah, these equations get really simple. Once you think of them in the right domain. Again, the right domain is the complex exponential domain. Yes. Yeah, good question. What do I mean by frequency domain versus time domain? So this waveform, say it's a sinusoid, right? It's been going on forever. It has some amplitude V, and it has some frequency omega n, it has some phase shift, in this case is zero. So this is the amplitude. So in the frequency domain, I'm going to think of this as v hat e to the j omega t. Okay? It's the same thing, right? And then I'm going to just say, well, what's the, if I were to excite my circuit in the frequency domain directly, what's the response of the circuit? So by the frequency domain, I wanted to understand how does a circuit respond to different frequencies. So if I change omega is now my parameter. If I change omega, how does the behavior of the circuit change? Okay? You'll get more familiar with frequency domain. It will become very familiar shortly. So, in fact, if you think about your circuit, let's go back to our filter. Let's say I want to analyze this circuit in the frequency domain. How do I do that? Well, what you can do is you can redraw the circuit and replace, I'll put a little box here. These boxes, our impedances, RL and RS are also impedances, but they're just real. So I'll just still use the same symbol for them. But here I'm going to call this j Omega L1, one over j omega c1j omega L to one over j Omega C2. So I can actually, instead of writing the differential equations at all, because if you look at the differential equation at the end of the day, the capacitors have a conductance of j Omega C. The inductors have a conductance of one over j Omega L. So instead of even going through and writing this differential equation and canceling out e to the j omega t, I can just jump to the solution and say, This is it. This circuit, KCL at every node is equivalent to analyzing this circuit. So in sinusoidal steady-state, inductors become reactants is of j Omega L. Capacitors become reactants of one over j omega c. That's it. Now you can write KCL, KVL. You can analyze this circuit and find the transfer function. Okay, so let's do a quick example. Let me just see if there's any questions about that. So think about DC steady-state, right? In DC steady state we can say we can represent the inductors and capacitors with shorts and opens and then analyze the circuit. First of all, is that consistent with this picture? What is DC steady state mean in this picture? Is this figure consistent with what we said earlier? Well, a DC signal is also a complex exponential. What's omega? Basically spins around the circle infinitely slowly. In other words, it never moves. So omega zero. So if we plug in Omega equals zero, what does an inductor looks like? J omega L omega 00, right? It looks like a short-circuit. What is a capacitor look like? 1/0, which is 1/0 infinite open circuit. So it is consistent with what we said earlier. So DC is also sinusoidal steady-state, but it's kind of a special case. Let's do an easy problem and then we'll build up to a more interesting problem that we've seen before. We will solve it again. We'll start out with this easy problem. Very easy problem. We've seen this several times. Lets say we want to analyze this circuit. Say somebody says, what's the output signal compared to the input source? So again, this is our input and this voltage is our output. And we want to know the AC response. Sinusoidal steady-state. Instead of saying that sinusoidal steady-state, every time, we just say AC, AC is faster AC response. So what does the AC response? Well, to get the AC response, all I have to do is represent this guy has an impedance of one over j omega C. And then I'm going to say, oh, this is just a voltage divider. So V out over V S is one over j omega C divided by R plus one over j Omega C. In other words, the transfer function is one plus j omega RC. I'm done. That's the solution in the frequency domain. Was that fast? You guys remember deriving this earlier? Remember how we derived it earlier, we plugged in that integral equation, we put an aside, I made a couple of errors. Somebody pointed out the two errors cancelled out, so we still got the right answer. And this is what we got, right? That was the final answer. Here. We got it in just one step. Everybody see what I did questions. Very good question. How do we take this back to the time domain? I'm glad you asked. Okay. So we know that the transfer function H of j Omega is 1/1 plus j omega RC. What does that mean? That means that if you put in a frequency of zero, the transfer functions one, DC signals get through, no problem. Does that make sense? Yes. Capacitors are open circuits at DC. Whatever the source voltage is, turns into the output voltage. At very high frequencies. This transfer function. If we look at the magnitude of H of j Omega, it goes to like one over j omega RC magnitude. So it's dropping like one over r Omega. So this transfer function, the magnitude response is attenuating like one over Omega. So really high frequencies are getting attenuated. That's why we call this a low-pass circuit, because it lets low frequencies pass. But high frequencies get attenuated. We also define a cut-off frequency for this, the cutoff frequency is kinda the transition region between things getting, passing and things getting attenuated. What's roughly the cutoff frequency if you look at the denominator. When that denominator, when the first term dominates, when one dominates, that's low frequencies. When the second term dominates that's high frequencies. What is the transition point between the second term beating the first term? By the way, I'm answering your question, we'll get to it. So actually we can just write this transfer function is just a complex number. The magnitude is one over real part squared plus imaginary part squared square root. And so we can see that the cut-off frequency is basically one over RC. Once you've crossed this frequency, that's when the second term dominates. So this is the magnitude response. This is called the magnitude response. Now, what's the phase response? The phase response is just a phase of this complex number. The phase of this complex number is minus arc tangent, imaginary part, the real part. So if I were to make a plot of this, this is frequency. This is the phase of H. It's going to do something like this. It's going to asymptotically approach 90 degrees at the cutoff frequency Omega c. It's exactly 45 degrees -45 degrees because right, arctangent of 145 degrees. Okay, So how does this relate to time domain? Get back to your question. So what we say is in the frequency domain, V out is H times V S. Remember these are phasors. This, this is a phasor. And this is a phasor. So that means that if we wanted to now go into the time domain, V out of t is going to be v of t. Let's say, which, let's say it's a cosine. Let's say the S of t is the cosine Omega t. What we're gonna say is the output has a magnitude like that. And its phase times VS. And its phase is going to be whatever the phase of H is at omega. That's it. This phasor notation tells us that to go from a source VS cosine omega t, The output is also going to be cosine omega t, but it's going to be phase shifted by the angle of H. And its amplitude is going to be different by the magnitude of h. That's it. Let's, let's actually look at this picture again. So what we found was again, let's plot H. So this is H of j Omega. We like to write j omega rather than omega, but you can just write omega. It's the same thing. So let's say that we excite this circuit exactly at a frequency equal to one over RC, right? Let's say this happens to be 100 mhz, like the example I gave earlier. And remember, at exactly the cutoff frequency, we get 45 degrees of phase shift. So just putting in real numbers, we have our circuit. And I have a source voltage which is cosine two pie hundred megahertz t, with an amplitude of 1 v. I want to know what's the output voltage? Well, the output voltage is going to be 1 v times the frequency, one over j RC magnitude. Cosine two pi 100 mhz plus the phase of H. Well, at this particular frequency, if I evaluate the magnitude of h, this is equal to one. So I get one over square root of one plus one or one over square root of two. And the face T in here as well is -45 degrees. So this is still 1 v. So this is the attenuation. And this is the frequency shift or the phase shift. Now it's frequency shift, phase shift, sorry. So I just picked a convenient point, but now you can pick any point. Anybody want to take a stab at what happens if you put in 10 ghz? What's the response is going to be? Keep in mind, 10 ghz is much higher than the cutoff frequency. Yeah, the attenuation will be roughly 10 ghz. Basically it'll be 100 mhz divided by 10 ghz, right? And what will the phase shift B -90 degrees, right? Because if we look at this asymptotically, it's getting closer and closer to 90 degrees. So that is AC circuits. In a nutshell. It's actually, once you get a hang of it, it's really easy. Of course, getting a hang of it. That takes time. Yeah. So maybe I'll paraphrase your question. So in this circuit, you can see the resistor looks the same in time and frequency domain. Resistors are like that. They have no memory. So in frequency domain they behave exactly the same way that they behave in time domain. Capacitors though, would behave very differently in time domain, right? Where they have memory and they have a derivative operator. But in the frequency domain, we just see that they just multiplied by a complex number, an imaginary number, that is a capacitor in the frequency domain. What we do in AC analysis is we solve the whole problem in the frequency domain where capacitors, inductors just becomes simple. Complex resistors for lack of better term. It's the capacitors and inductors that were the root of all the problems, right? Those are our state variables. So those are the ones that were dynamically evolving that we had to keep track of. In general, to keep track of them, you have to do these integrals, right? But what we found is that those integrals in sinusoidal steady-state just turn out to be numbers, complex numbers times the same sinusoidal waveform. Yeah. Good question. So what we found was really the response for e to the j omega t. So we found was we put this through our circuit. And it pops out H of j Omega e to the j omega t, right? This is what we found. Well, you're saying, I want a cosine, not a complex exponential. So how do I do that? Well, you can use two ways to do it. One is superposition. You can say, well, our cosine is just a superposition of two of these waves. So the output would be at plus frequency and minus frequency. Right? So this would be times, let me just do it properly. With a minus sign. We haven't gotten there yet, but we can show that this is actually the complex conjugate of the other one, which lets us combine them and get a real, real tone. So we'll do that next time. But here you could also say, well, this is just the real part. If I take the real part of this, if this circuit is linear, so I could just take the real part of the output. Then the real part of this is going to be cosine over two. And so whatever response I get, I just take the real part and divide it by two. So this is probably the easier way to see it. The key idea is that you can take the real part of the input is the same as taking the real part of the output. Because it's a linear system. If it were nonlinear, you couldn't do this. So we're interchanging the order of real part with our circuit operation. Okay, great question. I'm glad you brought it up. Other questions. Yes. How do you tell if a circuit is linear or not? If it's made of inductors, capacitors, then it's certainly linear. If it has dependent sources, you have to look carefully to make sure all the dependent sources are linear functions of currents and voltages. If you see any non-linear function, It's not linear, right? If you see a square or quadratic, non-linear. Okay? Thanks everyone. See you next time. 