Okay. Time to get started. Welcome to the last week of classes. I'm sure you're as happy as I am. Ah, so we'll begin with the usual announcements. You get a quarter point extra credit for every lecture you attend to this one and the next one. That's the rest of the term. And there's the link and NEMA will be here. To do the needful. We have also the lab design contest announcement that I've been putting off for the last couple of lectures. The last announcement is bad about the course. Eval will make it worth your while to do them. If 80 per cent of you respond, everybody gets 1.5 extra points. So urge your fellow peers to respond to the survey, to the course evils. Any questions? Okay, good If not, let's get to what we were doing last time. So last time, we had given you a pretty deep motivation and introduction into principal component analysis, which is a special case of the SVD, which we have been covering for the last few lectures. It's a great topic and important one. So it's good to know this PCA, particularly it's used a lot as we motivated last time. Lots of applications related to dimension reduction, reduction, data reduction and compression. Showed you the picture of the cat using. More and more principal components after just about ten per cent, I don't remember how many you get. You get indistinguishable quality with respect to the original. That's very typical of PCA. One of the keys behind the PCA, which we will not be proving in this class. But you should know about is what's known as a young eckart theorem. Sometimes Minsky's name is attached to it as well. But that's okay. We'll two guys are enough, right? What does, what does it say? If you want to minimize? So let's say you want to approximate a. Okay? Okay, So if you want to approximate a with a B, such that B has lower rank than a. In fact, in particular, it has rank L. L can be any number from one through the maximum rank. And you want to find the best possible low rank approximation of a particular rank to a given a. The answer is given by the young eckart theorem. It says take the SVD of a and take the top principal components. Okay? So you've been, you've been doing this quite a bit in the last few lectures. So this should be quite familiar with to you. And what does, In what sense is it the best? It's actually best in a couple of sensors. One of them is the Frobenius norm, which you have been used to. There's also other norms for which it is also optimal, but we're not going to cover that in 16 b is beyond what we can really explain to you. What is Frobenius norm? It's just the Frobenius norm of a matrix is just the sum of squares of all its entries. So if you take, if you order them by columns, take the column, sum of the squares of the columns and then go through all the, all the columns. So in one column you some through all the rows. In the next column, sum through all the rows and take the square. And that's exactly the Frobenius norm. As an example, if you have a matrix like this, what would be the Frobenius norm is the sum of squares of all these entries, which if you work it out, will be 19. Okay? So that's the statement of the young eckart theorem, which we did last time. Any questions? Okay, if not, what are we going to be doing today? We're going to wrap up PCA. And in particular, the young eckart theorem, which holds for all l greater than or equal to one. We will show you an outline of a proof or a proof for what I think is the most important insightful case, which is L equal to one, the top principal components. How do you get the top principal component and how do you show that is the best? And after we do that, we will give you a summary of the PCA algorithm, which is used quite a bit in practice. So it's a good thing to be on top of. And then we will, We will move on to the last topic of the course, which is linearization for control. So we'll get to that when we get to it. But if you want more details, see node 16 attached to the webpage, the class notes. And we'll get to the details of the linearization sometime later in the lecture today. Okay, So if there are no more questions, I don't see any. Let's get on to the PCL. I'm gonna give you a quick run-through of what we did last time. It's always good to be in the mood. And if you remember, we started with this Netflix problem, a movie recommendations. And the setup is that you have a dataset where the rows are the videos, bees, and the columns are the users use. So you have this example. Just to have a concrete thing in mind. You have 1,000 users and 100 videos. And these users, or have given their ratings for these videos and shown on the screen. These are the ratings that they have given, right? And our goal is to use this to see what is the actual amount of information. This is a modest size, It's thousand by 100. But in real-world applications, not necessarily involving Netflix, but other general datasets, medical sets and so on. This can be in the millions. So you'll have millions by thousands, hundreds of thousands. So it's a huge, huge amount of data. You want to extract. Sort of the significant inflammation that's present in this swath of data. There's too much data can deal with it. So just give me the most significant bit. Give me the second most significant bit. I don't have time for going through all this data. So PCA gives you a tool for doing that. And in particular, q i, j. The ijth entry of this matrix is the rating of user j for video. And as you recall, I purposely named these u's and v's in anticipation of the u's and v's and the SVD. The use of the columns and the V's are the roles in the SVD, right? So this has just, it's not a coincidence that I used users and videos as the example. So it can, it keeps going in your head. So this is 100 videos on the thousand users. And the goal is to extract the low-dimensional structure that's underlying this huge, this is not a huge matrix, but in general we will have big data matrix. Okay? So if you remember, we said that we would attach four attributes without loss of generality to the movie recommendation problem. Where basically we are identifying what sort of genre the video comes from. Action, blood, comedy, drama and so on, ABCD. And there'll be a score for every I, which measures how much action, blood, comedy and drama e.g. is in that movie. Okay, So that's the amount of content. And these are also called features for people who are doing machine learning. These attributes or features. And these four features are just an example, okay? In general, there can be thousands of features in actual data. So user j has a sensitivity or affinity for these components. How much the user likes or dislikes. These various attributes that this movie has to offer. In particular, has a J. Measures for user J how much user TJ loves action. And SPJ likewise. And a, C and S D, Okay? So our q i j is going to be the inner product between the movies features, the movie feature vector, which is SAS be sc, SD, right? That's the, how much the dice. And I said no, no, sorry. These guys. So how much of the movie is action? How much of the movie, the movie contain these four components? And you dot product it with the users affinity for those components, and you get a final score. And that final score, because q i j, that's the q i j That's populates. Say that entry there. And in particular, as an example, if user J has an affinity of 077.24, the four features, and the movie itself has this much component off the features that I said, it's 20 per cent action, ten per cent blood, zero per cent comedy, 70% drama. Then the q i j is measured as this inner product, which comes out to 30. So for this particular j and i, q i j entry would be 30, and so on for every entry on that matrix. Okay, So that's the way we populate the matrix. And now let's collect together all the action vectors. What are the action vectors? The action amount in each of the hundred movies. So it's 100 length vector. Likewise, there is a sensitivity to action of the users, but this is 100 length vector a. This is 1,000 length vector S, which contains the affinity of user, of the particular user. For, for each of the users, 12 up to 1,000. For action. Likewise for blood, comedy and drama. And likewise here also. This is the action vector, but you can also have a blood vector and comedy vector and drama vector. They're all 100 length vectors. Want to set the stage, okay, Now, instead of considering the inner product which populates the queue ij's, let's consider the outer product. The outer product of a with SA. So a is a column vector as a, as a row vector essay transpose. So what do you get? So you plunk the action and vectors of 100 movies and the affinity of the users, the thousand users for action. And what do you get? Well, you get the outer product, which is a rank one matrix, right? We've seen this before, which is the product of the AIs and the sai. So this is A1 and A2, A1, A1 and A2 and so on. The next one is A2, A1, all the way to a one-hundred one to 100,000. Hundred by thousand vector that captures the coder product of the air with the essay transpose. So likewise, you can do b with f, B transpose, C with a C transpose, and so on. And you will get outer product of four matrices. Now, this looks very similar to what we have done earlier, which is the outer product form of the SVD. Svd, you had used sigma V transpose and you take the outer product, some of our being the rank of the other SV of the matrix Q. You will get sigma i u IBI transposed sum from I equals one to r, where r is the rank of Q. Remember that? So it sounds like there's a nice match between stuff we have learned and what is emerging from our feature representation of these movies and users, right? And this is going to be very typical. And so, but I want to caution you that these are not one-to-one with these, right? Because in some sense, these AIs, an essay eyes, nobody knows about them. We don't know about the algorithm, doesn't know. Even the users probably don't know. They don't even know their own sensitivity. They only know by how much they liked the data, how much they liked the movie. So these are sort of hidden features which we are extracting. And that's the key in PCA. It's what's called unsupervised learning, meaning that nobody, there are no labels attached to cats and dogs, as in typical supervised learning. So it's going to extract the information automatically. It's going to tell you how much action, comedy, drama and so on you like based on the PCA analysis. Okay, so there is not a one-to-one mapping between this and this. So the UI is or not or not, the a, B, C, Ds. Likewise, the v i's are not the high SAT has BSC and SD. But inside here, you will get the principal component of what AdWords. So an example might be that the Eigen user, remember we said we use the word Eigen as an adjective. Now, because every user is an ideal user, the top component than the main component of the principal component user. Maybe somebody who is 80 per cent into drama and 20 per cent into comedy and 0% into the others. We don't know that, but that's, that may be the ground truth and the PCA analysis, we'll discover that. Okay, so that's exactly what we have been saying. So the k principal components of q along the columns are the UIs, along the rows are the v i's. This is stuff we have learned before. You know that the column space of Q is spanned by UIs and the row space spanned by the v i's. And in our case when m is 100 and n is 1,000, just to make it concrete, you have the users are, these are the q, q i's. This is the data matrix, right? So the queue is comprised of the actual data matrix that's given to you. And if you order it according to the columns, which is what we did as a start. And I think in the notes also as a default, you organize the data according to the columns. But you could also have organized the data according to the rules. So you could also have done, you could have found the principal components along the rules rather than the column. So in this example, it would mean that when you do across the columns, every column indicates how much a particular user rates the movies. That's Q1 is, how much user one likes. Movie one through movie 100. And likewise Q2. When you go across the movies or across the rows, it's saying, how much is video one liked by the thousand users. Okay, so likewise, how much is video two liked by users? So it's from the videos perspective. That's the rows. The columns are from the user's perspective. They are equally valuable, but usually one may be more useful than the other depending on the application. That's very application dependent. So for, for what we're gonna do, now, we're going to organize the data by columns. Like that's what we did. That is each data point. So when you give you a data point, that's 100 dimensional data point, which consists of a user's preference for scores for the hundred movies. So it's very high-dimensional object even under dimensions is a lot. Although you can imagine it being 1 million dimensional, that's in many applications you have that. So it indicates the q j denotes uj is rating, so 100 videos V1 through 100. So what are we trying to do with PCA? In particular, we'll see that we'll do an analysis of a principal component. We want to find the first principal component vector. And a vector has a direction after all, right, you normalize it to unit length. So I want to know in which direction, if I look at the data, will I get the maximum amount of information that this data has to offer? If that's the highest order bit in this data, like we did the analogy last time it's writing the number 0-1 and the highest order bits because the first number after the decimal, right? So in other words, if you want to write it here, take Pi, the highest order bit. That guy, that's the principle component. So we want to apply that concept to matrices, right, to the dataset. I want to find the equivalent of finding the three in Pi, which is the highest order bit of amount of information that Pi has to offer to this movie dataset as an example and see where's the biggest component of this? That's what principal components. Let's erase this stuff. Okay, so we want to find the most informative direction in which to view the data from that we'll extract the button maximum bang for the buck. So that's the idea. Any questions so far is all review, right? Okay. So here's kinda the hope. What are we looking for? If we pick the right principal component direction, right? Let's see. So if we pick the right principal component direction, this high-dimensional data will be projected onto a lower-dimensional representation that, if you are lucky, will already start to appear clustered. That means you can group certain users together according to their likes. And you can, you can group other users together according to their legs. And you can see that in this kind of idealized dataset, oftentimes it may be true that you can clearly see there are two groups emerging. And so if a new movie has to come out and you want to recommend it to one of these users. You want to see whether the movie is closer to the red cluster or the blue cluster. And you would likewise recommended. So that's the idea that clear. Okay? So in typical applications these clusters will emerge. But even despite this huge dimensional data and much larger than what we're dealing with, if you look at it in the right direction, That's what this means. You have to look at it in the right direction. Meaning this is the direction in which you have the best extraction of information of the huge dataset here to, here to pick one vector, it would be this vector. So in the case when the data's organized and columns, that would be the u vector U1. U1 is the first principle component of the SVD, right? You want, because U Sigma V transpose the longer columns. The first important component if you want. So we want to show that if you look in the U1 direction, you will get the best looking answer that the highest order, but okay, so, so we want to find a W such that when q, w is the direction in which I'm looking for the principal component, right? I mean, I don't know what the what that will be. So I can e.g. I. Can imagine in this cartoon setting, I can imagine a W that looks like that, like that, like that and so on. But this W looks to be the most informative pictorially, but we will prove it also using math. So we want to find a W such that when QI is projected onto w, the error is minimized. So here's a picture for that. So first is because they are looking for directions. Let's normalize the direction in which we want the view to have unit length. So we'll normalize this is standard. So the w that we're looking for, we are interested in the direction of W, not the length. And we'll normalize it to one. So it will save some computations later. So the picture we are looking for is if this is the QE, right? So let's say that actually let me make this a QI is a data point. It's the, in our example, it is the movie ratings of the user number. It's 100 dimensional vector, yes, rating for 100 users, 100 movies. Rather, it's a QI is the i'th users data point. So you'll have data points corresponding to all the thousand users. So this is the user's data point. And you're going to, if you pick w to be the direction, you're going to project Q onto w. And you will get. At this point, which is the projection of Q onto w. So let's see that. What is the projection of v onto w? Is your 16 a staff projection, right? How do you project one vector into another? You take the dot, you take the inner product, multiply by w and divide by w squared. But that's why we made W1 to make life easy. We don't want to divide by something nasty number this one. And therefore, the dot-product between q and w in the direction of w is exactly the projection we are looking for. Clear, this is important because it's all about projections. Okay? Now, here's a picture that shows for a few more data points just exactly what I'm looking at. So here are my two directions, x and y. So these are my data points. These blue guys are the data points. And the w is the candidate direction in which I'm going to do the projection of my data points. So a project them. So here's a projection, here's a projection, here's a projection. This is already on the line, so it's a projection. So now these projections will induce headers. Write E1, E2, E3, E4, and so on. So you want to find that W, What do you, what do you think? What WD want? Anyone? Yeah, Some of the error squared should be minimum. That's very natural, right? So make please give me that direction for which if I, if I measure the error vectors E1, E2, E3, all the way to 11,000. Ah, I will get the sum of the squared length of those vectors squared is minimized. So that's w. That's a good goal to shoot for. Because if you could, if you could only have one direction, what would it be? It should be this. Is that clear? Okay. Yeah. Stopped me if you're not falling because we're gonna be building on this. Okay, That's the projection of v onto w. So as we discussed and as was correctly pointed out, what we're after is to, for the user, which is the skewered, the ICU that comes with the QA vector, right? That first data point. You want to take the q i minus the projection of Q onto w, which has exactly, say if q is three, is taking Q3 minus this. So it has E3. For the user, it will be EI. And you want to the norm squared of the error STI squared. But you want to do this not just for one user because there's no VIPs industry in this example, everybody is equal. So every user has to be dealt with equally. So I want to make sure that on average everybody's error is accounted for. So this is the error due to the, this is the error of the user. And I'm going to square that and summit from I equals one to n. In our case, n is 1,000. So it's 1,000 of these things. And of course, we have this condition that we are interested in the unit normal W setting where I'm only interested in the direction. So that's the problem we want to solve. Is that clear? So Matt, twice this is the problem. So we studied the applications, the applications boiled down to solve this math problem. And you have your w, the W star. So the goal is to find that W star. That achieves the minimum error in this sense. Okay, good. So we can also maybe let's do some writing wondering whether if you use the board or the iPad. What do you guys prefer? Board. Board. Okay, we'll do the board. I'm going to use the board by popular demand. Okay, Let's start here. So we want to do w star. So we said we wanted to minimize the sum of this guy. Of course, EI. Now W star is not the middle of this. W star is the arg min over all w. Two people know what argument means. Nobody knows, okay, good argument is exactly what it says in place. So if I were to do the Min of this, that means you find the minimum value of the right-hand side and that's the Min, right? Argmin means it's that value of w for which this has minimum. Okay? It's the value of w for which you get the minimum. It's not the minimum. If the argument for the for which you get the minimum. Is that clear? Yeah. Okay, Good. Yeah, so get used to this notation. You'll see it a lot in courses that you're going to take, coming on 127 and so on will be full of these kinds of notation. Yeah. Louder. Yes, we want to find w star. We want to find that W star, which minimizes this expression so that value of w, then w is w star. This has minimum, of course subject to W star. So this is subject to W is normalized to one. So that's what this math expression means. Any questions because I'm going to build on this. So if you have any doubts, now's a good time. Yeah. Okay. No, that was not. Okay. So let's look at E I squared. Let's simplify this. This is EI, right? Let's simplify this. So we can write it as. So, let's just take one of these terms, right? So forget the sum for now. Let's take this term here. It is something squared. So you can write it as the inner product of that with itself, right? So let's take the inner product of q i minus Q i comma w. W transpose with q i minus q i comma w. W. All I did was I wrote that the norm squared is the same as a times a transpose. So this is norm of a squared, this is a, this is a transpose times a. Now you have to do some sort of dirty algebra. You have to multiply out these terms. So you have to take this transpose times this. So the first term will be Q transpose Q minus. Then you'll take this transpose times this. So you'll have Q Pi. This is just a scalar, right? Because an inner product, so this is just a disaster number. This number can go along anywhere. So then w transpose QI, right? Which then you'll have to take the other two terms. So that will be QI, this guy with this. So again, this is a scalar. So there's a skew I w, which is just a scalar times Q transpose w. That is this term and this term. And then finally, these two terms, which is q i comma W times W transpose W squared, because there are two of them. So that's the expression is just nothing more than algebra. We know w transpose w is one, since W has unit norm, right? Because that was given because of this. So when you do that, what do you get? You get QI squared, that's the first term. Now, what about the second, third, and fourth terms? So, what is w transpose Q? That's the same as inner product of W with Q. I write the order doesn't matter. So it's just minus q0 inner product w squared from here. And then you have, wait, I do something wrong. This should be minus because it's this with this. And you have another one are the same. And then you have plus of the film. So you'll have two minuses and one plus net, one minus two minus two of this, plus one of those. So it's just math. So you'd have minus two. So this whole thing is just minus q i. W. Okay, so what have we shown? So e squared is equal to q squared, first term minus q i wi squared. Right? Okay. We want to find, That's one of the terms. What, what, what do we want? We want, we'll continue here. We set W star from the top of the board. It's just the armed men. For all w's is subject to W has unit norm, right? It's like a tailor to keep dragging everywhere. But off the sum I equals one to n of I squared, which is q squared minus. That's our expression. Ready with me? I just went through the, through the math. Okay. What do you say in English? What are we saying? We want to find that W star, which minimizes distinct right? Now. How can we simplify this, the right-hand side, right? So how can we simplify the right-hand side? Yeah. Right. This doesn't matter. Because it's not a function of w. For any w, it'll give you the same number. So minimizing that number or that number minus a plus something doesn't matter. So I can quickly eliminate this, right? Because it's not. So it doesn't depend on w, So you can knock it out. So the argument of this expression is the same as the argument of just the second term squared. And that's exactly the insight that we are going to fuse. So let's write down the clean expression that you want to. So W star, the arg min over all w with an eraser. I'm not gonna keep writing this expression. Okay? This is implied because it's kind of a pain to keep dragging this around. It's always w has unit norm of w, of summation of I equals one to n of minus q i comma w. How can I get rid of the minus sign? Anyone? Yeah. Yeah. Yes. Do an argmax. It's a deep statement. So that is it. Okay, I'm gonna write so this expression. It's the, it's the problem that we want to solve. We have boiled down our original problem of minimizing the prediction errors, right? That's how we started into that of maximizing the sum of the inner products of q over w. In other words, find that W, which maximizes the sum of the inner products between W and Q. That's more kind of easier to digest, right? It's the same statement. So choose, so choose a W such that the sum q i, w from I equals one to n largest. So if I gave you a particular w, particular QI, let's say it's this. That's my cue eyes. And I need to find sorry. Thank you. Thank you for that. So suppose I want to find, so what does my job I'm given, I'm given a QI. Let's say it's, it's, it's that arrow, that vector. So, um, I have, I have the degree of freedom of a W, right? I can put my w anyway, I can oriented anywhere. Where should I orient it for that? My inner product between Q and W is maximum. Someone else. Yeah. That yeah, exactly. So if you have a W, can you can you can swing your w around any which way you want. Which way should you swing it? So the inner product between Q and W is maximum. You know from your inner product formula that this, if the W star that you want to be aligned with QI, is that clear? Because inner products gets smaller as you, as the angle increases from zero. And that's the key, yeah. Not negate the trait that will minimize it. Yeah. No, no, no. That's fine. Yeah, you can you can take that if you want. If you're only interested in a direction, you could take, you can take w and the opposite direction too, if you want to be difficult. But if you want to be normal, you would say that in same direction. That's fine. Alright, so that's the key. So let's go back to, I want to show you some pictures and then we'll come back to the board and a bit. Things very dusty. Okay, back to the iPad. This is all the stuff we did on the board so we don't need all this. Okay. We did all this stuff. Okay? Now, I want to, so for a given Q0, Q1, Let's say you want to find the w that has maximum inner product. And we say it should align it exactly like we did on the board. If you have, you have, you don't have only one user. You have many users, get thousand users in this case, right? What about Q2? Well, WE wants to be aligned with Q2. What about Q3? W wants to be aligned with Q3. Well, if you have both, something has to give, you have to trade off, right? You can't be aligned with both perfectly than the others crude. So you want to find that direction which on average is best aligned. So that's the intuition. Okay? So this is just graphically showing what the math on the board did. So here's a nice picture where if you have projection of the data points onto w, These are all the data points, the blue points, and they all projected onto w, w star in this case. And let's call the projected points to be z. J is just for notation, is the projection of v onto w star. We'll call that VJ just for notation sake to make it more convenient. And we said that we have this, this is the direction in which that's a W star is the winning direction, right? Where the sum of the squares is maximized. So I just wanted to remind you that our two perspectives to doing this. The first perspective is w star minimizes the sum of projection errors are the data points. That's how we started out with. You want to minimize the sum of the East quits. Then we derived on the board and written here, It's also the argmax of the inner product of q and w squared that has the connotation of maximizing the spread. Spread in quotes of the data as measured by the squared magnitude of the projection. So in other words, if you look at this direction and you take the sum of the squares of all the green points, which are the projections of the blue points onto the W, W star. If you take the sum of the squares of those green points, it will be the maximum over any other w You could have chosen. That's another way of looking at this rate. And that is that, that is known in the literature as the direction of maximum variance. So if you read a PCA in the literature, you might see things like maximum variance direction. You guys technically shouldn't even be told the word variance since you're not supposed to know probabilities, you might have heard of the word variance. I hope if you haven't. Meet it here. And that's why oftentimes you need to center the data around zero. Before you do the PCA. You want to make it zero mean. You don't want means around, because the variance is calculated using expected value of X minus the mean quantity squared without the mean being there to the x value of X squared. Don't worry if it went over your head. So probability concept, that is strictly out of scope for you guys. Although I know some of you are taking CS 70. You might have seen probability before, but it assumes that the data is mean removed or mean centered. So oftentimes when you see the mean removed statement, that's what it needs. So you're maximizing the spread. So find that direction in which the spread is maximum. So it has a direction in which the variance is Maximum maximized. That's the principal component. Okay? Any questions? Okay, We still haven't proved you haven't solved this problem. We just wrote it down the alternate form. So in the interests of time, I will not use the board for that. We will use we will use the, it's a couple of pages. But if an interesting proof, I want you to go through the proof. It's definitely accessible to you. W star is the argmax of wi comma q quantity squared. Let's see, maybe I should start on the board. Okay. Let me use the board a little more. Okay, I'm going back to the board. So we want, from our optimization problem that we want to solve, what is w star equals the argmax of w, of course, subject to UNF norm of I equals one to n. Let's write this out in the following way. So let's write it out as w inner product with q. I. Write the sum. These are all the users. This is the inner product of w with the IT users data. And let's write this out as q i comma w, haven't changed anything and just change the order of w and QI in the first factor is a distinct squared is exactly this. Now let's write this out as arg max over w of the summation I equals one to n wi transpose Q times Q transpose w. This is this guy, this guy, this guy. Okay? Now, how can we simplify the right-hand side? So the right hand side is a sum of a bunch of terms. First thing you see is that this, It's an outer product. But even before we get to that, since the arg max over w of the summation from Pike, okay, this is where we have to be. I want to move the summation around. This is just math tricks. Yeah. No, no. Okay. This is fine. So what is this? This is the sum of I equals one to n of q i, q i transpose, which is nothing more than QQ transpose, right? So the summation wi transposed. Why is there a W I messed up? There should be no, i next dw my bed. I got confused myself because only one w, The eyes are associated with the cues. Okay? So now I can move this w transpose, which is not a function of I outside the summation, right? This comes out summation I equals one to n of q i q i transpose times w on the outside. Okay? I didn't do anything other than bring the summation in and recognize that this guy and this guy or not. Indexed by I. So I just wrote them outside like that. And the eyes only come with QI QA transpose. But this is exactly QQ transpose, right? That's the definition of number. When you do you I VI transpose, then we did. This is a definition of UV transpose outer product. So u and v are the same, which is Q. So this just becomes Q times Q transpose. Okay? Putting everything together, W star is equal to the arg max over w, subject to w equals one and norm of w transpose Q, q transpose w. Nose, eyes, everything gone, everything is inside the Q matrix, which is our data. Okay? So what is Q, Q transpose? We met the baby earlier, right? I guess, if you think of Q as the a that we started the doing SVD on. What did we say about a transpose a and a two a transpose. First, what is the qualitatively, what are they? What kind of matrices? Square, what else? Symmetric. All the good stuff. Square and symmetric, which means their eigenvalues, eigenvectors are all orthonormal. Going back in a few lectures ago. So this is nothing more than a symmetric matrix which has, which has orthonormal eigenvectors from a few lectures ago. So lambda is the diagonal. Sigma one squared, sigma two squared dot-dot-dot Sigma n squared sigma m actually should be everywhere, it should be m naught and this is M. Because M Mr And you an M by M orthonormal matrix, that is the same. You in q equals U sigma V transpose. So go review your notes on how we did the SVD. We actually started with either QQ transpose or Q transpose. Q has a starting point, found the eigenvalues of that, and then took the square root of that to make the singular values you remember? So that's exactly the same you, this is for the aa transpose Q, q transpose case. Okay? So, so, but actually let me go back and write one more thing. So you can see here I don't need this. So you can see here the W star is the argmax of W transpose S w, which is w transpose U sigma U transpose, right? So S has eigenvalues given by sigma i squared and Eigenvectors given by the UIs. And S times UI equals Sigma I squared times the eigenvalue equation for S. So what are we after? We want to maximize W transpose times w over all w, such that w is unit norm. That's what you're interested in. Now, this problem has been studied in the literature. Fair to guess? What should if, if you, if you look at the Eigen decomposition of S, S has eigenvectors, UIs and eigenvalues Sigma I squared, right? What do you think if the maximum value of w transpose x w, w is in the same space as the US, right? W is a linear combination of the US. Okay? Yeah. We're the largest eigenvalue is in fact, the largest eigenvalue will emerge. The minute the maximizer, Why is that? Well, if I substitute, let's take on a whim. I'm going to take w equal to v1. Let's see what comes out. Okay? I get W transpose S, U1 transpose. You will get U1 transpose S times u1. But f times v1 is lambda one times u1 lambda one is sigma one squared. So let's not write lambdas. Let's see, right, sigmas, which is equal to sigma one squared times u transpose UI. And the user are orthonormal. So this is just one. So there's just sigma one squared. So I observe that when I plug in w equal to U1, I get the maximum eigenvalue, which is sigma one squared out. And the argmax of that expression is u1. What happens if I plugged in YouTube, you get sigma two squared. Or if I plug in Sigma U3, I get sigma three squared. What if I do a weighted combination of U1, U2, U3 only have three UN, where the weights have to be normalized. So the sum of the squares of the weights adds to one. It turns out as what w norm equal to one means. You'd put all your weight in the U1 direction. Because any linear combination, what's called a convex linear combination of the UIs, it's definitely not going to be larger than the putting all your mask on the first guy. And that's exactly why you want emerges. Let's see that hopefully will make it clear in this is really messy. Okay, so, so that's what we, what we said here. These are all the statement. If you can show that W star, which is the argmax of w equal to one of W transpose W is W equals U1. U1 is a maximum eigenvector of S corresponding to lambda one equals sigma one squared. Exactly what we wrote on the board. And the proof, I'll run quickly through it. I gave you the intuition. When you're discussing. You can write w as because it's in the RM and UIs span RM and they are eigenvectors, orthonormal eigenvectors. You can write w as a weighted combination of the UIs, right? Alpha IUIE. Now, our normalization condition, if you write it out, will turn out to be equivalent to summing of dub lub w transpose w is just writing out w in terms of its basis expansion. And then you put everything together and exploit the fact that the youth are orthonormal, which means they are perpendicular to each other. And you will get this condition. The sum of the Alpha I squareds have to be equal to one, follows from the condition of their being orthonormal and the W is having unit norm. Okay? Now, we are almost done because you can write here the eigen decomposition of S. So S one is lambda one, U1, U2 is lambda two, U2 and so on. These are eigenvalue equation for S. And remember, we ordered them from largest to smallest. So you want to find that W that maximizes W transpose S w, such that w is this. Alpha I squareds have to sum to one. And this is just noting, noticing that if w is u1, you get lambda one, WSU to get lambda2. Lambda one is strictly larger than lambda two are greater than or equal to. And likewise, the eighth guy will be lambda i because of the ordering, which is our convention. The first guy is our winner. At least either call winner if lambda two is same as lambda one, right? So we need to pick a weight weighted combination of the UIs. Since w is a weighted sum, where these alpha i's grades are sum to one. What is the choice for Alphas when you use a U1 guy hits that, you put all the weight in Alpha-1 and zeros everywhere else, which satisfies the fact that the sum of the squares is one, then w star will be U. One big argument of that argmax of that will be u1. And in general, if you pick any other choice, you're going to be strictly worse. So that's one of those optimization problems where all the mass goes into the winter and none of the others, because they are bringing you down. Lambda2 is bringing me down. If I put any mass of my Alpha squared into Alpha-2, then it's going to be strictly smaller than lambda one, right? So that's the argument. Is that clear? That's it. That's the entire we've managed to prove that the principal component direction, if the data is organized in columns, should point in the U1 direction. Sigma squared itself is not important because that's just a scalar tells you the amount of Wait in that. But the direction is really important. You one is the principal component for this, for this problem. Okay? And the second most important. Likewise, you can show us U2. And you can generally you can generalize this to all ELC if you want L principal components, it turns out you have to take the first one and then the second one, and then the third one all ordered. And that's exactly the young eckart theorem, which we didn't prove. But hopefully we gave you a glimpse into how to do it. It, the actual proof is somewhat involved. And for those of you are so inclined, take 127, they'll go through that in gory detail. Much go earlier than what we did here. Okay? If the data, now, if the data was organized in by rows instead of columns, we mentioned that the data was organized. This way. You can also solve the optimization problem by just replacing used by V's. And you would be doing one to n instead of one to m. And what would pop out will be the first right singular vector. So you ones are the optimization problem when you order according to columns V1s, how the optimization solution, when you order according to rules, just as you would imagine. So this is just something we're just repeating. So let's summarize very quickly the entire algorithm. So you don't need to write this down. So you're given these data points X1 through XN, arrange them in columns. This is our convention. If you want them in rows, you can transport them or they may have their own meanings. So let's just assume that the data that you're interested in if organized according to columns. So every data point is this m-dimensional vector. As I mentioned, you may want to remove the mean in order to make it centered around zero. So the way you do that, as you take the, the sum of the, the, the, the average value of the x i's and subtract them from each of the sites will have zero mean. And you do that for every column, and that's the data you bring to the table. So before you complete tryptamine, okay, come with the mean stripped version of the columns and then we will do our PCA analysis. That's how typically things work. So you compute x equals U sigma V, t, which is exactly this. And then if you're interested in k principal components, you choose the first k columns of U, U1 through UK in that order. This is why we ordered them highest to lowest back going back what you just said, you will know it later. This is the reason we do it. And then likewise, if you want along the rules, you choose the v's, v1 to vk. And you can also project data onto these UK's. If you want to kind of high-dimensional data, you have an approximation based on rank three or rank for you can project the data onto that rank and Troy and the entire dataset. So you'll have a cleaner representation. So this is sometimes used for de-noising, removing noise. Okay? So that's an optional step that you could do. And once you classify it, you will have a picture like so. After projecting data onto the top two principal components, let's say the pink and the green emerged In your, because your use or such a good separator of your content that the information of the dataset is extracted in the form of two clusters. Okay, so this is typical of PCA. And then you can say, if I had an extra data point that came in. So this is another user who came in, right? You want to see, should I recommend him the movies that these guys like are the ones that these guys like. So let's say these are the action loving months and these are the actual heating ones. Well, this guy is much closer to the action loving ones. So you would give him a pink. So you would recommend the movie that the pink guy is like, which typically is not action because pink is associated with girls, but I don't want to be sexist. And this green, this guy on the other hand, is closer to the greens than to the pinks. And so he should be given the green level. So that's how PCA works in practice. That's the big picture. We're not going to do classification. You did some classification in the lab, but not mistaken. Or you will be doing it in the remainder of your labs. So anyway, so that's the entire story on PCA. We're finished with a hefty treatment from SVD to PCA. We spent a lot of time on it. But I think it is time well-spent because you will use this knowledge later in your, in your career, in your even in your four years that you're gonna be at Cal case, this is really important topic. So I I kinda spent more time on it. Are there any questions? Before we move to the last topic of the class? Which is something that should be somewhat easier to you guys. It's called linearization for control. So, so far in this course we have focused on linear control systems, right? Everything has been linear. Where in discrete time, we had our familiar equation of x k plus one is a x k plus b UK. X k is the state vector of dimension n. Uk is our control input of dimension m, I think. Yeah. And or in continuous time you had differential equations, vector differential equations, dx, d t equals a x plus b. Bu, right? This is what we've been studying so far. Well, first, I want to make sure we are on the same page as to what a linear system is. Okay, I know that Professor nicknamed job covered it during the circuits part of the module of the course. But let's revisit it. Okay, so what is, and this will be useful if you're taking classes like one-twenty and so on, which we'll be building on top of this knowledge. So first, what is a linear system? Well, very simply, again, it's going to hold for continuous time or discrete time. I'll only show the continuous time case, but you can imagine replacing the t's by Ks and round brackets by square brackets. And it's the same story for discrete time. So what do you get? If I put in X1 into my, this is my box. I'm wondering my box. I'm asking if that box is linear or nonlinear. If I plug in x one, I get Y1. If I plug in x two, I get Y2. If it is linear, it has to satisfy the following properties. A scaled version of the input. So let's say instead of x1, I plugged in alpha x1 into the input of my blackbox. If the output comes out with the same output as before but scaled by the same alpha, then that's a necessary condition for linearity. That's the, that's one of the scaling conditions. The other condition for linearity is if I sum these two guys, then the output should be the sum of the two outputs that came out earlier. If both these conditions hold, then and only then are you a linear system. So that's called superposition principle. So you can actually put that into one picture. If I plug in alpha x1 plus beta x2, I should get alpha y one plus beta V2 for any choice of Alpha and Beta. Then, and only then do I have a linear system. Okay? And the same thing for discrete time, which I've shown in brackets on the top. People familiar now with linear system, I think, yeah, this should be familiar to you. It's just a reminder. No examples. Matrix multiplication is a linear operator. Y, Y equals AX. If I were to give, if you call the operator as L, is multiplying by a, then a X1, Y1, hey, X2, Y2, right? Because that's how I'm starting under that premise. If I plug in x one outputs y one. If I plug in x two out pops Y2, then it must be the case that if I plug in alpha x1 plus beta x2, hope must pop alpha y one plus beta Y2. And indeed it does. So that means the multiplier matrix multiplication as a linear operation. Okay? What about differential equations? Also linear? Why? Y of t equals dx dy, dy dt is a constant coefficient. Differential equation can put a constant here, nothing will change. So first is you plug-in dx x one into the input and you get Y1, which is dx1, d t. If you plug in x to enter the input, you'll get Y2, which is dx2, d t. So if I plug in alpha x1 plus beta, x2, alpha and beta are constants out should pop. What will we get? Well, let's do the calculus. Alpha dx1 d t plus beta d x d t, which is alpha y one plus beta y two, which is exactly what you want. So again, any kind of constant coefficient differential equation is a linear operation. Okay? So this is getting the linear system stuff out of the way, very basic stuff, but really needed to understand what's coming ahead. The point is that in the real world, where you are not doing just matrix multiplications are just not doing constant coefficient differential equations. The world tends to be a little more non-linear. Examples. Transistors, we've modeled them as on-off devices, but they actually worked continuously throughout essay, it's a continuous time. Physics. Underlying physics, device physics governs the behavior of these transistors, and they're highly non-linear. Likewise, robotics and controls. So we have considered the linearized version, but even in some of the examples we have shown, you can have highly non-linear dynamics and applying feedback control to stabilize e.g. an inverted pendulum on a cart of a called a segue problem. So all of these problems are very, very nonlinear in their nature. But we only are equipped with linear tools. What are we to do? Are we screwed? Fear not TO rescue comes Taylor has an Taylor series. So anything that's nonlinear can be linearized. So it'll be locally linear. And that's how everybody proceeds, including the so-called fancy non-linear guys. They'll have a non-linear equation, but they will linearize it, solve the linear equation around the operating point, and then report the results. Because our knowledge base on nonlinear systems, compared to the knowledge base and linear systems is zero. Literally zero. We know almost nothing about nonlinear system analysis compared to what we know about linear system. So that's a, that's a fact. Okay. I'm not I'm not going to, you don't point out some teller example of a non-linear system that give you that but you understand the spirit. Also machine-learning if you want to take 189 and so on, you will have a rectifier elements called ReLu and so on in the machine-learning pipeline, highly non-linear. Okay? Then they won't satisfy the linearity properties. And gradient descent, which you might have heard of, which is the linchpin of machine-learning algorithms today can have highly non-linear trajectory. So finding optimization, it can be painful. Anyway, this is just to tell you that it's not sufficient to just have linear system knowledge. So generally, how do you model a nonlinear system? Well, this is the most general. So let's say in discrete time x of k plus one, the state vector at time k plus one is going to be some arbitrary function, non-linear function, arbitrary function of the state at time k, and the control input vector at time k. Likewise, in continuous time, differential equation is f of x of t, u of t. In the linear case, the f assumes the form of AX plus BY BU, right? Then you have the matrix a times multiplying x and the matrix B Multiplying you. That's a special case when f can be anything. Okay? So how do we deal with non-linear systems? Well, we deal them in the way that is most natural. You approximate them locally using a linear model by a process called linearization. Around half the function f. Let's do an example. If f of x is equal to x squared, f of x comma u. And let's say f of x is x squared. That's a non-linear function, right? It's not just x. Likewise, hex and New. Now you'll have two variables that since you have state and control the scalar x squared nu cube. So likewise, as I mentioned, dx d t is f of XU. In the linear formulation, it will be a x plus b. Okay? Let's look at the scalar function. So it will give you the punch line and then we will continue next time. Hopefully, next class will be a shorter class because I just want to finish off quickly and I'm sure you're eager to get done too. So f of x is f of x star plus tough. What are we doing? This is the picture. So here's my f of x. Should use the blue guy. So here's my f of x, some non-linear function. And I want to know, I want us to kind of peek at the operating point x star. We will talk about what x star are good to choose later. But if I were to linearize around x star, I'll draw the tangent line to the curve at x star. And that's the linear approximation. So I have f of x in this neighborhood is f of x star plus some deviation. So what do I do? Well, I have to look at how the derivative happens. What happens to the derivative at a neighborhood of x star, right? This is from your calculus training. You should know this. So basically i'm, I'm zooming in at an x star and finding the df dx at x equal to x star. Because I'm interested in that neighborhood. Why that neighborhood will see in a minute. But that's all we ever do. We always pick neighborhoods around operating points, which are typically equilibrium points. Where if you deviate from equilibrium, you come back to equilibrium or you run away. So that's how you do stability analysis, e.g. so good old friend, Taylor series. So you're going to meet Taylor series quite a bit. I'm sure you've met it in 16. You're going to meet it in 16 be you'd meet it in 120, 6 m in 127 mixture. If you don't recall your Taylor series, go home and review it. It's really fundamental. So f of x is f of x star, which is a constant. It's the operating point. I guess you do f of x equals f of a. Typically, a is used for Taylor series, but we use x star. By that, I mean use the star in the subscript and superscript, so bear with me, they mean the same thing, okay? If sometimes I use x underscore style and some Hoover's coarser. What is it? Well, you know from your Taylor series expansion that f of x is constant term plus the linear term, which is df dx at x equal to x star times x minus x star plus the second derivative of f with respect to x at x star times x minus x star squared over two factorial. And then third derivative at x star times x minus x cubed over three factorial, which is six, and so on, right? You guys are familiar with this. So you have a constant term and linear term, quadratic term, cubic term, you can keep going forever. And we are going to typically assume that you have derivatives available at the stars. And then linearization just means throwaway other anything other than the linear term. As simple as that. Check the square term, check the cubic term, check the fourth order term because they are not useful very close to the neighborhood, that they don't bring much value. So scalar function case. Let's do this later since we are short on time. And his Niemeyer on There's NEMA. Do your thing. Yeah. 