Okay, let's get started. Clef is kind of thin. People start trickling in before the attendance is taken, right? We'd have a slightly different way of doing it. So stay tuned. You guys are fine. Anyway. So the usual announcements, you have quarter extra credit point for the lecture, attending the lecture. And just a reminder about the lab design contest that is on the ED post. So read it carefully and there's lot of extra credit opportunities. Okay? Any questions? If not, let's kind of do a quick review of what we did last time. So last time, of course, still on the topic of SVD. But we basically had three forms of SVD that we went through. The full SVD is of course, a equals nu sigma V transpose, where U is orthonormal square. The transpose as orthonormal square and sigma is whatever shape it is. So it'll be diagonal if a is diagonal and then rectangular. If a is rectangular and square if a is square. And we asked diagonal as possible, the compact form reduces the redundancy in the representation of the full SVD. The full SAD gives you the big picture. So he'll get the full view. But the compact form gets rid of all the redundant entries that you don't need to take care of. And so you, it's reduced to you. Our Vr V is reduced to VR. And sigma R is not a square matrix, R by r, all the r, the rank, the rank of the matrix, the number of singular values on the diagonal. And then finally, the most compact form is the outer product forms. So we went through that last time. So that's the form where you write a as a bunch of rank one terms. Each of these is rank one because it's as opposed to having row multiplied by column multiply by rho. Now you have column three here, row multiplied by column, you have column multiplied by rho. So this is a column, this is a row. So this whole thing is a matrix, but it's a matrix of rank one. And there are such terms when the matrix has rank car. So it's comprised of our rank one components. And that is going to be key to what we're going to study today, okay, so make sure that you understand the outer product form. I think there has been homeworks on it and you had discussions on it and so on. Okay. Then we went into the geometry of the SVD. And I'm just giving you a cartoon sketch of what that means. So if you multiply a by x, that's like operating on X with U Sigma V transpose. So the first thing that you hit x width as V transpose, which is rotation, right? Because V is a Unitary orthonormal matrix. So in cartoon pictures, if v1 and v2 are these green arrows here, and you hit a width b1, meaning x is equal to b1, the first right, singular vector. Then you will rotate the green coordinate system to the pink one, right? Everything rotates by theta, where theta is the angle of V1. And then you stretch because of the sigma. The sigma is going to stretch where sigma one is the maximum stress, sigma 2s the second maximum stretch, There's only two here. So sigma one is greater than sigma two, which means that you're going to stretch it this way because X1 is in this direction. And so you stretch horizontally more than you do vertically. And that's the sort of elliptical shape that results. And so then you have to go through one more stage of rotation, which is u, since u is an orthonormal matrix. So basically if you start out with x here, first becomes this, then it becomes this, and then it becomes that. And likewise for x2, for Ventura effects is V2. Then it will follow the smaller path. And any combination of X1 and X2 will follow the combination of those two paths, superposition. So that's the geometry of the SVD. Then finally, we spent a lot of time on the pseudo-inverse. So let's go through that somewhat carefully. Just as a reminder, I think you guys had a discussion on pseudo-inverse restaurant, right? So you should be well-equipped. So we'll go fast. Hey, is you are sigma or we're transpose the reduced form, then the pseudo-inverse, which is denoted as a dagger or a plus, we'll call it a plus. It's actually a dagger, is equal to u. V transpose becomes Vr. But your first operate by VR, and then sigma r becomes sigma inverse. Remember that sigma r is a square matrix and it is fully invertible because the diagonal matrix with non negative, non-zero entries on the diagonal, all positive entries. So you can invert it and you'll get one over sigma hours. And then you have your transpose. So again, note that the pseudo inverse operator flips the signal, the size of the matrix. So if a is m by n, so if a is m by n, then a plus will be n by n. Right? So you did some examples on that. The outer product form of the pseudo-inverse is most compactly represented as follows. A is of course given by this. So the a plus or the pseudo-inverse just invert the sigma i's. And then do instead of view IBI transpose, you flip the order and do via UI transpose. That'll also sort of flip the size as needed for doing the pseudo-inverse. So this is the pseudo-inverse operator. Then we reassured that the solution to the equation AX equals y can be done for any size of a, where a is m by n. Whether a is m is less than n, greater than or equal to n. And no matter what the rank can remember, the rank is going to be. The rank cannot exceed M or N. It's got to be the Min of that. But it could be smaller, right? So it could be also less. So for any rank r and for any size matrix, you can always solve the equation, AX equals y by doing X equals a inverse or pseudo-inverse y, which is what you would do. So if you pretend that a was perfectly square and invertible, and he said AX equals via what is x? You say x is a inverse y because a is invertible. When a is not invertible, it's not even square, then a pseudo-inverse does the trick. Okay, so it's a very powerful way of dealing with equations, systems of equations which are either over complete or under complete, meaning m greater than n or m less than n. Or even the rank is deficient. You don t have full rank for full row rank of full column length. Which is actually pretty typical. Because take least-squares, e.g. we always assume that a transpose a was invertible, right? That's why you could do the least squares formula. But if you look at the a matrix, how do you get the a matrix? It's typically measurements, right? You get measurements. The first row is the measurement 1, s row of a measurement two, and so on. It is highly possible that he suddenly duplicated a measurement. The first measurement may be the same as the fifth measurement, in which case you don't have full column rank. So meaning that you're not going to be able to invert using your formula, but pseudo-inverse will come to the rescue. So oftentimes these matrices are not full column rank of full row rank. Likewise, in the wide case, you can have rank which is not less, not, not exactly equal to the number of rows, right? It's a trap full row rank. When it has full row rank, we have a formula that we don't have full row rank. What do you do? Well, this is where this comes to the rescue. Very powerful. In fact, this is a real problem that was facing people doing least squares in the previous century. They actually realized that this is the way to do it. Okay? So it's actually a practical problem. Okay? The last lecture we looked at, I'm just recapping what we did in the previous page. So we had a equals. I'm just rewriting here what the pseudo-inverse is for a. And we showed that x was a dagger. Why? I'm just a reminder that when a is square, when a is square and invertible, that is m equals n equals r, then you want the pseudo-inverse student nurse to become a inverse, which it does. Likewise in the tall skinny case of least-squares, when m is greater than n, as shown here. If you have full column rank, that means if the rank is n and rank is equal to n, then the pseudoinverse becomes your familiar least squares formula that we have seen so many times, which is a transpose, a inverse times a transpose. So we will go through that in a second. But I just wanted to tell you that the least squares solution formula that you know is recovered as a special case of the pseudo-inverse applique operator when n equals r. And likewise, when we did the control, we saw that if you want to go from a to B and there are many ways of going, what is the most energy efficient way of going? The minimum norm solution. Remember we did this several lectures ago. Well, that's the case when n is greater than m, right? And you have infinitely many solutions because you have fewer equations than unknowns. So you have millions of infinity of equations of solutions. Which solution should you pick? You should pick the one that has minimum norm. That's what we said. And if you have full row rank, meaning that m is equal to r. So if, if the rank is equal to the number of rows, then you have a formula. We derive that formula. A transpose times a transpose, a inverse times Y. Award. If that is not the case, then you can still do it. You can still always do a plus times y. This always works. Okay, here is kind of a rundown on. But before we do that, I just wanted to remind you in this box here that the columns of Q are orthonormal number. We did this last time. If you do Q transpose Q, you will get an identity. But if you do QQ transpose, you get a projection. Now just to see that more clearly, let me go to the board. And so I'm going to the board now. Maybe we'll do it here. So take the example of q equals 100. I think we had done this example last time. So q is three by two and it has orthonormal columns, right? So what is Q transpose? It will be 100010. What is Q transpose times Q? You have to multiply this on the left, chevron Q transpose here. But you can see here that you will get 1001, which is the identity, right? Multiply 100 on the left by 100, you get this one. You'll get 010. So Q, Q transpose Q will always give you the identity. But QQ transpose, what does it give you? That's not going to give you that energy and less q squared, q is not square. Q is three by two. So I said the QQ transpose gives you a projection. How do you see that it's a projection? Multiply Cuba Q transpose. What do you get? You get? So q is three by two. Q transpose is two by three, so you get a three-by-three. So what are the entries? Anyone? What are the first row? 100, second row, 010, third row is 000. So you have a third row of zeros at the third column of zeros. So if I were to multiply Q, Q transpose by system, let's call it u. Okay? What do I get? The US has to be a three by one vector, since QQ transpose a three-by-three. So if I were to multiply it by u, let's call this u x, y, z, right? You're going to get u, x, uy and zero. So what does that operator that's like taking your u vector, which has x, y, and z coordinates, and projecting it on to the XY plane, where z is zero, because the third component is zero, right? And therefore, it's like taking this as a projection onto the column space of q. Okay? So just wanted to remind you, this is a, this is a really important observation that will be useful for you in all the things that you are going to study. Okay, back to the iPad. Any questions? So it should be clear. Okay, so now we mentioned that the least square solution becomes this. When you have full column rank. Did you guys do this in discussion yesterday or that you think? Alright, so let's go quickly through it. Just, I just want, I think you did, but let's go very fast. So here's the reason. So a is our sigma v transpose. And because V, V is full, column rank, R is equal to n. So I don't need to carry bigger VR. Kinda always indicates the fact that the V matrix is not fully, fully, it's not square. But because r is equal to n is full, rank. V transpose B transpose a transpose, just transpose this. So V comes first, then sigma R transpose is just sigma, because sigma is diagonal and square. And this becomes your transpose. Then just multiply. Eta is just algebra, but let's go through it. So a transpose times a is what? Firstly write V Sigma transpose a transpose. And then you write this as d a part. You are sigma V transpose U transpose times U R is IR. Faculty like we thought that Q transpose Q is high. So this part will become identity. And Sigma R and Sigma R gets multiplied. So it gets sigma r squared times V transpose. So this is e transpose times a. What is the inverse of that? Well, what is this inverse? V transpose inverse is just v, since v is orthonormal, v transpose equals V inverse. So we inverse first, then sigma r squared becomes sigma one over sigma squared because they are all diagonal entries. So sigma to the minus two. And v, If the inverse, but V inverse, V transpose a transpose a inverse is just v sigma r to the minus two power times V transpose. Now, you have to multiply by a transpose to the right to get our least squares formula. So a transpose a inverse times a transpose is, this is here, a transpose, a inverse, and this is a transpose from here. And so you see that v transpose v will become i sigma r to the minus two times sigma or to the plus one will become sigma r to the minus one. And then you have v sigma r to the minus one, you are transpose. But that is the definition of a pseudoinverse. Right? There you flip the order of V and U, and you put an inverse on the sigma, and you get exactly the pseudo-inverse formula. You recover the pseudo-inverse formula for the special case. Yes. This is for tall matrix with full column rank. The instant you don't have full column rank, this formula will not hold. That's the point we're making. Likewise, in the white case, that formula, the least square, the minimum norm formula only holds when you have full row rank. The instant row rank is not full rank. If not the row rank equal to the number of rows, you cannot use that formula, but you can only a few pseudo inverse. You don't care. If the way you know it is, then a is white or tolerate. When a is fat to get the minimum norm. When a is tall, you get the least-squares all in one swoop. Pretty cool, right? Powerful, powerful tool. And it's actually used. Can you speak a little louder? Which one? No. No. Okay. Sorry. Because we're not looking at this formula. We are doing this one now. This is the least-squares. This has this case, tall and pin. When it's tall and thin, then the rank is equal to n, meaning you have full row rank. Then that's why VR becomes V. Because V is R by n and r is n, So b is m by n. Therefore, all of this is valid. Now, you can do a similar thing for this at your exercise, okay? You guys should do it similar to what we did here. You can consider this case, the m is the shorter dimension, n is big. In which case the pseudo-inverse, doing exactly the same kind of work. Who should become this? When the rank is equal to m? Is that clear? Okay. If a is granted full rank, then you just do. They'll, they'll become the same formula. Okay? So you'll see that they will become the same thing. And in fact it'll become a inverse. Because then it has an inverse. And the pseudoinverse also recovers the full inverse. It's a one-size-fits-all, no matter what the size of the matrix. So it's a very powerful tool. Okay, that's all. And we are ready to embark on the remainder of our journey. That's the principal component analysis. So this is really, really useful. So pay attention if you were sleeping before, try to wake up. Alright, so the question is, why do we care about the SVD? I mean, other than the fact that you can do pseudo inverses and least-squares regions powerful enough. Why else do we care? So suppose you encountered a and it looks like this. I did SVD and it's an m by n, u m by m. And you have all these diagonal entries on sigma because it's m is greater than n. M is smaller than n. In this case, it's a wide matrix. So you see these singular values, right? And there are 1,905,051.50, 0.01, it never goes to zero. It's always some small residue leftover, always greater than zero, but never zero. What is the rank of this matrix? If there are no zeros on the diagonal, force rank m, exactly the smaller dimension m. That's what, that's what we discussed. We didn't, we talk about the other case. So the rank is gonna be m, which means to do the SVD need to the full SVD or the reduced SVD will become, as a compact FED will still have to carry along this anti M. We should think about cases where m's and n's are in the thousands or millions. That's the, that's the apps that we're interested in. Okay. There might be 1 million by 1 million and you still have to do 1 million by 2 million SVD, even though most of these singular values are zero, but they never zero. But the true rank is really 1 million. That's too much. So what do you do? Well, you can see here that these values are really small and they can be truncated. I'm not going to lose much by saying those are zeros in comparison to the big guys. They're hundred 90, 55, Daniel and the subordinates, Right? This is useless stuff. Whereas these three we should keep because they are significant compared to the rest. Everybody see that. So there's no point in carrying along this big pain that has no, not much value. And in fact, in most applications, the tail actually consists of noise. So you want to get rid of it anyway. So it's kind of killing two birds with 1 st by truncating it, you're also cleaning out the noise. So it's very powerful. Okay, so that's what the point is that when I'm zeroing out all those singular values from, say, after entry number three, what are we doing? We're actually identifying a lower dimensional structure that my data is actually lying in. Because this has ranked three, not rank 1 million as I thought. So we can identify the three-dimensional, low-dimensional structure that essentially captures most of the data, the bulk of the data. And that's very powerful when you're dealing with huge datasets as we are these days. So the point is that the underlying structure is low-dimensional. But this SVD can help you discover automatically what the structure is by doing this. So that's, that's the real value of the SVD. So this is also called the machine-learning parlance is called the unsupervised learning setup. So in supervised learning you have labeled, right, you have a dog and you label if a dog, you have cat images of dogs are labeled dogs, images of cats are labeled cats. And then you will be given query image, which is either a dog or a cat. And you have to, basically, your neural network will have to guess, is it a dog or a cat? That's, that's most of machine learning today. 98% of that is that this is not that, this is unsupervised. There are no labels. Nobody gives you any labels here. It's just finding it by itself. So it's actually even more powerful. So this concept of reducing the dimensionality, the dimensionality reduction is what is known as principal component analysis. So it's a, it's an application of the SVD where you just throw away the lower, the low singular values and singular vectors. So at the big picture, clear. Okay? So what are the uses? You want? Fewer dimensions or more dimensions? Well, depends on how much the dimensions are getting the value. As long as I'm telling you that you're getting most of the value by keeping only a small fraction of the dimensions than fewer dimensions are to be preferred. Fewer dimensions means less computation. It means less storage. It also means you're redundancy is removed systematically. And this is an interesting also benefit in that you can narrow it down to two or three principle components. These are called principal components. Then it's also easy to visualize. So you can take a 50,000 dimensional dataset and visualize it by, by compressing it or projecting it onto 2D. You can actually see where the datasets. And if you see clusters forming, then it has some value. Guys can see all these guys are different from those guys. But that's the idea. So you can also visualize using 2D and 3D Plots. You know, as we, as humans can only visualize two and 3D. I can't even do 3D. If you want more than 3D, maybe you have to be on some strong drugs. I can do it. But clearly, we are, we are incapable of visualizing more than two and 3D, but the PCA has the benefit of being able to provide visualization by projecting onto the, onto the right dimensions. So it depends on which two-dimensions you look at. The way you view it is the way you will get meaning from the data. And that's where PCA is powerful. Okay. It has, therefore it has better interpretability. They can interpret it. And the number one thing is that it helps automatically discover the key features in your, in your data are the patterns in your data and you can skip the rest. Okay, So more high-level spill. What are the application space is huge. I mean, I'm just named a few. So where do you find such huge humongous datasets everywhere today? So there are applications in health care where you want to predict a patient's health and how much the patient is susceptible to getting a certain disease based on their health risk factors, which are all part of the data. That's one app. There's also how do you want drugs to administer to patients in order to cure them? This is a huge trillion dollar business industry. In biology. There's a lot of excitement in curing cancer these days using computational methods. And the SVD and the PCA play a prominent role that you can predict which mutations lead to the bad stuff. Okay, based on, again, tons of data that you've collected from patients over there in the history. And then of course, more to the heart of doing Amazon shopping carts. You can, they can use it to predict which customer will buy which product based at historical data that they've collected. All of these fall under the purview of applications. The SVD slash PCA is really, really powerful. So you should have something not just for me, but this unit you'll take with you throughout the rest of your life. Once you learning the foundations of this is important that you understand why people do what they do. Okay? So what are the applications? More concretely, I've given you the landscape of the big picture. So it's clearly a data reduction technique, right? We have tons of data, but really the valuable stuff is only this much. So you can use it to compress. So it's used for image compression, but it doesn't, it's not used as is. But the spirit of it is used for doing image compression. In fact, you know how image compression works. It's based on JPEG, right? You guys know about Jaipur clearly. You take a picture and then store it in the JPEG format. Jpeg actually consists of what's called the discrete cosine transform. That's a, that's a variant of a Fourier transform. It's almost like the FFT, the Fast Fourier Transform, but it's a tweak on that. But that is something which is fixed no matter what the data is, meaning whatever picture I should. I'm always going to be using the discrete cosine transform to process the image and then compress it. What the SVD buys you is that it is something which is data-driven. So for a particular data, you will have a particular transform. For another data, you might have another transform. It turns out that in the image compression business, it was too much to adapt your transform for every image you take that wouldn't fly, that, that wouldn't scale. Therefore, they said, let's choose one that works reasonably well for all settings. And that's where the DCT came about. Just this is just for your historical knowledge. So this is more like a data-driven generalization of the Fourier transform, which is very prevalent everywhere in signal processing, tailored for specific problems. And this is used universally by the big tech companies. It's used by Google every time you click on something and you have a page full of entry showing up based on what you want your query. Its underlying that is a PageRank algorithm which uses the SVD. Okay? So that's under the hood. Likewise, Facebook, when you use face recognition software, is actually based on doing PCA on faces. So they take every column will be a face. So if, if, uh, if an image is M by N of a face, image is m by n, What they do is they make the image, the vectorized image into m times n. So basically the stack all the rows together, make it a huge column of the matrix becomes a big column. And each of these columns is a face. And these are the faces from the database. And then they do principal component analysis or SVD on this columns of faces To find the most principal component face and they call them eigenfaces. Believe it or not, there's a term called eigenfaces. Okay? You can do Eigen in front of everything. Eigen, whatever means that they're doing this. Okay? So then why is that useful? Because you can use that to represent a new face comes in, you just projected onto the eigenfaces that you have already collected. And you'll only need a few coefficients to store that new face. So that's how e.g. in the face recognition business, facebook works. And then netflix of course gives you recommendations for movies. So we're gonna go through that in some depth, in depth later in this lecture. But basically, the people know about the Netflix price. How many people have heard of the Netflix price? Nobody. Okay. Well, because you guys are too young. But it wasn't that long ago. But maybe ten years, 15 years ago, something like that. Netflix gave a challenge. So what they did was they wanted to know how to recommend movies to users. And so they gave a contest where they have a matrix of users and movies. And of course now not every user has viewed every movie. So whenever the user has viewed a movie, the user would give a rating for it, right? But there'll be a huge matrix with a few sparse entries that are populated. And the challenge was, can you use these entries and complete the full matrix? It's also called matrix completion in the literature. And that's not what we're going to study. But that's another technique for doing matrix completion that predicts, if you like this movie, you will also like that movie, right? That's how, that's how Netflix make money, right? Recommending movies to you. So they fill the full matrix based. Can you come up for they knew the ground truth. They had they had the data before. They knew for a particular setup, I don't remember the size. And then they made an open contest to everybody. Can you guess what these users like on the movies that they hadn't seen. And whoever came first, we're getting 1 million bucks. And in fact indeed, the people who, who came up with it use some combination of techniques. Of course, neural natural always involve these days. But even though this was 15 years ago, even then there was the ordinary neural nets around them. But just to give you the flavor. So today we will study a case where we are given the full matrix. When you don't have to complete anything already been done by the first part. And now you want to use this to predict if a new user comes. Which user is this user most typical like compared to the user's already in the system. Again, you want to reduce the dimensionality. We'll get to that in a minute. But just to show you that the application space is huge, and this is one area where linear algebra meet money. So if you want to make money, understand this, and get creative about how you use your knowledge. Because there's money to be had, because others have done it. So that's the example. If money motivates you then pay attention. It's also simple and interpretable, and of course it's scalable. Otherwise Google wouldn't be around, right? Everything Google does this at huge scale. Okay? One caveat is that you will often see terms like correlation and covariance. These are statistical terms of probability terms that you will understand best after you take probability classes, I would recommend 126. As far as the class to take an upper division class. I do teach it from time-to-time. 70 is of course, a gateway into one-twenty-fifth. That also covers some of the basic concepts. But for 16, we will not assume any knowledge of these things because you guys are not supposed to know probability. How many people have taken probability. Okay. Not too many, but that's what I suspected. Okay, so moving along, let's do a low-rank approximation. So we're going to take this big matrix, as I mentioned, and we're going to reduce the dimensionality. So we're going to approximate it using a low rank approximation. So the idea is that you're given a high rank matrix, m by n and m and n are huge numbers typically. And R If approximately the Min of M&M, depending on the size of m and n. As I mentioned, the rank will be very close to M and N because of noise. Because noise will prevent exact zeros. They won't be zeros. So in the singular values, there will be some small nuisance numbers. And you want to form an approximation. However, as we saw in the example here, the actual rank is three. I mean, the effective rank of three. Even though the actual rank, if you go by the letter of the law, is much more than three. So we don't we don't want to worry about the letter of the law. We speed all the time, right? So will it be just want to take the three top guys? Okay, so now let's take the case just as a numbers game. Suppose it is a square matrix just for ease of numbers. So m is n equals r. And let's say if 10,000 by 10,000 is the size of my, of my data matrix. And let's say that there are ten insignificant components. So namely, there are ten significant singular values which are much, much bigger than the rest. Then what are the numbers we're talking about? So this is what we would do. We're going to approximate a as the sum of the top ten principal components. Each of these is called the principal component. So you take sigma one is the largest one. That's why we ordered them. Remember, we did the SVD. We said sigma one goes to the top. Sigma two goes to the second. The reason we did that for this moment, so that we can take the top guy is the most important one. In many ways. What we're doing here, let me go to the board. So, so we are saying that a is approximately sigma i, u i transpose I equals one to L. And L is some small number. And the way you should envy, of course, we know that sigma one is greater than sigma two, greater than sigma three. So these have, these are matrices of rank one each. And they have an ordering to them. It's not very different. And if you want to make an analogy, let's say that you have a fraction 0-1. Okay? So suppose you have a number which has an infinite number of digits, safe. Take Pi, e.g. a. Fight never ends, right? 0.3, 1417, blah, blah, blah, blah, blah, keeps going. But we don't, we can't read forever. So we want to say, hey, what does that give me the significant digits of pi or of this number? What is the most important digit? This is 0-1, that's in the range 0-1. What's the most important digit if you had to give you? If I told you you can only have one digit, which one really why? I wanted to say here, think of each of these as a matrix. Now, of all the sum of all of these rank one matrices, which one would you want? The most significant one, which has dominated by the sigma one, sigma one of the biggest. So please give me the most significant component. This exactly, they're good analogy. If you want to intuitively think about why we do PCA by definition. Because if I give you a bunch of rank, if I have given you a rank one plus rank two plus rank three. And of course, you also have to pick the UIs and VI that go with the sigma I carefully. They have to be pointing in the right directions. When you do that. The first component is the biggest chunk of information, most informative component of all of my sum of all of these zillion components. He's the top guy. You'll see how we'll go. It's not just a sigma, sigma, this is just one of the three things, right? Sigma i, u, i, and VI all go together as a package. And the sigma I is the biggest number. So once you eyes and the eyes are normalized, the highest sigma I assumed the highest important. Yeah. Yeah, you could think of it that way. Yeah, Exactly. Okay. So yeah, let's do some numbers. So that m is n is ten to the four. Then what is the number of entries? Suppose I wanted to store this for compression sick. Then he has ten to the eight entries, ten to the four times ten to the four, right? Now if I told you that there are only ten significant components that really mattered here, right? Then I should use, okay, Then, then what, how much do I end up needing to store? Well, UI and VI, or 10,000 entries each. For every UI and VI you have 10,000, right? Because the 10,000 by 10,000 vectors, so UI is lengthened u1 if 10,000 long, U2 is 10,000 long. So u and v together, or 20,000 law, because you need to store both. You also have a sigma which I ignored here. So you have ten such things because hell was ten. So if you multiply it, you have to store 200,000 of the order of 200,000 NT entries, as opposed to 100 million. Instead of 100 million, you need to store only 2,200,000. That's a savings of 500. If I did my math right, I might be wrong, but it may be one-zero, too many or too few. But generally you get the picture right. So you get huge savings by not having to keep all this data. And we're in the age, we are generating data by the gazillions. So we cannot store everything that we are creating. You need to be very smart about how we do things. Okay? So now. There is interesting image application, image compression application in your note, Note 15. So I don't know how many of you have looked at North 15, but at the very end of node 15, there's a cool app. This is one of the TAs friend's cat Snyder. Okay. So this thing is 4,032 by 3,024. That's a high resolution image. And it has R, G and B, red, green and blue components. So there are three times 4,032 by 3,024 is the original size of the image, right? Because there are three channels, RGB. Now, here's what's happening. If I were to use my PCA and truncated, where I keep only the principal component either. I can't see much of anything in that at two little dots to show you what should I pick L? That's a good question. How much is too much and how little is too little. But clearly one is not doing the job. I don't see a cat at all. To we kinda start to see it's kind of a fuzzy picture and you can see the outline of the cat. And that's when L is five. So if you keep the top five principal components, you get that. When you go to ten, it's not a great resolution, but It's very clear that's a cat and not a dog or a zebra or whatever, right? So that is already there. And then as you keep going at 25, you get this picture. At 50, you get this picture. And at 100, you'll be indistinguishable from the origin. So if you look at the size of the image and it, even if you were to say I had to keep top hundred principal components, that's completely it's a game changer because in terms of storage, you can go through it. The original entry is 36578304. And I drank 100. You will get about 5%, okay? And this is doing crude calculations. So you have huge savings in terms of the data reduction. Now, more on this story, I think you should also read 6.2 of your nodes. They have an interesting case where they have students like you and assignments like midterm and final. And based on that, they're going to do clustering. You should read it. I'm not going to, I don't want to spoil the suspense if you haven't read it. Okay, So here are some typical plots that people do in this business. You have on the x-axis is the number of components, so that's J. We will denote the number of components you are keeping. And on the y-axis, we are plotting the singular values, but in log scale, because the numbers very quickly shrink. So I can't see both 0.0, 110 million on the same scale unless I do something about it if I take the log scale. So you can see in the log scale. So this is ten to the 510, to the 410 to the 310 to ten to the 110 to the zero. And do the typical plot where in fact it's even steeper than this. So it will be typically something like that. Where the idea is that you can see that the, the knee of the curve at the beginning you have really, really large singular values that corresponds to the first ten or 20 or 50 of the principal components. But then it very quickly decay is out to almost zero, but not quite zero, no matter how many thousands of components or left that you haven't covered. So this is a very typical plot of the singular value decay that we fall. So in the picture, I showed you the example with three, but this is typical of most datasets. And the other metric that is used when people ask how much, what should be. A typical kind of rule of thumb is that you should pick L such that you capture 95% of the energy in your system in your data matrix. How do you measure energy by the sum of the singular values? That's one way to do it. So here's the cumulative sum of all the singular values. So this plot is that I'm taking sigma one divided by sum of Sigmas. This is sigma two, sigma one plus sigma two divided by all the sigma. So it's clearly always going to be non monotonic, right? Because they're always including, cumulatively summing them up. And you can see that very quickly. It will spike up to having full coverage. So typically, even taking one or two components, you'll be hitting about 40 to 50% of the total energy in your system. Quote unquote energy. Energy in a loose sense. So these are typical plots and this motivates you to study the fact that I should truncate these guys. This is really useless carrying all these components. Any questions? This is just a qualitative stuff, marketing stuff if you will, but it kinda needed for this part of the class. This is why we are studying these things. And in terms of, we can say that we do heuristically, we're going to keep a few components and the rest of the one. And we can hand wave and say, you know, it works, it's good enough for me. But it's more than that. There's actually some foundational knowledge that is backing you up. And that is known in the literature as the Eckhart young theorem. It's a note 15, you're not responsible for it. Don't worry about it, but it is good to know what it says because it's giving you sort of theoretical backing of what you're doing. What is it saying? It's saying that. Okay, so first there is this helpful? Okay. Let me have to write some things. So he is exactly like it. If you want. I'm going to the board. This is a L. L because I'm keeping L components, right? So let me write it on the iPad. So he is I equals one to L sigma i u i transpose. So it turns out you can prove a theorem. And I, I think it is proven in Note 15. But if you really want to learn this, you should take a class like 127, which covers optimization. You study more closely in that setting. But what it says is, suppose I wanted to find the best rank one approximation to my matrix. What is that rank one matrix that best approximates the data I have. I know I have a data with 1 million dimensions, but this gives me the rank one approximation to that, to the best of your possibility, meaning the best rank one approximation. And how do I approximate it in the Frobenius norm sense people know Frobenius norms. I think you met it right? One of the homeworks are very loosely Frobenius norm of a matrix is just the sum of the squares of the entries of the matrix. That's it. Simple as that, just given a fancy name. So basically A11, if a is your matrix, A11 squared plus A12 squared plus all the way to A1 m squared plus A12 to Amazon. Some of tick, tick all the entries of the matrix square them and add them up. In other way of doing it as you're just vectorize your matrix and take the L2 norm. Tempting to take the first row and make a column out of it. Take the second row stack at below the first column. So make a huge column, which is actually a vector version of the matrix. And just take the L2 norm of that, of that huge vector is the same thing as the Frobenius norm. So in that sense, which is a very meaningful norm sense, it turns out that the best rank one approximation is A1, where you take the principle first principal component times u1 times v1 transpose. The best rank two approximation is one where you take A2, which is the first two principal components. The best approximation is the first of these terms. And nobody can be done. That's the best you can do in this sense. Okay, So it's a kind of formal backing of what you're doing. So just to kind of make things even clearer, just want to highlight the fact that if this is x, this is x Frobenius norm of this as equal to X1 one squared plus X12 squared plus dot, dot, dot plus x n m, m n squared. Okay? Just basically take the sum of the squares of all of these entries. Okay? Suppose you wanted to the best rank one approximation, all rank one matrices. So note that this is a very powerful statement because you're competing with every single rank one matrix of size m by n in the world out there. How many of them are there? Infinite. There are infinite many rank one matrices. And yet the theorem says, the best among all this infinite bunch is this guy. So it's a pretty powerful statement. So just wondering, appreciate the power of the statement that the best, the winners of rank one SVD decomposition sigma one, u1, v1 transpose. The sigma one is the first largest singular value of a and u1 and v1 or the other left and right singular vector. Let me think about messed up. The first singular vectors left and right respectively of a bestest in defense of minimum Frobenius norm of the error. There are other norms for which also this is true, but we won't go there. Likewise, the best rank two approximation is going to be when I take L equal to two sigma one u1, v1 transpose, sigma two u2 v2 transpose. And that'll give you the best rank. L is going to be sigma i, UIV I transpose might tell where the sigma i's are ordered from largest to smallest. Another reason that again, reminding you, this is why we ordered the sigmas from highest to lowest. Okay. Any questions so far? Okay. That's pretty easy stuff, right? Relatively speaking compared to all the stuff you've been doing. But hang on. There'll be some math given. Principal component analysis or PCA. Let's now, let's, let's kinda roll up our sleeves and do some analysis. Okay, So we're given kind of a high-level picture, justified things. I mean, we didn't visit the Eckert young theorem, which is a profound theorem, but we didn't do anything other than stated why it works this partially we can we're going to address it either today or in the next class. So the principal components that is lower-dimensional structure. So what I'm doing here, I'm visualizing because as I mentioned, we can only visualize 2D. I can visualize only two D. Some of you can do 3D, NO. But these are some data points that are coming from my dataset. So because it's Tuesday, I can plot them x comma y, and this is an R2. Now. But you should really, you should see this. But you should imagine 1 million dimensional dataset or a 10,000 dimensional dataset, which we cannot see, but it'll be a glob of data. And I don't know what to make of it. This is typical challenge. We had to mine that data and extract the information in it. That's our job. Even in 2D. This is kinda, this is meant to represent that it's all over the place. I don't see any structure in this thing. It's kind of I don't know, I can't make anything out. But if you can imagine if the dimension was even larger, even 30 or 40 dimensions, it'd be a mess. There's nothing you can visualize. On the other hand, if I told you the ground truth is that the data actually comes from 1D. In particular. Maybe a nice case where it's a beautiful line going through the origin, which is one-dimensional. And these are my points that I observe in my data matrix that I can not only visualize, but I can say I can tell you anything that's going to ask me any question I can answer it because the structure is so simple. But life is never this easy. It's never going to be the case that for our analysis purposes, we can say it's a 1D line. Let's assume that in reality, what will happen is you'll get something which is almost 1D. So this picture is meant to depict the fact that these lines are not as all over the place as the green the green dots that I showed earlier, right? It was a green and it's also blue or pink. These pink dots here, I kind of have less structure, loosely, loosely speaking, than this picture, right? Because you can almost see this dominant kind of 1D hidden dimension that most of the data is following. But there is some variations around that line. In this dimension. There's some up and down. And some of this could be due to noise, and the other could be due to the fact that your data has not truly one-dimensional. Because that's kind of more of a theoretical setting in most cases. So the ambient data is 2D, so we look at these blue points, they are really in 2D. But if you put a gun to my head and say tell me read from, I'll say it's one. It's this one D. Alright, so that's the point. Now. So the data lives mostly in this 1D space. Now, suppose we have 2D points which are XI, YI, 1, 224-364-8510 have seen this before. We saw this example many times, 34 times we have, at least in the last few lectures, we have been seeing it. If you remember, we have seen this in the context of SVD. And this example is exactly this. That's that line. That's the one to see this as. This is one, this is two, this is two, this is four, this is three, this is six, and so on. That's where this data is coming from. Okay, that was just an example. C12, 2436 should use this. So if you recall, we did the SVD for this and we showed that this was the full SVD. And indeed the source in one d sigma one represents that the weight of the first drank component and the second rank component was truly zero because I really had a 1D system. I had a 1D line. In practice that won't be practices might be 0.001 given. And that will destroy the 1D structure in theory. Okay? And then we also looked at the compact SVD. And just as a reminder, I want to keep reminding you as much as possible so it sinks in use. The UIs are the basis for the column space and the v i's are a basis for the row space, right? Just remember that all the time, but could be using it many, many times. So in other words, if you look at un, un is proportional to one too, right? If you look at the matrix, they're all scales of one-two. So you can see that the UI is our spanning the columns of a kind of a visual proof of that. And likewise, if you look at the rows 12345, and if you look at V1, indeed that's proportional to 12345. If you have to take care of the root third 23s and root 46 is, and so on. So that will always be the case. So just remember that you either basis for the column space and the V, either basis for the row space. And x comma y in column space away if y equals to x, That's exactly what this picture is showing. Okay? Now, now let's get down and look at a real example. So far been a fluffy lecture. Mostly motivating things. Give you did, did all this, did this. That is okay. Okay. Now we come to the fun part. Not that so far hasn't been fun. We're going to generalize the PCA concept from the toy example we just did to the movie recommendation problems. So it's an actual app. You mentioned Netflix had this contest for movie recommendations. So let's look at this example. So this is an example of you have 1,000 users. Users are you and me, viewers of videos and videos of the movies. And the reason I used users and videos, or because he gets u and v exactly. Because I want to match them to the u and v in the SVD. Okay, so that's why it needed some creative thinking. Otherwise, I would have said people and movies, and the SVD is not P and M if u and v. So anyway, just a joke. So users happily start with you. So this is user one, user, two, user, three, user thousand. We have a sort of quote unquote modest sized system where you have 100 movies and thousand users. And on these entries here, I've marked the ratings that these users have given for these movies. Either they gave it at the time they were asked. Some other engine inferred that disorder ratings. As I mentioned, based on this Netflix contest as a sort of a completion aspect to the problem that we're not going to worry about that. So we're handed this matrix and your job is to see what are the principal components are the same. So what is the value of this? So if you look carefully, you see that user one and user two are pretty similar. And I only showed you three entries because they rated the first video roughly in the high level, 75, 80. The second video in the lower range, 32, 20. And the last video they really loved at 1995, right? So whereas user three almost takes the opposite view. So you'd say that user three cannot be clumped together with users 1.2. Now, let's say a movie comes along, which I know that user1 lakes. Then what would you do? You would recommend it to user two. Since user2 is very similar to user one. So you see there is a clustering going. That's the dimensionality reduction that the SVD is after. And maybe it's because user one and user two both love action movies. So it's possible that video one was an action movie and video 100 was an action movie. Whereas video to watch a romantic movie, which they don't care. But user three loved romantic movies. So user three loved that. That is a kind of a narrative around what does entries are. And we're going to label q i j as the rating of user j for video, I guess so that's each of these entries is q i, j. And as I mentioned that 100 videos and thousand users who rate or score the movies. And the goal is to learn the underlying low dimensional structure. So the point here is this in general is not gonna be 110,000, you're gonna be millions of users and hundreds of thousands of movies. But if you wanted to keep that size, it's not scalable. You know, you're not gonna be able to do anything useful with it. But the reality is that even though there are 100 million people or whatever, whatever your population sizes of your testing. In effect, there are ten kinds of people. There are ten types of people, not 100 million. You are one of those ten types. I'm going to classify you, okay? And then I'm gonna put you in that class. Likewise, even though there are a hundreds of millions of movies, that really seven kinds of which kinda movie Will you like? There are only seven types that one can identify with this that I'm grossly simplifying it, but gives you the picture. So I'm going to, of course, as I mentioned in practice, there are millions of these things. I'm sorry, I skipped up on me. Okay. So the goal is to learn the structure. And the goal is to, as I said, understand different types of movies. We're going to use this to recommend to users. So the point of this is that a user will come along. And if I have some attributes about the user, I can recommend the right movies to that user based on what I learned in the low-dimensional structure. Okay, so we're going to simplify it even further by saying that every movie has four attributes, okay, I know it has more, but for our purposes, we'll say it's four. It's kinda reasonable. What are the four? Well, I just picked action. I just want ABCD so I couldn't find a good one for B. So I used blood, blood level, godliness, horror. So a is for action, b is for blood. Fees for comedy, and D is for drama. Okay, So I'm saying that these are the four types of genres of movies that are around. And every movie has an attribute where it has a certain component attributed to action. So how much action in this movie will be a sub i. How much blood is in this movie is b sub i, how much comedy is c sub I and so on. So that's the attributes. And users have sensitivity functions to users, or have affinity for certain kinds of movies, right? So if a user loves action, so that means F is the sensitivity. I think that's what I Not yet. Okay. Now words the user j has also four numbers corresponding to these four attributes. Sag captures how much the user likes action part. How much FB j corresponds to how much user J likes blood, CJ, how much user j, Let's comedies and so on. So what is a reasonable model for the score, the final score that we are going, that he's going to put down the user j is going to put down corresponding to movie i is going to be the weighted combination of his affinity for action times the action component in the, in the movie. Affinity for blood. How much blood was in that movie? The comedy affiliation compared to times, comedy content and drama and Rome. Okay? So this is a model for what the user j is rating movie I. Of course, nobody is giving you SHA, BJ, CJD. Nobody tells you that you have to infer these things. In fact, you don't even need to know exact numbers. But the point is that this is a model for representing how I think this matrix is represented, as I clear. Okay, now, here's an example of a particular user J, who loves action. So these numbers in blue are the user j's sensitivity or affinity for that component of the attribute, right? So he reached actions as being AT, hates blood to zero, likes comedy and doesn't like drama. So, but this is kind of gives you a relative numbers. How much he likes. It doesn't matter. These are just made up numbers, right? He doesn't know and he won't tell you. But this is what we feel. It's kind of like he's doing these recommender systems or shrinks. There, they're inferring. You must like comedy, even though you don't know it. They're telling you something you may not even know yourself. And then these in red are the amount of action, amount of blood, amount of comedy, amount of drama content in movie, I. So this says it's about 20 per cent action, ten per cent blood. No comedy, 70% is drama. When you synthesize everything together, you get a final score of 30. So that's how this 30 was populated as an example. So underlying it what these latent factors that we're multiplying. But of course, the system doesn't know it, the algorithm doesn't know it, and you need to discover it. So it's pretty cool, right? I'm gonna discover the principal component, meaning that I'm going to find out what is an Eigen movie and what is an Eigen user? What's a principle component users, what I'm calling Eigen user. Okay, so that's what the system will do. Five more minutes. Okay? Now, I'm going to have to do some work to find out these principal components for this setting. Again, remember that? Now let's consider the outer products. That's why we did outer product extensively to date. So we are going to take the outer product of the action vector. What is the action? Okay? What is the action vector? Action vector is the ACT scores of the action content of all 100 movies. So there's an action. A1 is the amount of action movie one, A2 is the amount of action movie two and so on. And since there are 100 movies, it's a vector of length 100. What about essay? Essay if the user side, how sensitive is the user to action content in the movies? So SHA1 reflects how much affinity user, a user one has two, ah, has to action user HESI A2 is the affinity of user 2.4. And it seems there are 1,000 users in this example. So f is a, f a is 1,000 length vector. And likewise you'll have for Sb S, E and F d. And here also like, just like he is 100 length vector B, C and D will also be 100 length. Okay? Okay, good. Now let's form the outer product of a and S. S a, a transpose a is on the columns, SAT on the rules. What do I get? That's exactly what I get. I have a outer product of 100 length column vector times 1,000 length row vector. And you get this, right. Just multiply this through A1 times one times S, A2 dot, dot, dot here one times 1,000. Then you have here two times SHA-1 and so on. Hey, 100 times. What is the rank of this matrix? One. Select any old, any outer product with one vector, one column and one row gives you a rank one. So this is a rank one matrix, right? Where else have we seen? Hotel products and ranks? You've been studying it all along, right? So this is Q. First. The way we model q, which is the matrix, is that we are postulating that Q consists of four rank one matrices, which are a times a transpose plus Beta, because in our model there are only four attributes. In reality there'll be more, but this is an example. There are just four things that underlie the Q matrix that we saw at the beginning that they encountered. And they comprise a times a transpose plus B times S, B transpose, c times c transpose and d times as well. Where else have we seen outer products? In the context of the SVD? That's why these are users. These are videos. Okay, good. This is the rank, our decomposition of my SVD. So you're gonna be given a Q matrix. You have no idea of essays as these, all this junk, right? You're just given the whole matrix, those numbers. What are you gonna do? I'm gonna do an SVD. What will the SVD reveal? It will reveal a decomposition like this. Now, does this mean that the UI is the same as a true that let's say the action was the most popular part of the four attributes. Does this mean that u1 is equal to a and U, U2 is equal to b, and U3 equals c and U4 equals d. What do you think? Not necessarily because we don't know the a, B, C, Ds, right? We don't know that. All we know is from the data. I can tell you what is the most popular or prominent component. The first component is say I wanted to do it very well. May be in fact, will be some kind of blending of action, blurred comedy and drama that the most dominant user will be probably 60 per cent inclined towards this, 20 per cent towards that, 10% towards that. It's not like 100 per cent of action as the first one. It could be, that's a special case. But in general, these are not going to match. But we don't care because they will allow you to discover what is the underlying structure in the big data metrics. Okay, I think we should be running shoe. I will pick this up next class, but it's Niemeyer on alright, Nguyen think NEMA. Okay. 