Okay, it's time to get the show on the road. Welcome to the penultimate week of semester. Next week is the last week. Can see the end line is in sight. So hang in there. The usual announcement that you have extra credit for coming in, which I see many of you haven't. But I guess if you make the deadline, you'll make it. The other thing is I think the there's a lab design option also for extra credit that you should check on edX, okay, There are some suggestions for how you can get extra credit. And mostly it's, it's a design contest. So the winner will get booleans of points. But the details aren't med, check that. Last time we had looked at the SVD, we had analyze the algorithm, justified it. And we had done it for what's called the full SVD, which is the form of U sigma U, Sigma V transpose. And we also talked briefly about the compact SVD, which we will talk a little more about today. Which captures the number of non-zero singular values are and writes, the decomposition in terms of r. And the key idea here is that in the full SVD, your u and v are square matrices and track their ortho orthonormal matrices. And sigma will be almost diagonal, but diagonal to the extent that can be. But it will sort of mimic the shape of a Sigma and a have the same shape. But in the reduced SVD, you AR and VR transpose are actually rectangular. But sigma r a square because it's all the non-zero singular values. And so it's an R by R matrix. And we will, we'll talk about that. So that's what the plan for today. We're going to recap the SVD algorithm that we met last time. And then I want to introduce you to another form of SVD that is actually used. It's the most useful one, which is the outer product view of SVD. And show you an example of that. Then I will talk a little bit about the fact that SVD has a beautiful geometry. You want to, those of you who are geometrically inclined will hopefully see the beauty of the SVD from that viewpoint. And then a lot of what's coming forward. Both this class and mostly next class. It's about applications of the SVD. You've invested a lot of effort into learning this thing. It's a beast. We've mastered it. Now. Time for payoff. Now, how do we exploit the fact that we know all the stuff? And first, before we get into the true applications, we'll talk about the pseudo-inverse. That's one of the actually applications to that's kinda why I listed there. And then most of the principal component analysis stuff we will do in the next class, but I want to introduce you to it now. And that's probably the most useful application of the SVD, which is used heavily in machine learning. So you cannot do machine learning without PCA, least-squares and PCA because the only two dominant things. Okay? Any questions? If not, let's embark on our journey. So this is the SVD procedure from last time that we can go through. And again, we are interested in decomposing the matrix a, which in terms of u Sigma V transpose, right? Can never forget that formula. A equals U sigma V transpose, C equals U Sigma V transpose whatever contexts might be. The first step is to compute a transpose a. So immediately you take the rectangular guy and make it square. By making a transpose a. Now you're in square land. So all the eigenvalues stuff comes through. So in fact you compute the eigenvectors of S and you write that as v. So the orthonormal, of course you know that the eigenvectors of S are orthonormal because S is symmetric, use Vi to populate the V matrix and the SVD, although rate of V transpose. So you take the VDU transpose it, and That's the third entry of your decomposition, V, V1 to VM. Our correspond to the positive eigenvalues. This is for the case where actually I should, I should correct that. What is the, what can somebody tell me what is the right set of basis vectors for the positive eigenvectors corresponding to the positive eigenvalues. Given that, we are assuming that the rank of C is R. Yeah, exactly. So this is not correct. So let's, let's make this. V sub D1 through VR correspond to the positive eigenvalues. And they correspond to the Eigen. These are the eigenvectors corresponding to the eigenvalues lambda one through lambda r, which are written in order form from largest to smallest. And the last n minus r eigenvalues of a transpose a or zero. Okay, That's the assumption because that's what the rank of RS. Then you're all set. I have my v is in place, so I have V1 through VR. How do I complete Vr plus one Vn for if the rank is r and hardest less than n. Time for recap. Anyone? Yeah. Yeah. Using Gram-Schmidt. Exactly. You, you you extend the basis using Gram-Schmidt. Whenever in doubt, use Gram-Schmidt if you have to complete the business. Okay. That's a good rule of thumb. So that will give you these guys, they're all zeros, but they'll give you the VCE from r plus one to n. Now you have the V's, so you have one-third of the story actually have two turns of the story because the sigmas are just square root of the Lambdas, right? Sigma i squared lambda I. So the only thing left is the use. And that is given by the relationship that we started out with. The wish list that you I times root lambda I is eight times VI. And that gives you the use. And you plunk that there. And you are done. Of course the sigmas are sigma i's and the sigmas are the square root of lambdas. So this is just a review of what we did last time. And that leaves you with a equals youth sigma v transpose. Any questions on this? Everybody? On top of it. Any doubts? No. Okay, good. Alright, so here I've summarized the SVD and written it in terms of we elaborated a transpose a. We started with a transpose a and we did our Eigen decomposition and then we derive the V's from the eigenvectors and then the US from the relationship that you as sigma IUIE, whatever the UI is, one over sigma i ABI. That is the left corner. You can also start with a, a transpose. Equally valid. A transpose a is square and symmetric. Aa transpose is also square and symmetric. They are equally. It's equally game to start with a, a transpose as your starting point. And then you find the eigenvalues of a transpose. Now treat aa transpose as though it was a transpose a on the left-hand side. And again, all of them will be strictly positive. And the remaining, of course, AA transpose is what dimension? A transpose a is n by n. A transpose a is m by m, because you've got to flip them. Because a is n by m. So a transpose is n by m. Therefore, if only are of the singular values are going to be non-zero, it's the same as here. There's not a different order. But if a transpose a has our assets number of positive eigenvalues, so we'll AA transpose. And so you use those guys to be the positive guys. Again, you order them lambda one greater than lambda two and so on. And the rest will be zero. And now in the left-hand side, you were deriving use as a function of V is the risk came first and they use came second. In the AA transpose, the US come first because aa transpose will give you, we'll give you the use. And therefore, once you know the use, you can derive the v's by doing a similar relationship as mentoring the left-hand side. Only thing is that it's the same sigma i because the sigmas are the same for both decompositions. But it now becomes a transpose, transpose the whole world. The world is seen through transpose flips. Okay? And that's it. End of story. I just wanted to make sure that you don't believe that a transpose a is the only way home. There's another way home to a transpose also works equally well. Is that clear? Okay. So which one should you use for a given problem? So I want to, want to find the SVD for a given a. Should you use a transpose a or AA transpose? Depends rate. Depends on what? Primarily it depends on the size of relative sizes of m and n. If m is much smaller than n, which way would you go? You always want small stuff to work at, work with. You don't want huge matrices, right? So you always want as small as possible. So if m is less than n, which side of that? So m, m less than n is, so n, this is n, this is n smaller than M. So we can see here that this is m, this is n, right? So if M is the smaller dimension than AA transpose is the way to go because m by m is smaller than n by n. I mean, it's a matter of taste. On the other hand, if a is such that so this is the the wide metrics. When a is wide, it's probably better to do AA transpose. But when a is tall and it's the smaller dimension, and a transpose a is n by n. So that may be a better choice if you want to decompose it, but it's not cast in stone, you can do either way. Sometimes you also want to look at which 11 of them may produce less off-diagonal terms. But that's just wanted to inform you that there are two ways of doing this. Okay, let's do an example. Now. Before we do that, I have a question for you. I already answered it, but We've done the SVD. Is it unique? Well, clearly, it won't be unique to within at least sign changes, right? Because I can make all the fun, make u1. If I flip the signs of U1 and flip the signs of v1, I get the same answers. So clearly, at least within sign changes your invariant. So the SVD can include those, but it could be even more. And that's the example that I want to expose you to. Take this example. Say e is 100 minus one. So let's, let's maybe use the board. I want to stretch myself so that's a. So let's begin with AA transpose. So we've done a transpose a quite a bit. So let's give it a break and let's do AA transpose. So what is aa transpose? A transpose is the same as a, because a is symmetric. So a transpose is going to be 1001. You can check it out, which is just a two-by-two. So what are the eigenvalues of a transpose? One-on-one, right? These are the eigenvalues. Lambda one equals one, lambda two equals one. Both are one. And therefore sigma i's are also one because the square root sigma one, square root of lambda one, which is one, sigma two is square root of lambda two, which is why what are V1 and V2 corresponding to lambda one and lambda two? You know that aa transpose v1 equals V1, right? So you multiply this by V1, that should give you one choice of B1. Yeah. Yeah. Hi, convince yourself this multiplied by 10 gives you 10. This material is 01, gives you 01. Clearly, V1 and V2. V1, V2, they're a good choice. What other choices do v1 and v2 hat. So very special example, right? So geometrically, V1 and V2, 10.01, right? What other choices work? Yeah. Yeah, you can also change the sign minus one. The S-I-G-N. Want to cut, not confuse with the SIMD. Yeah, the signs can be made ones and minus ones. That will work. What else could work? Yeah. Any two vectors that span R2. Yeah, but, but they also had to be orthonormal. So what is that characterize? Rotation? That's the word I was looking for. If I rotate this coordinate system any which way, I'm still going to get the same answer. So I can e.g. take let's erase this. These are the axes. So if I make this my, you should have said u and h and put this, you're not V01, but I made it smaller. This should be U1, U2. So if I make this 90 degrees, any U1, U2, and what are the coordinates of U1, U2. U1 can be cosine theta, sine theta. That is this guy. And new to the orthogonal orthonormal compliment of that, which will be minus sine Theta and cosine Theta. In fact, you can verify that this orthonormal basis is exactly a rotation of the standard basis by theta counterclockwise. Right? So what do you conclude? That v1. Of course, this is how do you, do you want? These are new ones. So V1 is gonna be C transpose U1, which is going to be cosine theta minus sine theta. And V2 is going to be a transpose times u2, which is going to be minus sine theta minus cosine theta. So the moral of the story is that repeated eigenvalues of a transpose, a, a transpose are another source of non uniqueness of the SVD. Okay, is that clear? So it's an example that shows there are many SVDs, not too many, but you can characterize them here in this example, there are an infinite number of SVDs for every value of Theta, right? Okay, so let's go back to the drawing board. And that's what we did here on the board. Now let's meet our example that we have C and we're going to keep seeing this example. So get to meet it and know it intimately. So very simple example, but it's got complicated. U Sigma V transpose is. But you can see here that That's my a. And we have seen this earlier. We did, we found the full SVD to be. This is the view, this is the sigma and this is the V. Sure enough, as full SVD it. As a reminder, you will be square and orthonormal v will be square and orthonormal sigma will be diagonal to the extent it can, but it will be rectangular and mimic the shape of a two-by-four sigmas, two-by-four in the full SVD. In the compact SVD, we get rid of all the garbage. That garbage is all the zeros, all the extraneous stuff that you don't need to. So in this particular decomposition, all these guys are useless. These things have no place. So I just take the first column of a and the first row of V transpose and retain the singular value, or the singular value that's on the top left because the second singular value is zero. And just as a reminder, the number of singular values is the rank of a. The a is clearly rank deficient because the second row is two times the first row. So the rank is one. That most rank can be two, because two-by-four by rank is one. That is automatically borne out by the fact that one singular value is non-zero and the other is zero. Okay? Now, and more generally, so this is for the example we saw, but more generally, just to hit the home, hit home the point again, a times V1 through VR because you want through your two Sigma, this is the compact form. So a can be written as you are sigma V transpose. Now sigma R is square, and U and V R transpose are going to be rectangular depending on our right. And a is rectangular. And the connection to the full SVD is that you can complete the basis of both these and use starting at R and ending at n for the bees, and starting at r and r plus one and ending at M for the use. And so this will be an orthonormal basis for R n. This will be an orthonormal basis for R m. Now sigma will now have a rectangular structure that it will be diagonal with zeros on the right. So this is the general connection between these two forms. And again, as a reminder, note that the roles, so the v's. Which are orthonormal basis for the null space for our n. So the non-zero eigenvectors, the singular vectors, left singular vectors, v1 to vn will span the column space of a. The row space, and the non-zero vectors will span the column space of a. So all this we studied last time. And we're going to use that in a minute. Okay, now, I want to go back, take a few steps back. And I want to give you the simplest review of matrix multiplication. It's as though we are meeting matrix multiplication for the very first time. Okay? So that's just an aside. So I'm gonna go fast through this. So there are, there are two ways of interpreting a matrix multiplication. So let's take an example. So a is the two-by-two matrix, abcd and B is the two-by-two matrix E, F, G, H. But what is the best-known method are probably the method you guys use. And I use this one to find the top left corner. What do I do? I multiply AB by EG, right, first row of a multiplied by the first column of B inner product. And that gives me this term. And likewise, for second row, first column, first row, second column, second row, second column. This is exactly what we do. And this is matrix multiplication using inner products. Because you're doing inner products. In our example, you have four scalar inner products. For scalars corresponding to four inner products because it's a two-by-two multiplication. And you always do the inner product of the rows of a by columns of B, row of a times columns of B. Okay? Here's the second way, which also works. It's called the outer product method. How many people have seen the outer product? Okay, then meet it for the first time. If you haven't seen it. You can also take the outer product. Now you flip the order. I take columns of a times rows of B. But they're not even matched, right? So if I take the column vector AC and multiply it by the row vector e, f. What do I get? That column one over a times row one of B, That's again a, C times e f. Then I take the second row of B of a, sorry, the second column of a times the second row of B. You will get the second big decomposition. Well, what does this multiply it through? There'll be a, e, a, f, c, ECF. And I color-coded them to show you that that's exactly the ADE AF c and c f, which is the first term of the sum of the, of the, of the a times b then the inner product way. And the second term will be composed of be GBH d d, d, d, h, which are the second column of a times the row, second row of B, will always be this way. Now, what do you get? You get instead of when you do inner product, you get one number. It's a scalar. When you do outer product, you get a matrix. But it has only rank one. So that's the key. Okay? So whenever you do outer product of a column times a row, you will get a matrix right? Column multiply by rho because your matrix, a row multiplied by a column gives you a scalar. So this is going to have exactly rank one. You can see here that the rows are proportional to each other and the columns are proportional to each other. So they can only have rank one. Likewise. So this is another decomposition where you multiply columns by rows rather than rows by columns. And you can express AB as the sum of two rank one components. So you can see here that this has rank one, this is rank one and a, B is the sum of them. Each component has rank one. So you can split them by ranked by rank, can take out rank one and then the next rank 2.3. And so now, more generally, if you have u times v transpose, again, column, column times row, right? That's what we're doing here. And if you have rank r of them, meaning if you're, if you're summing together are of such terms. It is exactly like doing a matrix multiplication of you AR and VR transpose. Where you are, is all the UI stacked up you want to reward. And V R transpose R all the v i's stack down because V transpose are in rows from top to bottom. So familiarize yourself to this because this is exactly column times row, column times row, column times row. So is that clear? This new for you guys. Know, how many of you have seen this? One too? Not that many people have still waking up. Okay? So this is a really important observation and decomposition because this is the way to actually represent the SVD in the most succinct way. Okay, Why? Well, let's look at the outer product SVD. So this is called the outer product SVD. So we have seen the full SVD. Svd. Now this is the outer product SVD. So what does that do? You always start with, let's say C is our matrix. We want to decompose u Sigma V transpose, right? So here are my U sigma and V transpose. This is the full decomposition. So what do I do? I know that r of the singular values are non-zero. That means the rank is r. So the goal is to decompose see in terms of our rank one components. So that's the outer product rule. So what do we do? You do first, write down the use the sigmas and the V's. We know that r of the sigmas are non-zero, are positive. So let's do this multiplication first. So what will you do? Sigma i's will rate the columns of this. Sigma is a diagonal matrix. So you get sigma one, sigma two, u2 up to sigma and sigma RUR. And then zeros are going to multiply these columns. So there'll be all zeroed out times this. So because I had to multiply three matrices, Let's do the first two first and then the, and then the next two. Now we multiply this. Now what do you do? Well, this guy here is exactly this here. Sigma one, sigma two, sigma RUR. And this is the zeros, right? Inherited here. And let's write these as v1 transpose, v2 transpose, v2 transpose. Now, even though these are matrices, this is, you can treat individually each of these as a scalar entry. It's as if that these are the rules of doing block multiplication in general. So you can view this even though it's a block, you can hear it as one number one scalar. Then what you're doing is it's as if you're doing the inner product of this vector, row vector times this column vector. Okay? Even though we're doing matrix multiplication, this is an easy way to visualize it so you don't get lost. So take a column, stack it, make it a scalar, quote unquote scalar. And that's this guy. And I've color-coded it. So sigma one, u1 goes with v1 transpose. Sigma two, u2 goes with v2, v2 transpose. So you never have v1 transpose. Multiplying sigma two u2 from another column two will never multiply row one. Column two, row two, column three, row three, column one, row one. That's the rules. This is following the rules of matrix multiplication. But this viewpoint is very useful as we'll see. So this gives you this beautiful representation. Now I can write the SVD as the sum of r, r being the rank of the matrix. Rank one components, each of the rankled. The first component is sigma one, u1, v1 transpose, sigma two u2, v2 transpose, and so on. So holder product form has the most efficient and compact for representing an SVD. Any questions? It's all clear. Okay, good. Alright, so where are we? We have done the matrix multiplication. Okay? Let's go do the geometry. Okay? This is okay. First thing is, yeah, let's do the geometric interpretation of the SVD. Okay, So this is your changing gears a little bit. So I've introduced you to three forms of the SVD, right? So let's make note of that. So we don't forget one as the full SVD. This is so these are the three forms of the SVD that we have looked at. And Now, let's look at the geometry of the SVD. Now. First fact is multiplying by a vector by an orthogonal orthonormal matrix. I should have said orthonormal orthogonal, NSA orthogonal and normal does not change its length. People comfortable with that? Everybody. Give me a thumbs up if you're happy with it. Okay, good. And here is the proof in case you are wondering it's a one-liner. So you want to say that the norm of q x, or the length or norm of q x is the same as the length of x, because q x squared is nothing more than the inner product of Q x with itself. And then this is just x transpose Q transpose times Q, Q transpose Q is I. So this is just x squared, literally one line. Okay? So multiplying a vector, that's why doing rotations don't do anything. Orthogonal matrix is actually a rotation, right? So if I rotate a matrix or vector, its length doesn't change. That's an obvious geometric statement. Second, if I multiply a vector by Sigma, so we're going down the stack, right? V transpose is alternate. This is the full SVD view, okay? So we have three views and each view has its own value. The full SVD D view is the one that is most is what we use to derive the SVD conceptually. And it's the most intuitive because it's the way things are. The problem with the compact SVD, It's very compact. But you lose sight of the fact that the UR is actually part of an orthonormal basis. It's like a shrunk version of an orthonormal basis. So it kinda loses some value in that sense. And also when we want to argue about spaces. And the outer product form is of course used for representing very compactly. Okay? Now when I multiply a vector by sigma one, sigma r, and we know that the ordered highest to smallest, what will happen? Well, the first component is going to be stretched by sigma 1, s one by sigma two, which is slightly less, sigma three even less, and so on. So if you combine all three, Let's look at the figure here. Okay? So let's look at this picture. I'm glad that I'm not using the board. I can draw circles like this. So here are the three operators. So a is U sigma V transpose. What am I gonna do? I'm gonna hit this guy with some vector. Let's call it. I'm going to multiply it by x. On the right, I'm going to hit it with an x. And what's it gonna do? It's gonna first V transpose, then it's going to see sigma, then it's going to see you. That's the effect of multiplying by a. If I multiply a times x, It's as if X is first encountering nu transpose, V transpose and then encountering sigma. And then because of the decomposition, alright? So what am I plotting here? So let's say that the v1, v2 vectors are shown here. And the U1, U2 vectors are shown here. But let's, let's go step-by-step. So if I multiply by, let's say that the input, what I am putting into the system, that is the vector that I multiply happens to be V1. V1 is exactly the right singular vector of the, of the, of the decomposition of a. So this is for geometric purposes. Suppose I hit the a by v1. What do I get? Well, I'll multiply v transpose by v1. And that'll give me v1 transpose v1, v2 transpose v1, v1 and v2 orthogonal. So you'll get 10. That means v1. After I do V transpose is going to become a unit vector in the x-direction 10, right? In other words, it got rotated. That's the same intuition which we saw on the example there. So all v's do is rotate. So v1 and v2 are both going to be rotated by this angle here. Now, v transpose just rotates or reorients your coordinate system. Now you're on the x-y axis. V1 is pointing there and V2 is pointing up. It's as if you're in the rectangular coordinate system. You're multiplying by sigma. What a sigma do. Well, sigma is going to stretch V1 by sigma one, which is larger than V2 by sigma2. Because sigma one is larger than sigma two. That's how we oriented it. That means the red vector, which is where we are. We started with this vector and then after the first operation, we became 10 and then we become sigma one. Sigma 10 is going to be stretched version of 10. Alright, This has done, this has done. The last one is going to rotate you again because u is also an orthonormal matrix. So this might be rotated by this amount depending on what U1 and U2 are. So basically, if you feed in V1 into the system, you're going to get out that one. So, as you know, matrices are linear operators. You feed in, submit a vector, it'll get out a vector. And the way it does it is it takes the V1 and mix it in that direction. And it's actually a b, this is a V1. It's exactly what we've written here. Now what happens if I had fed in V2? That means the blue guy would have been, let's use the blue thing. So V2 is input. V2 becomes 01 by the same argument, and then it becomes sigma 201 and becomes the blue guy. In general, any vector you have, there's gonna be a superposition, linear combo or superposition of sometimes something times v1 plus something times v2, because v1 and v2 are orthonormal basis for R2. So the component that is on V1 going to get stretched and rotated by that. And the component that is in V2 is going to get stretched, smaller amount rotated, stretched. And given that, so if you give some vector here, which is, let's use another color, not say green. So if this is the input vector, let's say, then you will get something around that. You have to work it out. The best way to visualize it is take the component of the composite vector in the red direction and the blue direction and follow them through nonlinear. So each one will give you a particular component and then either get stretched or not. Okay. So anyway, that's yeah. So we can like we wouldn't be able to if they have the same eigenvalues? No, no. It's not a, it's not square it right? So ache, it doesn't have eigenvalue. So first thing to remind us, when a is not square, it does not have eigenvalues of a transpose, has eigenvalues of a transpose eigenvalues, but it has singular values. Yeah. Okay. Repeat your question. The same singular values are the same as what? Same as each other. So like singular value one equals, oh, you're saying if sigma one equal sigma two, sigma one equal sigma two, this will be a square. Then there'll be the same, then there'll be stretched by the same amount. But in general, we are ordering them. So sigma one is greater than or equal to six. Especial case, sigma one can be equal to sigma two. That's true. In that case, there'll be stretched the same amount. Yeah, question. Not finding x only v transpose v1. V1 is a special vector I chose V1 to. So you can visualize the transpose times V1 will always be 10000 and the 2D case, it'll be 10v transpose v2 will be 01. So that's why that's the blue vector, whatever. Yeah. But in general, you take the component of x in the horizontal direction, in the b1 direction rather. And then you can always synthesized an x as alpha-1 v1 plus alpha2 v2, right? Then you can follow the path of V1 for the alpha1, v1, and v2 for the alpha2 v2 part. And then you add them together and that's the answer. Because it's all linear. Yeah. Question. Last part, can country just what? I didn't hear that rotate. Oh, no. Because remember the first rotation. Again. That's what this is a good visualization. Because you have, you need to have two rotations which correspond to the orthonormal basis, u and v respectively. And what we intuitively saying, the orthonormal basis, u, the, the, the U1, U2 up through you are span the columns of a span, either the columns depending on the size. So they are a basis for the 11 spans two roles and the other spans two columns. And the rows and columns are different for a rectangular matrix. So these rotations, I gave you a square matrix to visualize. Now if I, if I, if I gave you a to be a two-by-three matrix, then the rotations are, one is in 3D and the other is in 2D. So they're not the same rotation. It's not as if I'm if you were to do the die anyway. Yeah, that's that's the point. And the stretchings are exactly about, about encapsulating the singular values. That is, some of these basis vectors are going to be magnified and others are not gonna be magnified as much. So remember, sigma's are always non-zero, non-negative. So they'll always be some positive number. And V1 is the one that's going to get the biggest boom. And V2 will be the second and V3 there's a pecking order. Okay? So hopefully this kind of demystify some of the SVD stuff. Now, I want to look at the applications of SVD, but I buy applications. I don't mean, I mean in quotes, I'm not talking about something that you build a product about yet. Hold on. We'll get to that. Yeah. So the bigger picture that we will get to later is that you should have guessed by now that the SVD is really about dimension reduction, reducing the dimensionality of your datasets. And that's what we will get into when we look at the PCA. How it, you take a huge swath of data. And you can extract information from that data and represented in terms of a handful of components. So that's what the SVD is useful and that's how people use it when they do the PCA material get to. But before we do that, there is an important concept that we need to deal with, which is about pseudo-inverse. And let's revisit the least squares and minimum norm. Remember we had held that at bay least-squares, you guys know, like that. In your sleep, right? Hopefully by now, 16. You thought and decide again in 16 B, but you didn't have labs on it and so on. But the minimum norm solution, which you also looked at a few weeks ago. Also, actually it's a special case of the SVD. So I wanted to talk about them. And of course, the principal component analysis is coming not yet. So first, let's look at a to B U Sigma V transpose just the full SVD. And suppose for the moment that you have a square case where a is m equals n. And let's say it's full rank r equals m equals n. This is a special case. Then you know that a is invertible, right? Know two things about it. So, you know that a is U times sigma one to sigma n times V transpose. So what will a inverse B? A is, a is invertible now, right? Because it's full rank and it is squared. So what is a inverse in terms of U sigma and V? Sigma is like, Yeah, he's saying d Sigma U inverse, right? What is asking? What is a inverse? So I know that a is okay. Let's use the board. Need to wake up. So we know that this is n by n. So we know that I have to take the transpose Sigma inverse of inverse, inverse. But V is V inverse transpose. B transpose equals a inverse. B is orthonormal, That's just V. Sigma will be sigma inverse, and new inverse will be u transpose. Because V and Q are orthonormal matrices, right? So in terms of the SVD decomposition, that is exactly what you get. Is that clear? I hope this is not news to you. I'm just giving you a rule for inverting. Okay, So, but the SVD makes emerging easy. If I had the SVD, what would I do? The pose I, somehow, somebody gave me the SVD. You can see here, all I need to do is invert sigma. The diagonal entries can be inverted. One over sigma one or sigma one, sigma two up to one over sigma n. And then swap the order of v and u. U becomes v, v becomes u. That's the inverse. Pretty neat, right? Of course, the hard part is, how did you, who gave you the SVD and is in computing the SVD just as hard as computing an inverse, probably true. But we won't be, won't ask these questions because you're going to somebody I said somebody gave you the SVD. That's the assumption. Okay, then. But what happens if the inverse does not exist? When will that be when it's not for full rank? Then you can define a pseudo-inverse. A pseudo-inverse is exactly what it says. It's not really an inverse, but it acts like an inverse. So that's why sudo is called the term pseudo-inverse is just. Use to define it. So let's define the pseudo-inverse. That needs a little bit of notation, but it's actually very simple. So we're going to start with an m by n matrix. Let's use the red one. I think it's graphical. So we will start with the m by n matrix, and let's say it has rank r, and this is the SVD for it. U Sigma V transpose, where I have broken up the sigma into it's sort of the positive quadrant of non-zero diagonal values, sigma hours. These are all singular values on the top left corner, and then zeros everywhere else. And if you work it out, these are the dimensions of the zeros, okay? So this is a zero matrix of size r by n minus r. This is a zero matrix, m minus r by n minus r. I'm just, I just wrote them. So the dimensions make sense, right? Because we know that we need to have m by n is equal to m by m times the whole thing should be m by n, and this is n by n. So you can see here that this is n and this is m. That's how it's broken down. And you don't worry about the numbers. Just keep in mind that sigma has only a few entries, are entries in the top left corner and all the rest are zeros. Soccer, swimming in a sea of zeros. Okay? And by the way, the pseudoinverse has also, given the Moore Penrose, titled The Moore Penrose pseudoinverse of a is defined as the following. Basically, you take the v transpose, just like we did for the full rank case. So V comes first, it's no longer u, v comes first, it's n by n. And the inverse of a is going to be n by m. Okay? So that's the dimensionality change. So a tall matrix pseudo-inverse becomes wide. A wide matrix inverse pseudo inverse becomes tall. And you will have v n by n. That's this guy. The Sigma, exactly like here. You invert what you can invert, you can invert zeros. So leave the zeros alone. Right? And invert the sigma. Make sigma r to be sigma inverse because they're all positive. I can invert them and accommodate for the fact that this is now becomes, you also transports the sigma, right? Because this was m by n, this will become n by m. This is m and this is where, whereas this was m and this was n before, the sides have to change. That's why I'm plotting the size there. And then you got on the right, okay? So v goes on the left, Hugo's on the right and sigma gets flipped because a got flipped to get a pseudo inverse. So that's the definition of the pseudoinverse. But it's much easier to see in compact form because the compact form doesn't have any zeros other than the zeros and the diagonal matrix sigma, OK, but those zeros are unavoidable, right? So this is just to show you counting of the numbers of the zeros. But equivalently, you can write them as this in the compact form. So let's write this on the board also because this is important. Where is my chalk? And find the chalk? So this is the symbol. So you do a plus, right? Pseudo inverse. So this is a compact form. If this is the compact form SVD of a, then a, then a pseudo inverse is going to be Vr. Just flip the order. And remember that this is squared, right? And in terms of the outer product, we know that if a is equal to I equals one to r sigma i u times v I transpose the pseudo-inverse. Who wants to guess what the pseudo-inverse is? Yeah. You, this is UIV I transpose. What will be the pseudoinverse? You swap the order here to switch the order of u and v, right? That's what we did. There. Is that clear? So that's a pseudo-inverse and outer product form. Yeah. It's a plus T would be a transpose, right? Yeah. You wanted to yeah. It's a plus. Plus or a cross. B plus. Yeah. Don't confuse it for t. Because t would be transposed, which is entirely a valid operator to pseudo-inverse is always called a plus. Okay? Let's continue. So that's exactly the outer product form. Okay? I gave you one, I gave you a row vector 12. What does it pseudo-inverse. Take a minute to work it out. First, it's a one-by-two matrix. You know that the pseudo-inverse is gonna be to buy one, it has to be two by one. That's one thing you can say without even getting started. Then what is the SVD for? Compact SVD for one to value will be one by one. Sigma is one by two. V is d transpose as one, 2y2. Okay? I'm going to reveal it for you. So we know that this is sigma 1s are in the outer product form. Sigma one, u1, v1 transpose. Because this is rank one, right? There's only one rank, one row, row vector. So this is going to be sigma one is just root five, U1 is one, and V1 is the orthonormal version of one to the normalized version of 1212 divided by root file. That way, one over root phi two over root five is a unit vector. So you have to bring a root five here to make it equal. So anyway, you can go through this. This is sigma one, this is u1, this is v1 transpose. And as per our formula, sigma one becomes one over sigma one. If you want to, you want to find the pseudo-inverse. This guy becomes V1 transpose becomes v1, and u1 becomes U1 transpose. So you get one over root five times one over root phi 12. This is my V1. This as you want, transpose, he put it all together. It's one-fifth, two-fifths. That's a pseudo-inverse of one to that clear, That's the example that follows the definition. Okay? Now why are we learning about the pseudo-inverse or does its value? That's coming up next. But I want to pause and ask, is there any more questions? Sorry. For the cargo hold hold onto that they're coming to that. That's that's what's coming up next. Yes, what is a times a inverse? A times a inverse is going to be I always eight times a pseudo-inverse may not be. In fact, it won't be the only hint, okay? Before we do that. Yet another diversion. This is something that you should know. But in case you didn't, I'm reminding you again, because they are linear algebra, maybe rusty. But if Q has orthonormal columns one through q k, then Q transpose Q as what? Without looking. Now looking, yeah, it'll be IK Exactly. So if q, because this is Q transpose is what? Cute. Let's take Q1 transpose. Let's say Q consists of just one. What is Q1 transpose times Q1? That's an inner product of one with itself. That's gonna be one. What if Q1 and Q2 are there? One is orthogonal to Q2, right? Because an orthonormal set. So you can convince yourself Q1 transpose, q2 transpose times Q1Q2, you're gonna give you 11001, always identity. So Q1 transposons are put on the rows. Q1q i's are put on the columns. And then you multiply them out, you'll get exactly a k by k identity matrix. That's when you do Q transpose Q. And this is true whether or not q squared, if Q is square, then q, q transpose is also, I. Suppose I flip the order. I tell you what is Q times Q transpose columns? First, row, second. This first, this second. But then QQ transpose as I only when I, when you are q squared, because you know that Q transpose is Q inverse for q squared setting, right? So that's always the case. So keep that in mind. Q transpose Q is always orthogonal, always identity. But QQ transpose is not always identity. Only when Q is square, is QQ transpose and identity. That's what we are showing us an example here. So let's say Q consists of these two orthonormal columns, which are just the first two standard basis vectors in 3D, right? 1001010. If you work out Q transpose Q, you will get exactly I2. But QQ transpose is going to give you 100010. And the third, instead of 001, it's all zeros. Q times Q transpose. Okay? What is QQ transpose? What is the interpretation of this guy, of this matrix? When you do Q transpose Q, it's clear it's an identity. What about Q times Q transpose? Who can guess? Yeah. Not necessarily. That's happened to be for this example. This example, it happened to be that way, yeah. Sigma, what signal? From the SVD? What does this have to do with it? I think you're thinking too hard. Any other guesses or suggestions? So I'm asking Q transpose Q is always identity. What is Q, Q transpose? You'll always have extra zeros added, and that will result in a, I'm looking for a word, starts with a P, The second letters and our third letters and 0. Projection. Select, we're doing a spelling bee here. Okay? Alright, it is a projection. Okay? Why is that? Okay? Here, here's kind of the geometry of why it's a projection. So QQ transpose times x, here's my Q, here's my Q transpose times X is equal to one to q k. So let's first multiply these two out. So Q1 transpose X, Q2 transpose X to K, can Q k transpose x? So these are scalars, right? This is just a column vector. So let's write the column vector as a coefficient for one. This sorry, the first entry is a coefficient for 1, s entry is a coefficient for Q2, and the last entry is a caution for q k. What have we done? When I operate on QQ transpose x? When I went to operate on x using QQ transpose, this is my operator. What do I get? I get the linear combination of the columns of Q, where the linear combinations are given by Q1 transpose X, Q2 transpose x transpose x. What is this expression? Q1 transpose x. Where Q1 is orthonormal and access any vector, the inner product of q one with x. But it is also what? This whole expression here, it's the projection of x onto Q1. Remember your projection operation rule. What do you, how do you do projection? You take the inner product divided by the norm squared times Q1. But norm of one squared is one because Q1 is orthonormal. So this is just a projection onto one. The second one is a projection onto, to this thing is a projection onto q k. Is that clear? And therefore, this whole thing is just a projection onto Q1, Q2, Q3, Q4 letter Q, K, e, the column space of Q. So this operation, QQ transpose X Projects x onto the column space of Q. Very important. Okay, so let's write this down. You should remember this. Q transpose Q is equal to identity. The identity will be of whatever shape Q, if Q was. But you, Q transpose x is equal to the projection of x onto the column space. So make note of that, that is really useful. So let's hurry on because I want to finish some things. So Q transpose Q one transpose x Q1 plus Q2 transpose X Q2. I've shown you the picture. It's the projection of x onto the column space of Q by the orthonormality of Q1, Q2, and so on, right? Exactly how we do projections. Okay? Now, let's actually, somebody asked what does a pseudo inverse times d? Okay, let's, let's do it. Actually, I want to keep that. Let's do what I've been through my chalk here. So what is a times a pseudo inverse? So this is not a T, it's a plus. Okay, so maybe I should make it across. Okay, so what is a? Well, we know a is, let's use the compact SVD. So they're gonna be, you are sigma R, R transpose. That's a, right. What is a pseudo, pseudo inverse? It's the other, written the other way. So this is VR sigma R inverse times you are right. I just wrote a here, a plus here. Now, what do you notice? Right here? I have v transpose v are number b's are orthonormal vectors, right? But R says it's not a full basis because I reduced it. So what is v transpose v are anything, any orthonormal set transpose times that orthonormal set. The columns is going to be of dimension r. This is just r, ir, Sigma R and Sigma R inverse. I forgot what? Oh, yeah, this one. Okay. So V R transpose VR is just identity. Sigma R and Sigma R inverse. Cancel. What do I left with? A plus? It's you are, you are transpose using a similar, similarly, I'm not gonna go through it. You can show that a plus time. So a plus times a is going to be V times V transpose. And we know that this is a projection based on what we just saw here. Qq transpose. Anytime you have an orthonormal, orthonormal matrix here, Q times Q transpose will be a projection onto the column space of Q. So this, if I multiply this by x, it will be a projection onto the column space of UR. But what is the column space of UR for column, same as the column space of a. Remember, you, ours are the basis for the columns of a. That's how we started with defining the SVD. So therefore, when you multiply it by this, you're actually projecting onto the column space of a. And when you multiply by this, you're projecting onto the column space of a transpose. Okay, so pretty neat. So that's what, Let's finish this off in the last few minutes. Okay? So this is exactly what we did on the board. From the first equation. A plus is the projection onto the column space of U R, which is same as the column space of a. This is from previous times. Column space or via, because you know that the URLs are a basis for columns of a, VR's are a basis for the columns of a transpose. So the row space and column space. So what is the connection to least squares? The squares with SVD? We want to minimize a x minus y. Now, this is a case where m is greater than the tall matrix, right? You have many equations and only many equations and fewer unknowns. And this is our Age, old least squares formula. A transpose a inverse times a transpose should be, should be. Why? Now, question is, always assumed that a transpose a was invertible whenever we use least squares, right? Is all the time we assumed that a transpose a was invertible. Well, what if it isn't? The first time you're going to encounter you're going to meet the case when a transpose a is finally not invertible. Yay. It's a milestone in your evolution of studying least Chris, you always assumed in least-squares, a transpose a is invertible, whatever it isn't. What should you do? Any guess? I mean, what are the topics we have studied? You take the pseudo inverse of a times y. If x equals y, x equals a pseudo inverse y always. That's why pseudo inverse is defined the way it is, right? Pseudo inverse as that object for which when a x equals y and a can be any size, any rank. And and you want to solve for x in terms of ANY, it'll be x is pseudo, pseudo inverse of a times Y. Okay? That's okay. Again, I wrote be here, but this should be a way. In fact, this operator, this always exists, is always valid whether or not a transpose a is invertible. Okay? Recall, and I'll give you the one-line proof, okay? It's a geometric proof. Recall the minimizer for XLS is that, remember that if you want to find a x equals y or x is approximately equal to y. You want to project onto the column space of a. That's the geometry of least-squares. But what is the projection of y onto the, onto the column space of a? We just studied it. It's nothing more than e plus times y. A times a plus times y is the projection of y onto the column space of a. Because a plus is exactly the same as you are. You are transpose. That's what we worked out. And you are as a basis for the column space of a. Therefore, you have the equation that E times XLS is E times a plus. So plus y must equal XLS, and that's the formula. So XLS is a plus times y. Now, if you compare it to the case where we had the formula, that's when a transpose a was invertible and it was full rank. You can verify it. You can verify that when a transpose a as full column rank, that is an r equals n at the tall matrix has full column rank. That's the assumption for least squares we used in 16. Then he, we can verify that indeed the pseudo inverse operation defaults are degenerates into this expression. A transpose, a inverse times a transpose. And why is that? Well, that goes through a little few calculations which I not sure I have time to do now, but for now, we'll do it. Or you can post the notes, you can verify for yourself. But a is given by this. Then a transpose is given by that. And you work out a transpose a and you work it out. And you'll see that indeed x plus is going to be a transpose, a inverse, a transpose for the special case. Likewise, you have a similar story for the minimum norm or the minimum energy case, where minimum norm formula was given by that, we derived that a few weeks ago. You can show that a plus becomes the minimum norm solution when m is greater than, when n is greater than m, and it becomes a least square solution when m is greater than n. In both cases, the pseudo-inverse formula is gold. It works all the time. So the summary is if AX equals y, regardless of whether m is greater than n, m is less than n, m is equal to n. I don't give a damn. X equals a plus Y always works. That's always the solution to a x equals y. So it encodes, it encapsulates the least-squares minimum norm and invertibility in one swoop. And that's the power of the SVD. Okay, so that's all. And now we are perfectly positioned to start on the PCA. For the next class. Or from now on, there'll be Not so much more math. It'll be more applications. If NEMA here. What's that? Password is? How about pseudo P, S EU deal. 