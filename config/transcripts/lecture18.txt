Good to see the class trickling in. We needed to incentivize you as how it works. Remind me when it's 10:00, Okay. Because I want to take a picture to see how many people here. I'm gonna give you guys 20 min to come in. Okay. Welcome to a rainy Tuesday. The last week before spring break. Yay. We could all use it. So just have to flip through one more week and you can rest for a week or whatever you do during your week. Definitely don't work. Should rest. Okay. So I'm glad to see more of you trickling in. So at about 10:00, I'm going to take your permission. I'll take a picture of the class to tell the TAs how many people attended so you can get your bonus points if you cross the threshold. Okay. So today, a reminder that the midterm re-dose for the clever people, it's due tomorrow night and the grades will be released after that. Okay. So how's it going in general? People started on the homework. No. No. Yes. How are the labs going? Alright, so the agenda for today is that we are going to wrap up controllability. It'll be an extended wrap up because I realized many of you were missing in the last class, I wanted to do kind of a run through of all the concepts involved, even if it bores some of you. And then the new topic for today is about orthonormal basis and Gram-Schmidt procedure. How many of you have seen orthonormal basis? I think. Yeah. I feel like many of you are sure that you have it in 16. You didn't. You had it in a math class. Alright, so let's do a recap of controllability. Recall that we start out with the discrete time control system, which is given by x of I plus one is a x of I plus BU of AI in our familiar system. And we're going to remember n is the dimension of the state m if the dimension of the control value. So we're going to sort of assume m equals one more for ease of illustration than anything else. So the solution to this difference equation is given by the star green equation, which. We saw last time and you have seen many times, right? By just writing it out, finding the pattern of what happens, you can write that x minus a l e to the l x of zero is this matrix here, which the left. So B is column, one column because m is one, right? So B is n by m. So m is one. So B is just a column vector. So every one of these guys is a column. When b is a column. So you have n columns here. And it is called the controllability matrix or Seattle, right? This is just a review. Times the u, the control action from zero to L minus one. And the left hand side that I saw, people know how to derive this, right? Or at least, yeah, good. So just a reminder that n is the dimension of x, m is the dimension of U of I, which is one in this case. So a is n by n, B is n by one. L is a time index. Cl is the controllability index. So I just wanted to remind you that in general, if B is not a column for both, m was two, then b would be two columns. If M O3B would be three columns, then you just tack them. Just like you'd have three columns at B and three columns or three columns, so a squared b and so on. So the matrix is gonna get fatter very fast. So that's, that's the concept when m is not one. Any questions. Every individual. The matrix itself has to be independent. Independent of the rank has to be n. We get to that. Yeah. So question is, can we find an input sequence? You remember u is zero to L minus one is in our hands. We design them. So can we find a sequence such that if I, if I start at some state x of zero, right, I can reach any state X sub L. So that is written as a target state x target for a time L. If I can reach it from any state, then I say that the system is controllable. In other words, repeating that equation. So the target state is x over L. At time l. The initial state is x of zero at time zero. And here's my controllability matrix, and here's my, my u column vector. So the question is, can I make the left-hand side anything I like? Remember the left-hand side is going to be a n by one column, right? Because x target is a column and a column. So if you give me any x and any x of L and a is fixed, then the left-hand side, you have no control over. It could be anything you like. Can I make the right, left-hand side anything I like given this equation where CLS fixed rate because they can be fixed by just controlling the use. But what is the intuition? We did this last time? Just to review, Yeah. Yeah, it has to be full rank, right? In other words, the rank of that matrix has to be n, because the left hand side has state, has n degrees of freedom. So you need to have n independent columns. So we know that if I can find weeks, remember the U of I's are going to weight these columns, right? U of zero is going to weight this guy. You have one that's gonna wait, that guy and you have L minus one is going to be. So if I can find weights, then I can achieve any left-hand side I like. And that's exactly what we have written here. So this is called an x target. Minus eight. Zeros must be in the column span of CM, the same language. So that's what controllability is. So a system is called controllable. If given any target state and any initial conditions, we can find the time l. The time can be arbitrarily large. It doesn't have to be one or two. But we'll go through the time index also, as we did last time. So you can find an input sequence such that you can hit any target given any starting point. Okay, informally, I've written in red that controllability in an n dimensional state-space is the ability to go from anywhere to anywhere in RNA. I can pick arbitrary points and then I need to be able to go from that point to that point. So when n is two, then on my plane, I need to be able to start anywhere on the plane and go anywhere on the plane. So if the concept clear. Okay? So how do we check this? If Cl has n linearly independent columns for some l, then the column space is R n, right? So in other words, CL times this guy, which is the right hand side, can attain anything in R n by just choosing the weights, which are the zeros, the weight from the columns. I'm just repeating what we said earlier. We did two examples last time. So the first example, x sub I plus one is this is the a and this is the B. And let's do our controllability test. Is it controllable in one time instant, meaning when l equal to one? Well, for one time instant B is 01 and AB is 12, so we don't need AB yet. So the controllability matrix at time one is just be right. You're going to start out at the right of b and then move your way forward to the left, b, a, b, a squared b and so on. So when c1 is b, dimension of b is just one, it's a column vector. So you need to be, but x has dimension two. So you're not, you're not controllable in one time step. But that doesn't mean they're not controllable. Because controllability says you don't have to do it and immediately you have to eventually able to do it. Well, what about L equal to two? You have B and AB. So you have 01.123. If you compute AB, it's exactly the second column. Now this is indeed, these are independent columns, so the dimension is two. So indeed your controllable in two time steps. Is that clear? Okay, now let's do another example from last time. All of this is review everything I did last time. So example three from last lecture. Now, a is this matrix 1102 and b is 10. So the dimension is one. C1 is be, C2 is ABB. Now if you do it and if you look at a times B, It's the first column of a, which is 10. So dimension stays at one, at time two, at L equal to two. Well, let's try a squared b. No luck, it's still 10. So the dimension of this remains one. This is a rank one matrix, right? If all 10, all the same entries on every column. So you're not growing in dimension. That means this guy cannot be controlled. Louder, louder, please. Sorry, AB am I my bad? That's a typo. Thank you for pointing out. Let's clear it. So here's kinda the sort of without, I know the math tells you what to do. But let's look at it and see what's actually happening. If you write the two equations corresponding to the two components of the states, they have two states, right? Because two-dimensional vector, you see that X1 and X2 are given by these equations. I just wrote out the first equation from the first row, from the first row, from the first row and the second row. If you write them out, you'll get exactly these two equations. And let's say we want to go from x of zero equal to 00 to x target to 11. So I'm on the origin at time zero. And I just wanted to reach 11 on the heart to plane, right? Because this is my example. In l steps. I don't care how long you take. Can we do it with the appropriate choice of u of y? That's the question that the theorem says. Just apply the span condition, the column span condition for the controllability matrix. But let's see what's going on here. So x target is 11. So if you look at the equation, the left-hand side, which is this guy, right? So Excel is x target, that's 110 is zero. So this guy goes away. So the left-hand side is 11. So let's come back and put 11 has x target. Can I go to 11 and l steps given that I start at 00? Well, x target is gonna be BAB times user or you want. So the design choices I have, I can do anything I want in zero, in user on U1. But my Hebbian be a 1010. So if you stare at the second equation, I can put anything in the start column. So in the start entries of the matrix, because that's my design choice. Well, no matter what I do, I cannot satisfy the second. The second equation is kind of meaningless, right? It says one equals zero. Sorry, It's okay. It's a garbage equation. No matter what I put on star is gonna give me nonsense. So that's sort of an indication that this cannot be controlled, just an intuition other than going through the algebra. Okay, so. Why did we get stuck? And what are the conditions for getting stuck? Right? We got stuck at 10. Well, that's as a result of the lemma. We call it lemma. Last time I went home and thought about it. I said No, it deserves the status of a theorem. A theorem is higher up than a lemma because it's a powerful result. What does it say? It says if a to the b, that is the column has a TLB here, where's the LB? There it is. So a to the LB. Okay? It's, it's the column to the left of eight minus one. Okay, so let's go back. So if a to the b is linearly dependent on e to the l mine, this is my controllability matrix at time n, the column. So if I go one step more and I get, I generate a DLB and I find that I am linearly dependent on the other columns. Then this theorem says which we proved, which I'm not going to do now. But you can look, look at the notes from last time. Then e to the l plus one b is also going to be linearly independent. That's like confirming the fact that if you are stuck at time two, you shall be stuck at time three and time for forever. So the theorem shows it and you can prove it pretty straightforwardly just, just by algebra. But the theorem is powerful in the sense that what it's saying is if the dimension span of Cl plus one, that is the controllability matrix at time l plus one. It's the same as the dimension span at time n. And let's call that d as the dimension. Then if I go two l plus 21 step further, I will also be a D. I'll be stuck at D. Okay, exactly the generalization of the example we saw. In other words, once the dimension stops growing, it stopped growing forever. Now, why is this a proof? It's because we know, what do you know about C? What is the maximum rank CL can have? Remember that it has n rows, right? Because x is n-dimensional state. So Cl has n rows. And in our case, l columns. L times one in January will be L times M if b is not just a column vector. So what do you know about an n by n matrix? What is the maximum rank of an n by n matrix? Yeah. And max, min of n and l, right? It depends on what L is two. If L is 1 million and n is ten, dimension not going to be 1 million, you're gonna be ten. The rank will remain ten. It'll be, you're going to be constrained by the smaller dimension of the row rank or the column rank. You guys know that right? From algebra, linear algebra, okay? So that's exactly So I know that I cannot go more than n. Because more than n, I'm going to run out of room. So, but it says that you may run out of room even earlier. So that's what this theorem says, and we showed pictures. So how do I check? I don't need to check every time when I go to the left and does my rank increase or not. So that's what we have shown here. So I started L and distinct keeps climbing up by one each time. I stack. Column vector to the left of e to the l minus one, I'm going to increase in rank. And then I cannot go more than n because I have only n rows. So I'm stuck at in, everything will be stuck at most n. But this is a controllable system, but an uncontrollable system is gonna be stuck earlier. So he's gonna be stuck at some b which is less than n. And then once you're stuck here, our theorem tells us that you're gonna be stuck forever. Wherever you're stuck, you're gonna be stuck forever. Is that clear? Okay, Good. So let me quickly run through this example. I've done many examples, but these are just more examples showing in the vein of what we did earlier. So this is just showing an uncontrollable system because AB is 10 same as B. And again, the intuition is that if you look at the second equation, x two of i plus one is just 2X2 of AI. So the second component of the state vector keeps blowing up, increasing by two. And you cannot really control, you can't rein it in. There's no controller can run it in. On the other hand, this is controllable. And this is an example where if B, the second equation, so these are the two equations. Now if you look at this, it's kind of a little bit weird in that you remember u is just a one-dimensional action, right? Because we said that m was one, which means I have a scalar control for a two-dimensional system. So the second component, the second state vector component of the state vector. Indeed you can control with you of, I mean, you can, you can make it, change it. But the first one has no U in it. And the reason this thing is controllable is that you actually controls both implicitly through this equation. If you think about it. And this is a phenomenon that may relate to it mechanically. When you, when you have a parking your car, right? What are the possible things you can do? Well, I need to know, I want to park my car at an exact spot at a certain angle. Let's say it's parallel parking versus whatever side parking, slanted parking. So there are three components to my state, x, y, and theta, right? And you can actually control three components of your state using only two control actions, which are the steering and longitudinal forces and bricks. That is, you can drive forward and you can go steer. And yet I can control three degrees of freedom. So this is an example where with two control actions, you can do, you can, you can, you can control a three-dimensional state. Okay, So that's just a real world example of why these things work. Okay, let me very quickly run through CCF also. So this is another concept we did towards the end of last lecture. So this is called control canonical form. Single input m equal to one. If a and B have a very special structure, then checking for controllability. Controllability is guaranteed. Okay? And why is that? The special structure is the following. So this is called the a n matrix. So a and B of the, you guys know that the close loop is a plus b f, right? You're doing your lab that open loop and close loop this week. So we know that if your feedback gain is F, feedback component is F, F times u. So your closed loop system is ACL equals a plus bx, right? So this is a and B is gonna be all zeros and the one at the last component, AN is going to be arbitrary on the last row is going to be, let's call it A1 through AN. And you'll have the identity matrix in the inner n minus one by n minus one box. So this is n minus one by n minus one and zeros on the first column. This is the special structure. It's called the controller canonical form. And the example two we saw earlier was exactly of this form. Remember when we saw this, this example which we said was controllable. It's not accidental that is controllable. It's because it's in this form. Meaning these can be arbitrary. You have a one-by-one identity matrix is just a scalar here and a zero on the top left because the first column has to be zero other than the last row. Now, why does this matrix have special properties? If you look at the characteristic polynomial of n, which is determinant of lambda I minus a. It has to form given by this equation. And you can show this. And if you're not, it's a good exercise. Convince yourself that the characteristic equation of determinant of Lambda off of a is given by this equation where it's a polynomial, of course, lambda to the n plus something lambda to the n minus one. But it turns out that the coefficients of the lambdas, you can read off from the last row one by one. So you start from the right. Put that as the coefficient for m to n minus one, and then move your way forward minus A2 lambda one minus A1. So this is magical property that there is a one-to-one mapping between the a matrix. You can inspect by inspection from the a matrix, you can write the characteristic polynomial. If you're telling me an a matrix is of this form, I know the characteristic polynomial for that matrix immediately by inspection. And you should convince yourself this is true. In fact, you can show this for the two-by-two case. You can see that the characteristic polynomial here is going to be lambda squared minus two, lambda minus three, which you can due by determinant of lambda I minus a for the two-by-two case. In general, for the two-by-two case, we will have lambda squared minus A2, lambda minus A1. Okay? So in other words, just read it off. For n equal to three, you will have lambda cubed minus K three lambda squared minus two lambda minus A1. Literally go from right to left and write out the coefficients of the polynomial. And you should check that that's true. Now, why is this nice? Because if you form a plus b f, which is our close loop system matrix, which has a plus B, f, which has given me bf is just this. You see that a plus B, f is also exactly of CCF form, is just at the last row instead of being A1 through AN is now A1 plus F1 plus F2 dot, dot, dot AN plus Fn. So the FNs have enriched the last entry, the last row. So what is the characteristic polynomial for the ACL? Should know it by inspection, write need. It is exactly lambda to the n minus k, n plus f n lambda n minus one, and go all the way down minus A1 plus F1, right? Because it's the same form. So it's just a, it's just a new way now, new a's. Now, suppose as an engineer, I want to put the eigenvalues of this system matrix, the close loop system matrix at my desired point, some arbitrary lambda one through lambda n. And then n state system, I can dictate whether lambda should be. I want to dictate, that's exactly the controllability. So what should I do? Well, if you write out lambda one, these are the desired lambdas that I want my system matrix to have. I write it out. And you see that there's a one-to-one mapping that relates these equations to the coefficients that we just saw. So in other words, this is the A1 plus F1, this is a n plus f n, and so on for all of them. So I can just by pattern-recognition, write out all the FIS by writing it out this way. And the Lambda i's are known because they are my design choice. And so I can find the FIS needed to to control my system from anywhere to anywhere. So it's a very powerful trick that comes out of this matrix. Then you may ask, hey, but where are you going to see matrices like this weird stuff that are going to be just identity in the middle and zeros. And it looks like a really hard constraint or impulse. So why would it work? Well, before we do that? Well, let's do that first. It turns out that if you have any controllable system, it doesn't need to be in CCF form. So it can be an arbitrary a and arbitrary be. They don't have to be, doesn't have to be all zeros and a one as needed for CCF, right? No matter what system you have it. If the controllability matrix is such that CAN has rank n, which is the controllable tests that we do it, we did, therefore, it's invertible. It turns out that I can always transform that CAN into a CCF system. It's exactly like you studied earlier. If your, if your matrix was diagonalizable, what did you do? You diagonalize it so that you could decouple all the scalar equations, write the vector equation was all jumbled up by diagonalizing it emitted into n scalar equations. By doing a trick, which is you multiply it by a particular transform. What was that transform? How do you take, how do you take a square matrix which is not diagonal and make it diagonal if it is diagonalizable. Midterm. You must have studied this for the midterm, no. What did you do? You bombed? Yeah. Yeah. You all from changer, you use your change of basis technique to go from the standard basis to the eigenbasis. Right? Now, we need a similar transform, but it turns out there exists a transform that is not the eigenbasis, but it's the CCF basis, if you will, okay? If it's a transform that will take you from any system, any controllable system to a CCF form, okay? And so that's exactly what we do here. So y n plus one is AY. This is the, this is the CCF form. So I can, I can transform my x n plus one system. This I've labeled as why the CCF form by just doing the transformation that Y n is t times x n, where t has to be carefully chosen. And there's a homework problem in this homework that will guide you through. How do you find that T? Okay, so it's like a preview of what you need to be doing this week. So that t is going to allow you to transform a controllable system. But I don't know how to do the control to a system where I have in beautiful form that I know exactly how the control works. Because I know how the close loop for the CTF system is related to the eigenvalue stuff. Okay? So anyway, now, just to make sure that we understand, give you a short quiz. Suppose a is this matrix. Three-by-three, system. B is 001. And I want my eigenvalues of the closed loop, which is, remember that the close loop is a plus b f, right? We know that I want. So I want to place my eigenvalues at 000. I want all the eigenvalues to be zero, which I know is inside the unit circle, right? Which is good for me. Question is, what is the characteristic polynomial of a closed loop? One of the feedback value of F1, F2, and F3, I would have to exert. So take a minute and work it out. While you do that, I'm going to take a picture of the class to send to the TAs. It looks like a very healthy attendance. Have to go all the way back. Yeah. Just raving for the photo. Okay. Okay. Okay. Somebody wanted to lead the discussion. What is the characteristic polynomial for ACL? We went through the CCF tough, right? What is the, what is the ACL metrics? What was a C? So it's gonna be 010001. Remember this is in, in CCF form. You just add one plus F1 plus F2, three plus F3. Everybody see that? My a matrix was, I just added F1, F2, and F3 of tails to the A1, A2, A3, which has 123. That exactly That's what ACL does. Now, what is the characteristic polynomial for ACL? Yeah. Yeah, it's a minus, but you're right. That's exactly what we do here. So this is ACL, right? We just saw it on the board. I didn't want to show you the characteristic polynomial. Just read off from right-to-left. Lambda cubed minus three plus minus three plus f three lambda squared minus two plus F2 lambda minus one plus F1. Now, what do I want? I want the lambda has to be at 000. So what value should I exert for F1, F2, F3. So what is the characteristic polynomial I desire? Lambda minus zero times Lambda minus zero times Lambda minus zero. So lambda cube is the only term that should remain in water. Yeah. Exactly Perfect. That's exactly correct because we want lambda one, lambda two, lambda three to be all zeros. So that way the determinant of Lambda I minus ACL will be lambda cubed. To make the right hand side lambda cubed, I must get rid of all the lambda squared, lambda and constant terms. I can do that by making three minus three f, f2 minus two, f1 minus one. So here's an example of how easily you can control a system when it is in TCF form. Okay. Alright, so that's a really lengthy review. It's almost like a lecture in itself. But it was all mostly meant as sort of target all the missing people from last time, which there were many. So I hope you are all set to attack the homework and the labs. Okay, Now for changing material. So I'm going to beat it up. We did all this. I think I can delete this page, need this. Now. We are on to orthonormal basis. So today's lecture, I'm going to be mostly doing from the notes. I'm not going to be writing much because a lot of this is review. But when needed, I will have sold on how people seen Gram-Schmidt before. How many of you have seen it if you raise your hands? How many have not seen it? Okay. Good. So about 5050. So I'll go through the slides because I think today's lecture is something which is review for many of you, but nonetheless, I'll slow down and explain. You have any questions please ask me. So let's start with column vectors, one through q, k there called orthonormal. If Q transpose Q j, that is, if I take the dot product of q i and q j, or if I do Q transpose times Q j, now q i's are column vectors, that Q is a column vector. So Q transpose is a row vector. Row vector times column vector is a number, right? So there's a row vector times column vector when it is zero or when I is j. And it's one. When I is zero, when I is not equal to j, and one if I equal to j. So this is an orthonormal a vector set. The example for that is the blue vector here is QI. The green vector is Q, j there, all of unit length. That's the normalization that is imposed by the orthonormality. Normal means that it should be unit norm. The norm should be one or the size should be one. So that's an example of two column vectors that are orthogonal to each other and have magnitude of one each. So they're at right angles with each other. So very simple concept. Now, a matrix with k columns that are orthonormal will satisfy the following property. That Q transpose times Q. Let's work it out. So Q transpose is just Q1 transpose, q2 transpose, Q k transpose times Q1, Q2, Q3, Q K, right? That's right, the columns, the definition of Q transpose Q is, take the q vector columns of Q, make them roles on the left-hand side and then leave them as columns on the right hand side. So if you multiply this out, this is a k by k matrix, right? We have k elements here and k elements there. So you have Q1 transpose Q transpose q2 dot dot, dot Q1 transpose Q k. The last one is Q k transpose Q one and Q Q transpose Q k. Now, if these QI is our orthonormal to one another, what happens to these entries here? All zeros, right? This will get zero and this will be zeroed. And it will be of course symmetric. If all the rest of your row is zero, the rest of your column is also zero. The only survivors are going to be on the diagonals because that's where I equals j, right? So Q11, Q11 transpose this top-left, sorry, Q one transpose Q one is top-left, Q k transpose Q. K is bottom right. And you have all the stuff in the middle, Q2, Q3, Q4. So this is going to become i k by k by our definition of orthonormality, clear stopped me now because we are building on this. Okay, here's an example. So if Q1 is 11, but I have to normalize it to make it unit norm. So I have to multiply it by the annoying one over root 21 over root 21 over root two has norm one, right? Because it's square root of one squared plus 11 squared plus two squared should be one. So Q11 has norm one. Qh2 also has norm one. But Q1 and Q2 are actually orthogonal to each other. Because if you write Q11, Q12, you have Q11, Q12, you write one over root two. So Q11 is, so this is the cube matrix. Q transpose is the transpose. And if you look at Q transpose Q, it's going to be an identity. And it's not surprising because Q1 and Q2 are orthogonal to each other. If you take the dot product of 11.1 minus one, you get zero. So the diagonal, off-diagonal entries have to be zero. And that's exactly what Q0. So if Q is square, so Q transpose Q equals I is true for even non-square matrices. K If Q is, yeah, but if it turns out that Q is square, like in this system, then we know Q transpose, Q is I, which means Q transpose is Q inverse. So it's a very simple way to find an inverse. If you have an orthonormal system of equations. So if there's an orthonormal matrix and you want to find its inverse. In general, finding the inverse of a matrix is a pain, right? I want to avoid it at all cost. Now, what orthonormality buys you is if you want to invert it, just transpose a tough, I'd much rather transpose and invert. But then you have to start with the orthonormality condition to begin with, right? So that's why we don't mind investing in orthonormality to save the pain later. Okay, so that's the message that I want to convey. Alright, so, but in the literature, if Q transpose Q inverse, it's called Ortho orthogonal, even though it should be orthonormal, somehow the math people don't like orthonormal, but we won't worry about that. Okay, so here's another example. We did an example with 111 minus one, right? And if you were to plot this, so here's the 11 vector unit norm. And one minus one is there. Right? Now, this turns out to be a more general case of a rotation matrix. Okay? So here's another example of an orthonormal matrix. The entries are cosine, cosine minus sine, sine cosine. And so that's called a rotation matrix for the following reason. So the first column is Q1, which is consists of cosine theta, sine theta. Second column is minus sine theta and cosine theta. Let's check that indeed Q1 and Q2 are orthogonal. They'll take the dot product between Q1 and Q2. You will have cosine times minus sine plus sine times cosine zero. Good. And what about the orthonormality condition? Cosine squared plus sine squared is one that also orthonormal. So individually they have unit norm and they are perpendicular to each other. And therefore this is an orthonormal system, orthonormal matrix. So why is it called rotation? Well, for one thing, the system we just the example we just did, 111 minus one. Is that a special case of this matrix? For what value of theta? For what value of theta do you think? I get 11.1 minus one? Yeah, exactly. So it's a it's a 45-degree rotation so that you can see from here. Let's say I start out at 01, okay? So suppose my input is 0101 to the right-hand side of Q. What do I get? I get minus sine and cosine has the components, which is exactly minus sine and cosine of Theta, which will be that. So you have rotated by counterclockwise by theta. When you start at 01, what about if I start at 10? Then that's the first column which has cosine and sine. That's indeed counterclockwise rotation by Theta. Okay? So, um, they're giving you the geometry of the orthonormality. So usually it will involve rotation at some angle. Some useful features to carry along for orthonormal matrices. Matrix with orthonormal columns. First property is that your length does not change. Because in general, if Q is a rotation matrix, okay? So if I rotate a vector, I don't change. Its length rate remains the same. And we verified that in general. So this not just for the two-dimensional which I can visualize. So it preserves length. So the norm of x is just square root of x transpose x. What is the norm of q x, q x transpose times Q x? Well, you know what Q x transpose it is X transpose Q transpose Q X. We know Q transpose Q is I. We just derived it. So this is just cosine of square root of x transpose x, which is exactly the length of x. So that means whenever Q transpose Q is identity, applying a queue transformation, nothing to your length. That should be obvious, but this is the intuition for why, because you're just rotating. Any questions. Okay? So Q also preserves the dot-product. So I take the dot product between x and y. That's because if I have x and y and I rotate them by the same amount, I've not changed their dot products. The relative orientations are the same. So that's the geometry. But in general, Q transpose Q, x transpose times Q, y is x transpose Q transpose Q y, Q transpose Q is still lie, is I am therefore. The dot-product between Q x and q y is the same as the dot product between x and y. Everybody, good. Show me a thumbs up. Okay, good. Alright. Now let's go to visualization because it'll be useful when we are building up our orthonormal basis. So if you were to look at the column space, the column space of D, let's say D is a matrix and it's a two-by-two matrix. Because we can visualize two-by-two the column space of D, when D1 and D2 are independent vectors, but not necessarily orthogonal. It's going to look like this, right? You have one vector on the x-axis and other at some 45-degree angle or something, right? This is D1 and D2. But it's much prettier. If they are orthogonal, they become like your Cartesian coordinate system, which you can easily recognize. So here's one column, here's the other column. Now, the reason orthogonality is really useful as we will be looking at in more detail later in the lecture, is that doing projections are orthogonal projections onto these vectors is going to be very straightforward. So if I have, so this is my plane. And d1 and d2 are orthogonal basis for that plane. Basis vectors for that claim. And S is sticking out from the plane. It's a 3D object. I want to find the projection of S onto D1, and I want to find the projection of S onto D2. We will go through projection calculations a little more. It's also, you guys did projection in 168. I know there are notes on it. If not, go and review it. We will do a little bit more later today, but the projection of S onto D1, D1 transpose S. And times d1, the projection of S onto d2 plus d2 transpose S times d2. If d1 and d2 were not orthogonal, these are not true. Okay? So that's another sort of winning property of orthogonality. We will need to do projections. And doing projections with an orthogonal system is much, much easier because you don't have to. You can individually project onto the base, onto that vector and not worry about anything else. It's exactly like if I were to write, let's, let's give an example. I had a page. So my cartesian, Cartesian coordinates that I'm not doing anything deep here. Yeah, so this is my x and y. So our unit vector in the x direction is orthogonal to a unit vector in the y direction, and they are unit length. So if I have a point P, call it P, which has coordinates x0y0. What is its projection onto the x-direction? Just this guy. I just read out the first entry. Why did I do that? Well, that's because I projected it from projection of v onto the x-axis. Inner product of x0y 010. That's the unit vector in the x-direction. And what does this just x zero. You need to go do this calculation. Just read out the first coordinate is the x, the second coordinate of the y. I mean, it's kinda we're used to this. We don't even think about it. The reason we can do that is because they have an orthogonal system. You couldn't do this. E.g. if I gave you another system, let's say a blue system where the basis vectors are. Let's say these two. Because they're, you'd have to say, how much do I have to move in the first basis vector dimension and the second basis dimension in order to get where I am member or a professor nickname. This example with Berkeley. From, I think he was wanted to go from downtown Berkeley to somewhere. And he gave an orthogonal system versus, versus a screwed up system, right? The basis for our basis vectors, we're not at 90 degrees, is pretty hard. In fact, it's a pain that you should avoid. So this is just a reminder that those are concepts that you should keep in mind. Okay. Projection of okay, I'm too close and come back. Okay? Now, let's do another brief review of least-squares because this kept squares keeps showing up. We had used the notation of S equals dp plus E. So suppose you want to find the least squares estimate of p. Remember, d is my tall matrix and P is my unknown vector. This is my uncle. So this is unknown, this is known, this is known, this is not known. We know that the least-squares, I'm not gonna go through least-squares again, you've done this several times. Here's a picture of the orthogonal projection picture the least-squares is intimately associated with orthogonal projections that so you should ring a bell. So the least square solution is d transpose d inverse times a transpose S. If de Paul matrix that we do least-squares width happened to be an orthonormal. Columns had to have, happened to have orthonormal columns. How much easier least-squares would be, right? D In general, you don't have control over is given, it is what it is, it's given to you. But suppose by some blind luck, all the columns of D happened to be orthogonal to each other. How do, how sweet would life be? Well, let's see how sweet. The d transpose would be. Q1 transpose up to Q and transpose d transpose d is the columns Q1 for q n, let's say D has n columns. So d transpose d is going to be IN. That means d transpose d inverse here under do identity inverse is identity. So this guy goes away. And D transpose S is my solution. Where d is the orthonormal columns, rows of V transpose of the rows of the queue. The queue is that clear? So it's a nice objective to strive for, if we can, if we can pull it off. Okay? And that's exactly what Gram-Schmidt is at a high level. If you don't have these that are friendly to you, meaning the original columns are not orthogonal. And making them orthogonal, That's really, really helpful down the line. Why not take the pains and make them orthogonal right away? That's gone for it. Okay? So you're given a system of independent vectors and I want to make them into an orthonormal vector set. At the big picture of Gram-Schmidt orthonormalization. It's exactly that. But before we do that, I wanted to, I wanted to get rid of this. So let's kind of look at some other examples where these things show up. So I'll take another example where orthonormality plays a nice row. Okay? So it's kind of a, sort of a contrived situation that I'm going to put you in. It's an application. So let's take the application on the right, right. This guy. So let's say that I'm sitting on top of the air traffic controller office building. There. I'm regulating traffic in the airspace and my space happens to be drones. So I have drones flying and I need to identify which drawn has is how far and so on. But in the setup, I'm going to be given at time t, I'm going to be told that there are three drones in your neighborhood. Can you check their status in your space? Okay. So you're getting the word and then at some later time, t plus delta t or t, t1 and then later T2. You will be told there are five drones. Can you check? And this number will keep increasing in start at 12345 and I'll keep warm. Now. You could also, the situation also applies to the case of what's called Internet of things. People have heard of IoT. The Internet of Things is very smart. Everything's. Smart toaster, smart fridge, smart toilets, everything. And they all want to communicate and report their status and so on. Right? So anyway, so you guys know more about it than I do. So these are all the Internet of Things. And they have signatures. These SIs are signatures where they kind of are revealing themselves who they are. Now, you are sitting in the receiver antenna. Let's just stick to the drones. I'm just giving you two separate application scenarios where these drones have signatures, which we will call SIS, and these are typically long sequences. You guys did gold sequence in DNA, that true? Yeah, you did right for you, you have some familiarity with signatures. So my received signal is going to be alpha-1 s1 plus alpha2, S2 plus all the SIS that I'm receiving from. I want to know who is transmitting, which means these are my signatures. Let's say I have four. Just as an example, I need to find these alpha i's because I have a received signal which is the superposition of these guys. Now, if one of the Alphas is really small, Let's say that guy doesn't, if it's not in my airspace. If it is not smallest CEO, he's big and he's yeah, I can I can I can I can send them. Right. So that's the feeling. That's the general setup. So these SIs are long signatures and the alphas are, are, are small, meaning that you don't have that many drones compared to the signatures of the drums. So you want to estimate the values of these alphas. You go to your familiar least-squares equation. Now, what happens if we learn of each device are drawn only one-by-one? This is also known as the online learning setting. In machine learning, you'll have data streaming in. You have to make an inference and then the next data comes in. So you have to build on the knowledge you've built so far and then append to the next one. So that's called online and it's really hot. It's very popular because it's real time. Even for tracking the things known as Kalman filtering that you may have heard of. How many people have heard of Kalman filtering. You might have heard of it that also uses online and the techniques that we're going to talk about today. So these are really powerful techniques. Okay, so time to wake up. What if we learn about these devices one at a time? What will, what could I do? How could I do better? So initially I have s1 and art. I have only one drawn in my system. It has signature s1. And I want to know that s1 explain the received or in other words, are you sure that S1 is in your neighborhood? Well, for that I need to find what is my alpha, because alpha is R and this is a very trivial system if just one column of X is just a scalar Alpha. By the way, I'm changing notations from what are they using, a p and d, p equals S to a, x equals b. Little more familiar, right? But don't get confused. This is still the same today. In the original notes I had dP equals S as my linear least-squares system. Now he's gonna be AX equals B for the next few slides. So again, I'll find alpha one hat, which is S1 transpose S1 inverse times X transpose, our least squares estimate of alpha. Okay, good. Now, I heard that there are two drones in my neighborhood. I have S1 and S2. So what do I do? Estimate Alpha-1 and Alpha-2? Because that are the estimates of whether drawn one and drone two are close to you or not. So by finding, what do I do, I redo least-squares and I have to rerun least-squares without any exploiting any knowledge of what I did earlier. Remember S1 was already there in my first step. Now I got an S2, but now I'm going to start from scratch. I'm going to solve a new system of equations and do least squares on that. Seems a little wasteful. Know, avoidable if possible. If it's avoidable, you should really avoid it. Okay. Then you get the picture. Then the third guy comes in and I have to now estimate alpha one, alpha two, alpha three. Okay? Observed. He keeps getting fatter, right, starting at S1, S1, S2. So the columns are getting stacked up like kinda like the controllability matrix. Picture that I have nothing to do with this. They also thinks we're getting fat here, also things like getting fat. You need to go on a diet. So S1, S1, S2, S1, S2, S3, and on and on and on. The a transpose. A more and more work from scratch. For every time instant. A transpose a inverse, even worse, taking inverses, what could be worse than taking inverses, right? So we had to take inverses of growing matrices. Africa is getting fatter or picking his inwards and even worse, right? So it's really not what I want to do. Now to our rescue comes orthonormality. Okay? I think I need to. Least-squares is trivial if the columns of a are orthonormal. Okay? So now imagine a had Q1 to Q2 and we already went through this earlier, right? So Q1 has unit norm and it's normalized and it's orthogonal to the other columns. So you can see easily that x, which is a transpose a inverse times a transpose b, is just gonna be Q transpose Q, which we know to be I times Q1Q2 be. So in the second system. So in the first system, I guess I don't have it here. The second system, x least squares, is going to become Q1 transpose b, Q2 transpose b, which is also the dot-product between one and p and q two of B. So note that the first time around the solution was this guy, right? When only one drawn in my neighborhood, I only had to find the dot product between Q1 and b. B, b is my received received vector, and Q1 is a signature of firstborn. Now, when I have two a transpose AB, when things are orthonormal, you just append Q2 transpose dot product of q2 and b to the second entry of my least-squares. Pretty neat. You don't have to do any work, and we you reuse the work that you had. And Q1, Q2, Q3, Same deal. A transpose a is going to be an identity because Q transpose Q is I and X hat, the least squares estimate is going to be a transpose a inverse a transpose b, which is Q1 transpose b, Q2 transpose B transpose B, because a transpose a inverse is I. And just as a reminder to you, I'm using interchangeably the fact that the dot product is the same as x transpose y, which is the same as Y transpose X. Okay? These are, should be well known to you. So this is sweet. So we can reuse our previous work. Anytime you can reuse your previous work, you should be, you should be able to reuse your midterm cheat sheet for your final write, maddening and do it all over my study more if you started from scratch. So that's a bad analogy. But nonetheless, anytime you can reuse your previous work, don't do useless work there. That's the moral of the story. Okay, So the goal is to find my received signal as a linear combination of the signatures. Of the signatures, where we only find out the SI vectors one at a time. So if we can convert these into orthogonal form, orthonormal form, then we are in great trip. Okay? How do we do the orthogonalized station? S1 and S2? I have to write S2 in terms of S1. So this is the projection part that I have forgotten. Now's the time to pay attention. So how do you orthogonalize these vectors? Let's say you have a set of two vectors, S1 and S2. And S2 in terms of S1, It's gonna be has to as bitter as one plus q2. And here's a picture. Here's the S1 vector. This is in 2D. Here is the S2 vector. They are not orthogonal. How do I orthogonalize S2? So let's start with S1. And this is the Gram-Schmidt process which we will walk through in more detail in a minute. But first thing is, let's say it started S1. And I make sure first it's unit norm because we need to have an orthonormal system. So that's easy. That's just a scaling factor. Now I take S2. How do I orthogonalized S2? S2 is, makes an angle with S1 which is not orthogonal. So there is a component of S2 which is perfectly aligned with S1. And that is the useless part because I've already seen that. One already told me all the information that was in the x-direction, right? E.g. because S1 is in the x-direction, now the component of S2 that is new, it's also called innervation in the Kalman filtering language, but you don't need to know that. So S2 should be the orthogonal complement. That has not been seen so far. That's the new stuff. So that's exactly Q2. So S2 is. Projection of S2 to S1. That's beta. One by beta is the projection of S2 to S1. The orthogonal projection S2. S2 minus S1 is the fresh information, q2. Now, Q2 is going to be perpendicular to S1 and Q1, Q1 is just a unit norm version of S1. Is that clear? The geometry should be easy to understand. If you have any questions, stop me because it's important to grasp this. So how do we find this Beta? When this review of projections, you know, q2 is S2 minus beta one. Beta S1 is the red vector. So the blue vector is the red plus the green vector addition. So Q2, there's two minus beta as one. We know that s1 is called orthogonal to q2 by construction. That means that S1, what is Q2? Q2, there's two minus beta S1. So the inner product between S1 and S2 minus Beta S1 is zero by construction, right? That's when I have an orthogonal projection. Well, let's do the algebra S1, S2. So linearity of inner products. So as a type a comma b plus c, a comma b plus x plus a comma C, right? So the inner product between S1 and these two are, Let's do 11 by one, the two inner product with S1 and S2. And then take the, do the inner product with S1 and minus beta. One. Minus beta is just a scaling factor. And then S1 inner product with S1 is just norm of S1 squared. And this is just S1, S2. We know this has to be zero by the, from the picture. So that means what is beta? Betas just move things over. So S1 transpose S2 inappropriately and S1 and S2 divided by S1 squared. So the projection formula for In general, the prediction formula, the projection of S2 onto S1 is going to be the inner product between S2 and S1 divided by norm squared in the direction of S1. This is a review, right? People have seen production. Okay, Good. So, but for those who are a little shady, shaky on it, this might be a good reminder. We need to keep doing this, so this is useful to know. So that's a review of projections that leads us to the Gram-Schmidt algorithm or procedure or orthogonal validation method. Same thing. So we've built up the intuition for doing things. So now we should move fast. So what should we do? You are given a set of vectors, X1 through SN, like those drones signatures. And let's say at the outset they were not orthogonal to each other. But I'm going to make them orthogonal so that I can stack them later. Okay, That's the goal of this. So we want to convert this into an orthonormal set Q1 for q N one-by-one. And the order matters. Because if you swap the order of the SS, then I'm going to start at the leftmost end. Okay, so the order matters in terms of my orthonormal basis. So one through q n. So the inner product between QA and QI, this one and q and q j is zero. That's what orthonormality means. Now, here's what I want. Wildlife orthonormalize are orthogonalized. I want to maintain the fact that I don't lose track of the span of the columns that I am orthogonal, which means the span of S1, my original signature should be the same as the orthogonalized version one. When I get my second one, the span of S1 and S2 should be the same as Q1 and Q2. Span of S1, S2, S3, it should be the same as Q1, Q2, Q3, and so on. So span of S one through n should be the span of Q1 to Q2. And so you have to be respectful of the spans that you started out with, that you don't leave in some leave the system and some shaky spit, right? You have to maintain the states. Okay, so, so consider linearly independent. So we are going to, for today, given the amount of time we have, we're going to assume that these SIs are all linearly independent. They are not orthogonal, but they are independent. Okay? So I'm going to show us how the Gram-Schmidt algorithm works. Say, let's say, we'll do an example for three. It's the same for n. You can read the notes for that. S1. I have to first, I'm going to start at S1. Since I need an orthonormal system, the first thing I do is make sure it's unit norm. So Q1 is the unit norm version of S1, which means that Q1 is S1 divided by the norm of S. Well, simple. What's next? You have to check is the span of one the same as pan of S1? Yeah, of course, scaling doesn't do anything for Spanish, right? You're gonna be spanning the same space. So if S1 is the x-direction, Q1 is also in the x-direction. Same span. Okay. What about S1, S2. Let's put it in the box. In the box I'm going to put all the stuff that I have done, all the work I've done so far. We're not gonna do any useless work, only useful work. So Q1 is S1 or norm of S1 is in my box, is my first vector in my orthonormal set. Second, what's new and S2 are not captured by Q1. We just went through this argument. We have to remove from S2, the projection of S onto Q1. So same picture that we saw earlier. Here's S2. Please strip anything that is in the same aligned with one because that is not useful to me, has already been done and only give me the orthogonal or fresh or innovative dimension. So E2, S2 minus the projection of F onto, onto one. But that's the projection formula. So S2 comma Q1 divided by Q1 squared. Remember, we just derived this and you saw it last in 168. So this is the projection coefficient for one direction. But you know that Q1 squared is unit norm. We have that Phi, we took the care. So you don't even have to worry about the denominator. So the denominator is always going to be one. As long as you orthonormalized prior to coming there, which we did. So E2 is S2 minus x2 comma one in the direction of one. And that's the direct, That's my cue to vector. But it's not scaled yet. E2 is not scaled because e2 is S2 minus this, but this may not have unit norm. What do I do? Make it unit norm? So divide E2 by its norm. And now Q2 will have unit not very straightforward, right? So I start out with my S1 direction. Then my S2 was pointed that way. I want the component orthogonal and then make it unit norm. Because Q1 was unit norm, I need to leave the system in the orthonormal state that I founded in. So Q1 was already given to me in normal form. Qh2 is orthogonal to one, and now it's also a normal phone. I'm done with two. And now you can guess what's going to happen. This is where the orthogonality, it's also going to be hard for me to draw. So fun thing is you should check from the geometry. It's very clear we have chosen the Beta such that things are orthonormal, but it doesn't hurt to check, right? So let's check its Q2, indeed orthogonal to Q1. Q2 is just a scaled version of E2. We know from the picture it should be orthogonal to Q1, but let's check it algebraically also. So Q2, Q1, this is q2, right? E2 divided by its norm. Remember that's what we said Q2 was. Let's take the inner product with q one. What is e2? E2 is S2 minus the projection onto Q1 divided by the magnitude of, it should be by the magnitude of E2. E1 just copying through. Now, this is just a scaling factor. I can bring it out of the inner product. Then I do a component-wise inner product of S2 with one and then has two comma one, which one? This is also a scaling factor, is just an inner product, is just the number. So this is just like this. This also can be pulled out and then you'll have Q1, Q2. I'm just doing linearity of inner product, okay? Now S2 comma q one and has two comma 11 comma one is one because one has unit norm. So I have as two comma one minus x two comma one, which is zero. Indeed, further span of S1 and S2 is going to be the span of Q1 and Q2. The geometry should be the way you should view it. But the algebra just confirms that your geometry is correct. Every now and then you might, your intuition may be off. So it's good to not take it for granted. Now, let's do the last case. The third one, I still have a few minutes. We'll just finish on that node. So in my box, I have put in, so I have managed to orthogonalize S1 and S2. So that's my work done box I started at Q1, now I finished q2. Now what do I do? Here's S3. So in general, Q1 and Q2 are going to be on the plane and they're going to be at nine. And my picture is not good but the supposed to be 90 degrees, but okay, so that's to be 90 degrees on the plane. Now as three comes out. And I'm gonna project S3, find the component of S3 that is not. So if S1 and a, Q1 and Q2 are on the x, y plane, and S is some arbitrary 3D vector. What is the new dimension that F3 introduces? In the z direction? How I, x and y are covered. So I have to project, remove the projection of S three onto stuff that I already know. But the projection of my S3 onto this plane is the same as the sum of the projections onto the x and the y, because that's the orthonormality beauty. So I take S3, I want to project it onto the plane that's at 90 degrees and only take the component that is missing, that's not there. So when you work it out, the projection is F3 comma one. You project S3 onto the span of Q1 and Q2, which orthogonal vectors. But because they are orthogonal, I can separate the projection operations because they have no coupling, because they are orthogonal. So it has three comma Q1, Q1 plus Q2 with Q2, exactly as we saw earlier. And then I strip that. And what's leftover is the new component. That's the component that is orthogonal to everything I've seen so far. And that is e3. Then I need to normalize it. So I make sure I divide by its norm. And I check that I need. I have Q1, Q2, Q3 are spanning S1, S2, S3 as I was required. And you can also check that Q3, Q1 and Q3 Q2 will be orthogonal because you add a new component. My column space, instead of being some arbitrary independent space, is now this beautiful orthonormal basis, which is spanning the same space as the original vectors. Okay? Alright. That's the end of the lecture because we've run out of time. Thank you. 