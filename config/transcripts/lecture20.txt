Okay. Welcome back from spring break. Everybody have a good break. Rested. Well, you should be hopefully a recharge to go because we have a month left. Some announcements. The TAs wanted to remind you that the students support hours are available Mondays one to 03:00 P.M. this is everybody is welcome. But particularly those who are falling behind or think they can't catch up, or they didn't do well on the midterm, you know, those students are probably more should, you should definitely avail of this opportunity, the resource. And you can schedule an appointment on the class website, on the calendar. And I can see from the attendance today that the word has gotten around. Finally, we are figuring out what it takes to get you to come to class, to incentivize you to come to class. So there's a link document that NEMA will show up at some point and he will take, Don't worry about how it happens, it'll happen. But the links document, the links on this page is going to tell you how you can get it. So have you recovered from the April 1 joke? No, it's not a joke. We have a midterm two. No, just kidding. Okay, good. So there's a lot to be done, especially today. And fair warning, today's class is probably the most mathematical of all the lectures. Okay? So if you like that, great. If not, no. Try and hang in there. Okay. I'll try to make it as easy as possible. Okay. Last time we had done upper triangular authorization and today we're going to move forward. But I know that many of you would not have remembered upper triangular authorization. You remember what you did during spring break, right? Myself included. So we'll need to kind of get into the mood. So we will talk about, we will review upper triangular realisation. But I'll do it somewhat briskly. So if you don't have to take notes on that part because it's all gonna be available. And we've already covered in the last lecture. And basically we'll talk about this, but basically it's to cover the fact that any square matrix can be upper triangle erased. This is also known as the shore decomposition. I don't know if we mentioned that, but upper triangular authorization and sure decomposition are the same thing. So if you want to sound fancy, you say short decomposition. We will cover the proof of upper triangular authorization that every, every square matrix can be upper triangular raised. And we will look at the eigenvalues of upper triangular matrices. Then we will revisit BIBO stability for non diagonalizable systems. And to supplement the lectures, there's lots of resources available including the nodes. Node 13, which is posted on the webpage, is particularly you should look at, Wow, I love the attendance. Okay, recap from last time. We start with our system state equation. X of k plus one. K, x of k plus b U of k is known as the system matrix. And just a reminder that X has dimension n by one. A is n by n square matrix. B is n by m and U is n by one, right? This has been our convention. And we know also that a is diagonalizable if all the eigenvectors of a, n of them, since a is n by n need to be independent, right? That's, that's what it takes for a to be diagonalizable. And in fact, the diagonalizable, the diagonal, you diagonalize it with a V inverse. A V will be diagonal. And what will be contain the columns of the columns of v will be the eigenvectors, right? And if the columns are all independent, then you have a basis and you're done. So it's diagonalizable. Now, your BIBO stable. If your diagonal. We've studied this when things are diagonalizable and conditions, if you're BIBO stable, if the eigenvalues of a are all inside the unit circle right there. So to recap, then that raises the question, what if a is not diagonalizable? Because life is not always right? I think in your homework, this homework, you have a circuit problem which is not diagonalizable. If you have gone that far, but you will see it when you go. So examples of non-diagonal laser systems include is that looked like this, which you can see has repeated eigenvalues lambda and lambda. And then if you examine the eigenvectors of a, I'm not gonna go through the math. We went through this last time. You will see that both eigenvectors are pointing in the one-zero direction. So you don't have two eigenvectors, you have only one because you are degenerate, so you cannot diagonalize. So what do we do? Are we stuck? Can you say, talk about BIBO stability when you're not diagonalizable. That's what we use survey today. So the next best thing to diagonalize ability, if that you are upper triangular matrix should be upper triangular as in this form, right? So everything that's shaded, if stuff non-zero and everything unshaded, it's all zeros. So it's an upper triangular form. So if your matrix is of that form, it's the next best thing to being only non-zero on the diagonal. And a quick preview of this, which we will also cover later, is that if you have an example of dx d t to be, say this is your a matrix lambda one, lambda two times x of t. If you look at the second equation, you see that dx2, d t is lambda two x of t, x2 of t. And dx1 d t is lambda one x1 plus x2 of t. I'm just writing the matrix equation out into two scalar equations. And you can see that the second equation tells you all you need to know, e.g. if you want to know, if you are stable, what do you do? You check for lambda two because it's not coupled with X1, X2 all by itself. Now you solve for that. You can plug it into the first equation. And if you know your stable, the next of x2 will be bounded. And then you can say that x1 is bounded also if lambda one is less than one in magnitude. So that's the way we will proceed for back substitution. All this should be familiar to you. Now question is. Can we convert the sounds promising, so we couldn't diagonalized, but hopefully we can upper triangular is, can we do that for any system whatsoever? Can any square matrix M, we'll talk about, we'll switch to m, be put into upper triangular form using a basis transformation. And we've studied that. The answer is yes, otherwise we wouldn't be talking here. And you can always find a change of basis where you inverse m mu equals t. Whenever I use T, the symbol T for upper triangular. So T means you should be thinking like this. And because the change of basis, it also turns out to be orthonormal. In other words, U inverse is the same as U transpose. We don't like taking inverses. So if you can find a basis where all the basis vectors are orthogonal to each other, then that's the best possible scenario. So you will have u transpose equals U inverse. That's what orthonormal means. And we will show that any, given any square matrix M, whether it's diagonalizable or not, can be upper triangular risible. Using the change of basis. The change of basis matrix is orthonormal. Okay. Is that clear? It's just a recap from last time. And how many of you weren't there? That's got to be a record. This is this is sweet to see. Okay? So you is, this is the orthonormal matrix. The change of basis matrix U is u1 to un. And Orthonormal means that the inner product between u i and u j for i naught equal to j is zero. And the Orthonormal means you normalize it to one if I is equal to j. And the point is that the punchline is any square matrix can be transformed into an upper triangular form. So this whole still review. And just a reminder that upper triangular means the same as assured decomposition, okay? Okay, simplest case. So we're going to prove this. And in general, the proof is, I mean, I wouldn't say it's trivial, but it's, it's, it's kind of involved. So we will go through the steps of the proof. And just to build intuition, we will start with small examples. So let's start with what is the simplest example? That'll one-by-one. It's not even a matrix, right? But technically it is. And is it upper triangular? Sure. Is a diagonal. Sure. It's everything you want it to be because it's just one number. So m equals m is done. I'm done. So now, what about m equal to two? So you'll have two entries, two-by-two, 11122122. And for convenience, we're going to assume that all the eigenvalues of M are real. I mean, the proof doesn't need it in general, and it will be more clutter to bring in complex eigenvalues. So if you understand the real setting, you're good to go, okay? You can easily generalize. So what is the use? So we have to find a U which will upper triangular rise any given m matrix, any two-by-two matrix. So we discussed this last time and we saw that probably given our insights from the diagonalization setting, Let's try an eigenvector because I'd worked really well in the diagonal case. So hopefully it will work well again. So let's try eigenvectors, and let's pick an eigenvector. We know how to find the eigenvalues for m and find the eigenvectors for em. And so we solve for them. If pick any eigenvector that you want, MV1 equals lambda one v1. So v1 is an R2. And we'll assume that it's normalized, so it has unit length. And we will start with V1 as the first column of U. We need a two-by-two for you, right? So I need one more column. That's the shaded. What should we do? We want an orthonormal system. We know we have one vector which you are trying to trial. The first vector is the eigenvector of N. One of the eigenvectors. Pick anyone your, your, your choice. What should the other be? Yeah. Negate one of the values. You want something that's orthogonal to this, right? What is the systematic way of finding an orthogonal basis? Yes. That Gram-Schmidt, exactly. Remember we studied Gram-Schmidt. You have a homework problem to this week. How do we orthogonalize this? In a two-by-two case, you might do things like negate. What if I gave you a three-by-three or five-by-five, what do you do? In general, of course, I am giving you the two-by-two so you can give me the two-by-two answer. But in general, what do you do? How do you do the Gram-Schmidt? What does the process? Yeah. Yeah. So what she says is, you start with any standard basis vectors, say 10, we call them the EI is, write E1 is 10e is 01. So we will start with 10 added to the set and find the orthogonal complement of 10 to what has been shown. If it is a non-zero vector, you're done. Otherwise, move to E2 and try again. We went through this, right? And so basically, you know, you can complete the orthogonal set. Okay? So you build out the basis you think Gram-Schmidt. And let's say that the basis that completes it after you're finished, Gram-Schmidt is R. We'll call that vector r. So that means I have V1, sorry R1. R1 is the vector that completes the basis with v1. So v1 and R1 forms an orthogonal orthonormal basis, which means that their dot product is zero, the inner product of zero. And then let's see if our trie worked, right? We tried this particular choice for you. And let's see if indeed you transpose MU is equal to a triangular matrix, upper triangular matrix. So let's do that. So v1 transpose v1 transpose v1 and R1 transpose m v1, R1. Just writing it down. Again. All of this is from last lecture, so we can write it down using transposes, v1 transpose, R1 transpose and V1 R1. But we know that m times V1, V1 is an eigenvector for m. So m times v1 is what? What's a simpler form for M times v1? Yeah. Yeah, lambda1, v1. Yeah, because M1 times V1. We haven't used it yet. Let's multiply this through Mu1 transpose m v1, upper left, v1 transpose M R1, upper right, R1 transpose MV1, just multiplying it through our own transpose MR1. Now we use that R1 transpose m v1. The bottom left is gonna be R1 transpose m v1 is lambda one. Lambda one. I made the same mistake. Is lambda one v1, which is lambda one times R1 transpose v, where you can move the lambda one. Since it's a scalar, you can move it to the front. And then you have R1 inner product with v one, which is zero, since R1 and V are orthogonal by design, by construction. So we know that this guy is zero, right? That's what the lower left-hand corner. Okay? V1 transpose m v1, the top left corner is going to be again replaced MV1 with lambda one, v1. And that'll give me lambda one. That's the top left corner as V1. And so for love, we are in good shape because we have our Lambda one in the top left corner. And you have something here and something here. The important thing is that the lower left is, the lower left is indeed zero. Yay, success. So we should declare victory and say the general formula for doing the us, find any eigenvector, complete the basis using Gram-Schmidt and you're good to go, right? Not quite. Life is not so easy. It looks very deceiving in the two-by-two case. That's where, you know, taking too small and example can sometimes be dangerous. So it turns out two-by-two kind of works, but it doesn't work completely all the time. And to verify that, you can start trying to do a proof using this, using math. And you'll see that you'll get stuck. So let's build more intuition. What's after two-by-two? Three-by-three. Let's try three-by-three. See what happens. Before we try that. Give you, I'll give you an example of the short decomposition or this upper triangular realisation for a two-by-two matrix. By the way, it should refer to naught 13. For any gaps and understanding or any details that I may miss lecture, I cannot do everything here. So here's the example we'll just run through. I'm, M is the matrix that we want to portray Angular Oris. So it has entries 01 minus two, minus three. I'm just, this is an exercise in linear algebra of doing eigenvalues, which you guys should be familiar with. So find the eigenvalues and the eigenvectors of a shown here. Because the way to do the eigenvalues, you find the characteristic polynomial determinant of lambda I minus m. Factor things through and you'll see that the two eigenvalues are minus two and minus one. And the eigenvectors corresponding to minus two and minus one are v1 and v2 respectively, which are shown here. Okay? You can construct that. I'm not gonna do this year. I know your contract, I'm confident that you can do this. So what did the algorithm say? Let's, let's take v1 as our first starting vector for the first column of U, V1 and R1 is going to be the, as we said, we talked about it, that we will complete it using the Gram-Schmidt by doing Gram-Schmidt on v1, 1001, the standard vectors, and then finding the orthogonal complement of the orthogonal to v1. When you do all that, you'll see that it's R1 is given by this. So you can check that V1 and R1 are orthogonal to each other. And so u is going to be U1, R1. That's what the prescription was, just giving a concrete example. So you compute u transpose MU, and let's check here is u transpose, but you are symmetric, so U transpose is the same as you source. Basically. This is u transpose, this is m, this is you. And you multiply things through. And indeed, as advertised, you get a zero in the lower left-hand corner, which means this has upper die, upper triangular. Is that clear? Give me a thumbs up. Okay, good. Alright, so, um, now, okay, So the two-by-two certainly works. But let's not get too cocky because this doesn't work in general, but it almost works. So we need to do some tweaking. Some tweaking is needed to complete the story. So that's what we should do now. So this is all still review. Okay. I'm recapping from all of this I did last time. So let's build from the two-by-two case and go to the next best case, which is three bytes. The next smallest get three-by-three. And we will assume that again as before, we will assume that M has real eigenvalues. Just for sanity, you don't want to carry around complex numbers everywhere. And you transpose mu equals t. This is what we conjectured. And we want to be able to show this as I'm, as I'm writing here. Because I'm feeling a little bit uneasy. I have a bad feeling. I'm not going to be cocky enough to call this view, but to show my modesty, I'm going to put a Tilda. That's my humility to show that I'm not sure maybe u tilde as the same as you, but I need to prove it for now. I'm going to not take any chances and call it u tilde. And u tilde dA is going to be like, like before. Let's find an eigenvector for m v1. And then let's complete the orthogonal set, right, using Gram-Schmidt. And note that m v1 is lambda one v1 just like before for the two-by-two case. Then we will use D1 to complete the Gram-Schmidt process and bring two more vectors which are orthogonal to v1 and orthogonal to each other. Full orthonormal set, which is what Gram-Schmidt does, right? Then let's, let's abbreviate our notation because R1, R2 is too much to carry around. And later when we go to n and getting large, I wanted to do R1, R2, R3, R4. So we'll call the hormone R2 set to b are okay with just colored called the collection to be our, note that R is going to be 23 by two, right? This is R1 and R2. So here's V1, R1, R2. So r is three by two. V1 is three by one. So use three-by-three u tilde, that is three-by-three. So are we good with the construction? So we're going to try this utility. This is exactly generalizing what we had for the two-by-two case, right? The two-by-two, this word. So if it works for three-by-three, then I know it's got to go. I can prove it later. But let's see what happens. First thing is you should convince yourself that you Tilda is orthonormal. In other words, these three is an orthonormal set. That's not so hard to do given that you constructed it using Gram-Schmidt. It damn well be orthonormal. Otherwise, what are you doing here? That's the whole construction process. Okay? So, but nonetheless, for completeness, you should check it. So you can do that. Okay, I'm not going to go through that here. Now. Let's put it to the test. What is u tilde transpose m, u tilde. If life was good, this should be what? It should be upper triangular, right? This should be upper triangular. If our conjecture is correct. Well, is it? Well, let's see. What is V1? So let's try u tilde is just v1 transpose R transpose m is m u tilde us D1 are, and what do we get? Okay, now we get a two-by-two system. What is the upper-left corner? That's v1 transpose m, v1, which is the same as v1 transpose Lambda one, made the same mistake. Lambda v1 transpose v1 transpose v1, v1 has unit norm. So this is just lambda one. So upper left corner, like before, in the two-by-two case also it was lambda1. In the three-by-three case also, it's lambda1. What is the first row is going to be v1 transpose m. It is what it is. I don't know. It's some non-zero stuff. It's not important for upper triangular authorisation because the first row can be anything, right? It's to be upper triangular. This is the important column. First column. What is the first column? Hard transpose m, v1, transpose m v1. Again, let's use our eigenvalue knowledge m v1 is lambda one, v1 move lambda1 over to the left since the scalar, and you have R transpose v1. What is R transpose v1? Ours are the orthogonal vectors to V1 by construction. So R transpose v1 is zero. This vectors, it will be two by 10 because it has two vectors, R1 and R2. So this is zero. Wow, we are almost there. Two upper triangular you're stuck with hard transpose, MR. that's the lower left lower right corner. Are transpose MR. Is two-by-two. Upper triangular. What do I need to have upper triangular? Anyone? Yeah. Yeah, you need R transpose MR. to also be upper triangular. What does that mean specifically? It means that this guy should be zero. If this guy is zero. Because if you look at it, I have lambda one. I have something here, something here, zero-zero, right? I have something here, something here, something here. If this was zero, then you're done. Then and only then will you be upper triangular for the three-by-three case, right? But who told you that's not going to be zero? So our conjecture failed. Failed, but it kind of past. There's good news and bad news. What's the bad news is evident? It didn't work. What's the good news? It's always a silver lining rate unless it's a really bad storms we've been having. So what is the silver lining in our in our construction, in our in our approach? Something good that came out of this. Yeah. That was before. I agree with that. But even for the three-by-three, I'm saying for the three-by-three, all hope is not lost. Why not? Yeah. It's very close. And in what way is the close? Your first column is good to go, right? Yeah. What are that? It is not the way the first column zero. Okay? Okay, Let, okay, Let's go through that. That's important. That's so let me it's kinda shown here, right? Let's go through the first column. The first column is what? The first column is. R1 transpose m v1, right? So that's here. M times V1. Remember V1 is an eigenvector for m, which means m v1 has eigenvalue Lambda one. So v1 is this lambda one, v1 lambda one is just a scalar. So you can move it to the front. You can move around scalars anyway you like. So you have R transpose v1 transpose v1. What is our callback? Is R1, R2. How did I get R1, R2? By Graham Smith. So by Gram-Schmidt, the V's and the Rs are orthogonal. So R transpose V by Gram-Schmidt is gonna be zero. Okay? That's a good question. Yeah, ask questions if you don't follow up because I don't want to lose you because it's gonna get pretty hairy soon. So if I lose you now, it's not a good thing. Any other questions? Okay. So we talked about let's go back to the three-by-three case here. And we said, we need this to be also zero. So let, let, let me erase this. So we need, we need, we need this guy to be zero, but we need three zeros. We got two out of the three. That's progress, right? Maybe it's not. But we hope that the fact that I was able to clean out the first column, what is upper triangular? That the first column should be zeros below lambda one after the first entry, the second column should be zeros below the first two entries and so on. That's what upper triangular means. So we, we succeeded with the first column, but not the second column. Okay? So r transpose m, r should also be upper triangular as we discussed. But looking at our construction, as I mentioned, we have a good, We are upper triangular as far as the first row and first column are concerned. But the problem is the middle two-by-two section, right? So bad news is it doesn't quite work. The good news is, I think as somebody pointed out, is that we have managed to reduce the three-by-three upper triangular realisation problem into a two-by-two upper triangular radiation problem. Anytime you can take a big problem and reduce it to a small problem, that's progress. Because you can then take the small one and even smaller until you have nothing left, right? They can take the big one. You raise it to a smaller one, smaller, smaller, smaller, smaller until you're done. So that's the intuition. But we still need a, a U that is going to work. So how should, how do we do that? So let's go through that. And let's consider. Let's do this together. We'll write the reduced problem, two-by-two r transpose m. Now, the key is that we have already studied the two-by-two case, right? I think you pointed out. So from earlier two-by-two, from earlier two-by-two case. We know that there exists. And orthonormal, I'm going to abbreviate all n. Whenever you see 0 n, it means orthonormal. Orthonormal U2. I'm going to abridge, I'm going to have subscripts for the use just to identify what size they are after the U2 is for the two-by-two, you three for three by three and so on. So such that U2 transpose. Remember the metrics that we want to diagonalize, upper triangular rise is R transpose MR, Right? That's my two-by-two guy. So we want u transpose times r, U2 transpose times r transpose m r times u2. We know that this is upper triangular, so I'll always abbreviate upper triangular like that's the symbol for upper triangular square with the upper part shaded. We know this because we did this. We know that for two-by-two case, there exists some orthonormal system which is what, which are the eigenvectors. You take any eigenvector of this guy. Now the heart transpose M R is your, think of this as, this is just a matrix, right? Just a two-by-two matrix. And we just studied earlier that any two-by-two matrix can be upper triangular raised using V1 and R1, right? That corresponds to the eigenvector and it's Gram-Schmidt complement. So we know this is true. So we also know that U2 transpose transpose. I'm just writing this out. Our m, r, U2. This is the left-hand side. Let's write this as. Our U2 transpose and just using the fact that when you do transport the other switch the order. So U2 transpose R transpose R times U to the whole thing. Transpose times M times r, U2 is equal to T2. We know this to be true. This is upper triangular. Let's kind of shade this. I'm going to collect this together. And I'm going to call this a starred equation. We're going to use the starred equation later. But let's collect it. What, what we have shown us, we know how to diagonalize a two-by-two system. Okay, good. Now, we can also, there's a slight amount of detail which I'm going to just talk through now. We'll discuss later, which is what? Remember our VM, we wanted to do that. What we showed was any two-by-two matrix which has real eigenvalues is upper triangular feasible, right? So we still have a little technicality that I need to show that R transpose MR has real eigenvalues, right? I didn't show that. It turns out you can. And it basically boils down from the fact that the eigenvalues of this matrix, which is my three-by-three system. The eigenvalues of this, because of the all zeros on the first column are lambda1 times the eigenvalues, your lambda one and the eigenvalues of a transpose. Mr. You can show from linear algebra, Laplace expansion or whatever. Basically, you can take the determinant of this matrix. It's lambda one times the determinant of this guy, right? That's all it is. And the determinant of this guy will exactly contain the eigenvalues of lambda I minus that. So I'm not gonna go through the details. But you should, however, convince yourself that if the whole thing has real eigenvalues, That's our assumption. And you picked one of the eigenvalues which was real, then what's leftover will also be real. So this will, this will also have real eigenvalues. But you need to kind of convince yourself. But we won't go through that right now because it's too much. So let's, what is the idea? So this is already giving us something, right? Because this is a matrix. We know that this is a matrix. This is a matrix. They are two-by-two matrices matrix. So the idea is that instead of trying, which was our first try, u equals v1 are right. This was the trie that almost worked, but didn't quite. And I'm going to use a subscript of just as a reminder that I'm going to put the three in there to make sure that I identify this three-by-three setting because we're doing that as we did on our first try. And it didn't quite work. So based on this insight, based on the above observation, let's modify Our try. We'll do it take to take one, kind of failed, but somewhat worked, but hopefully take two will work. What do we do instead of doing R, which is the Gram-Schmidt stuff? What is a good try based on this equation? Are you to standing out? So let's try that, right? So let's try our YouTube. I'm going to modify my try. In the beginning I had v1 and then r, which is the Gram-Schmidt completion of V1. But now I'm going to tweak it a bit based on the observation that my T2 is diagonalized by, not U2, but by our YouTube, right? So our YouTube is what I should use. Because that will diagonalize the two-by-two case. So that's my try. So let's see if this works. So this is going to be v0, v1 are, this is an interesting expansion. So you can see that if I were to, right v1, this was my old one, right? This is this is the new trie, which I'm boldly claiming to be U3, although I should really not call it U3, but I'm kinda confident this is going to work. So I'll get rid of the twiddle factor. So this is of course, you three tilda, right? This was our first try. This is our second draft. You three tilde as a first try, new trees or secondary. So what do we do? Well, you can work it out. This is 102 transpose, zero to transpose. So let's talk this through. Okay, Let's talk this through. So we want to try, I want to make sure this doesn't look like a U, u's and v's look. This is v1 TV. So I want to try V1, U2, which you can write as V1 are without the U2, and multiply it by this matrix here. What is this matrix? If you observe, if I multiply this through, you exactly get the left-hand side. It's exactly like doing because v1 times one plus R times zero gives you V1. V1 times zero plus R times you to use your YouTube. So definitely the operation is successful. But what we have done is that we have the U2 is two-by-two. I want something which is three by three. So I stick zeros in the front and a sticker 1000 vector as the addition. So if you want intuition that what it is, but this is just math. Unfortunately, you can't get around some maths. So this is exactly what it is. So you see here that this is an important equation. So let's collect this together. So what is U3 transpose M, U3? Let's try now. If U3 transpose M, U3 works, there is. So this is going to be v1 transpose and our U2 transpose times m times v1 vectors. Now, our U2, right? Let's, let's see if our choice works. You should also check that indeed U3 is orthonormal. We'll do that in a minute. But you can see that U3, which is a product of u three Tilda and this, or if you look at it this way, all you did was multiply you to an hour and they're both orthonormal matrices. So you're gonna get orthonormality, but let's check that later. But let's first check if indeed you get a if this works. So what is this going to be equal to? This is gonna be equal to v1 transpose m v1. We've seen that before. The second one will be v1 transpose m r U2. And then you'll have U2 transpose R transpose m, v1 and v2 transpose R transpose m, r, U2. This is the important one. What is that entry? Yeah. That's T2 has exactly what we did here, right? Look at the green starred equation are two trends are to our U2 transpose m r U2, which is the same as when you use R times U2, has the basis change of basis vector matrix. You exactly get this. So this, the key is to recall this is equal to T2 from star equation, the green star equation right here. That's why we have done that. So indeed, our choice works because this is still zero. So this is going to be just like before. This is going to be, the first entry is going to be lambda one. Then the other two are going to be zero-zero, right? Just like before, because, because of the Gram-Schmidt construction. Well, let's go through that in case people are confused. Okay, maybe we will do it here. So this guy is what? U2 transpose R transpose m v1, which is gonna be U2 transpose R transpose lambda one v1, which is lambda one, U2 transpose, or transpose v1, which is R transpose v1 is what? V1 is the first eigenvector. Our consists of R1 and R2. How did we construct our Gram-Schmidt? So what is the R transpose v1? Zero. Whole thing is zero. So zero at the right length. So this is zero. So that means that this is going to be, I need some more space. So 00, this is v1 transpose M, U, U2. And here I have where we just showed that this was T2. So that means this is a zero. So I don't need this. That's a zero and therefore change colors. So you can see here that I'm upper triangular. Yay. Success at last. And this scheme will always work. So you have to tweak your YouTube. Your are rather to include the U2 on the right, which is the U4, which is the change of basis for the next smaller size. Okay, so this will generalize. So I just wanted to get you, give you the intuition. You can always prove this. But pure math with no intuition whatsoever. But I hope by going through the two-by-two and the three-by-three case, you get some sense of why we did what we did. Okay, so that's the key. So now we will whisk through the rest. So let's write down our success story. So let's make note. So this has indeed upper triangular, so we'll write it in red. So you three, which is equal to V1, followed by our U2, is an orthonormal basis that you should, you should verify that it's orthonormal. Okay. But I didn't verify that it's fairly straightforward. This was the, this was the more interesting part. That upper triangular matrices M three-by-three, okay? That is U3 transpose times m times U3 is equal to T3, which is also open. So. Okay, good. So let's continue. And that's what we did. I just read this word out. Now. Now let's take stock of what we did, okay? And here's kind of the argument. So you have a three-by-three case, the M3 that we did. We first tried U3 Tilda, which was consists of V1 and our right, this all Gram-Schmidt try eigenvector plus Gram-Schmidt completion. And that resulted in, we'll call it T3 tilda because it's not quite upper triangular, but with the Tilda in the sense that it can have there. So you had, the first column was zero in the upper triangular form. But then in the middle you had an M2. Then what we're saying is we said that from the previous try, we know that any two-by-two could be upper triangular raised. And that's YouTube. That was using YouTube, right? So you do upper triangular rises, this hooks, I want to do that. So U2, upper triangular rises, this guy, this two-by-two part of the three-by-three exactly lecture on here. And let's say just to distinguish which stage wherein we'll call it Lambda one at stage three. And this is lambda one at stage two for the superscript notation, but it's just to distinguish. So we know that this is upper triangular. Once we compose U3 tilda with a U2 for the two-by-two guy. But you get a three-by-three. So this is taking stock of what we've accomplished. Is that clear? Yeah. Yes. Yes. Good question. Hold your horses will be there in a minute. This method will generalize. I want to show you the method for 3.2 so that you get a sense. And then we'll see what happens when is much larger than three. Okay? And so this is more of the same, you just writing that U3 is the same as U3. The first try times the expansion that we just talked about. And which gives you this. And basically you're, if you use U3 tiller as change of basis, you don't quite get there. But when you, But you also know that U2 will be change of basis for M2 to T2. Now, if you compose U3 and U2 in the manner we just talked about, and you call it new tree, then you're good to go. Okay, There's just summarizing everything that we've seen in the lecture so far. Okay, good. So taking stock, we start with the three-by-three. Then we reduced it to the two-by-two. Then use the solution for the two-by-two to tweak back the three-by-three. This is what we did. It's a slight tweak. Okay. So your question, Hey, but what about 50 by 5,000 by 100? Any k by k. We'll see what happened for the general case. Say r k plus one by k plus one for k greater than or equal to one. So go exactly same method, MV1 as lambda one v1. First find start with an eigenvector, that's lambda one, v1 and lambda1, the eigenvalues. Then you pick u tilda. This is the trie that is not going to work in advance and type morning you, but it will almost work, which is V1, followed by the completion by Gram-Schmidt. Exactly like in the three-by-three case. Now, if you work out u tilde transpose m, u tilde for the k plus one by k plus one. Just writing things through. You will get exactly like before. You'll get your first column to be all zeros and the first row to be v1 transpose MR and the lower K by K sub matrix to be R transpose MR. but now the dimension m is now fatter, bigger. It's not no longer two-by-two. It might be k by k. Now, this is a k by k matrix. We started with a k plus one. This is row one, column one. So what's leftover is k by k. So this k by k. So we have reduced the problem of upper triangular arising the k plus one by k plus one to one of upper triangular arising the k by k. And so we can, but r transpose m, r is not necessarily upper triangular. So let's tweak it from utility u, where u is v1 times R times UK. This was YouTube for k equal to two. For the general case, it will be UK. Okay? So, so it'll be whatever upper triangular rises, the k by k. So our UK, the UK upper triangular matrices are transpose MR. So if we assume that there exists an upper triangular riser for a k by k case, we have shown, we can do it for the k plus one by k plus one. And that is proved by anybody know the word induction? Yeah, exactly. Because you start with two and you go to three, and then you start to three and you go to four. And from four or equal to five, we started any k. You can go to k plus one. That's what this construction shows. We know it works for two. So it works for three, so it works so forth and so on forever. So that's the proof by induction. Okay, so completing things through, so you can upper triangular, so it is UK transpose or transpose as TK. Exactly mimicking the three-by-three construction. So if you follow the three-by-three, you're done because 33 becomes k plus 1.2 becomes K. Okay? So for everything that follows. So by following the exact analysis we did for the three-by-three being built out, the two-by-two. You can show that the k plus one by k plus one can be built out of the k by k. And this is exactly going to be upper triangular by our construction. And we're done. So reduce to triangular arising in general. This is for the n by one case I shouldn't be. So we know two-by-two works, so no three-by-three works and so on. So the formal statement, or informal statement, is that for k greater than or equal to one, if k by k matrix can be upper triangular, then so can a k plus one by k plus one. That's a proof that a rigorous proof. There's nothing more to this. So this is also called proof by induction. If you want to see a proof by recursion, look at node 13, okay, So what we did is not what a node 13 That was approved by recursion. Recursion you go the other way. So an induction, you start with small and you build yourself up. Now, recursive proof is actually more intuitive in the sense that it lends itself to an algorithm. So you need to find an algorithm for upper triangular arising agenda. What we did was a math proof, right? It didn't, it didn't give you an algorithm. It almost gave you, but you'll have to think about it. If you want more details. I don't have time in lecture and go to discussion tomorrow. They're going to discuss this. Okay? So if, especially if you're lost as high was when I was first reading this, it took me a few tries to get it. So I wouldn't be somewhat confused. It's very natural. But you should have something going through and then go again to discussion tomorrow. The way you master this is keep reading the material. You need to add some effort. Without effort, nothing will happen. Okay? So you go to discussion sections tomorrow. They will exactly discuss the recursive version of the algorithm that comes out of this, which I have written here, but I'm not going to go through because it is exactly in the notes. Okay. Forget about this. Forget that I even have it here. Okay. I wanted to discuss two other things. Okay? This is important. We're done with the proof, at least to the extent that we wanted to show that every conclusion is every square matrix can indeed be put in upper triangular form. That should be clear, right? We prove this by induction in lecture. And there are other proofs also. Now. There are other things about can we say something that relates the eigenvalues of an m to the t. Remember t is an upper triangular form of m, right? So t is u transpose M. So that's exactly what we're doing here. So m is u transpose T U, U inverse and Nu transpose are the same since u is orthonormal. So if m is U inverse, Oops, shouldn't be doing this. So if m is U inverse t u, then the claimants that the eigenvalues of M and T are the same. There's also sometimes called a similarity transformation. So in other words, if, if you can relate to matrices M and T by m equals U inverse t, you change your basis. Then the eigenvalues in the, in this domain and that domain are the same. Intuitively it's clear, but we need to prove it. And here's a kind of a slick proof. There is approved that is not slick. I won't show you that since we don't have time. Here's the neat proof. Okay? So the way you do that is you suppose that lambda and V are an eigenvalue eigenvector pair associated with M. Okay? Then you know that m v equals Lambda v, That's the definition of eigenvalues. So u inverse. So let's try it. We know that m is U inverse TU, right? From here. That's a given, that given to you. So you inverse T U V equals lambda v. So what is this saying? Well, let's try and remove you from the left-hand side. How do I do that? I want to, I want to make you inwards gone. So I multiply by u on the left. So then I have to multiply by u on the right. That's what I do. So I do. I multiply it by u on the left to get u t u times v. That's going to be lambda times uv. Well, you can call UV to be w, some other vector. So you see that TWO equals Lambda w. That's an eigenvector equation. T times w is a scalar times w. So if you operate on TV, this vector, it just changes the scaling of w. That's what an eigenvector does. Which means that TWO is lambda, w, lambda and w are an eigenvalue and eigenvector pair for t. But lambda was an eigenvalue for m. Lambda is also an eigenvalue for t. Nothing changes. But the eigenvectors have to change. Because if both are the same, the same matrix obviously be if the eigenvector for m, But u times v is the Eigen corresponding eigenvector for T. Clear. It's an, it's a neat proof because it's just two lines. There's another proof which has five or six lines, but it says the same thing. So what is the message? Well, the characteristic polynomial for the M matrix P m of lambda, which is the determinant of M minus lambda I, right? That's how you find the eigenvalues by solving for the roots of this equation of this set equal to zero is the same as the characteristic polynomial for the T matrix, which is determined by T minus Lambda I, which means that all the eigenvalues of M the same as the eigenvalues of T and vice versa. So that's a fact. So anytime you see a similarity transform or an upper triangular realisation transformation, you are going to have eigenvalues to be the same. Which is a nice feature because obviously we know the importance of eigenvalues for many, many things, including for stability. And I'll analyze the system will be stable. I need to be able to have a handle on the eigenvalues. And if I transform it to another form, Hey, maybe I lost my eigenvalues. No, do not despair eigenvalues are the same. So that's what this result shows. It's okay. Now, not only true that the eigenvalues are the same, but if you have an upper triangular form, where are the eigenvalues? We've been hinting at that. Yeah, all through them, all on the diagonals. That requires a proof. So what we've shown is that the eigenvalues when you do this upper triangular realisation, change of basis remain the same. But it remains to be shown, which is true that not only are they the same, but they're also all along the diagonal. Okay, so let's do that. We'll start with the special property of t. Yeah, that's exactly the statement. So if you have an upper triangular matrix, lambda one through lambda n, exactly that we've seen before. All the entries along the diagonal are the eigenvalues, the eigenvalues. To find the eigenvalues of T, We want to find the lambda such that t minus lambda I has a null space, right? How do we find eigenvalues? We fight, we solve for determinant of lambda I minus B or B minus lambda I, t minus Lambda I. So we need to have what for what values of lambda does T minus Lambda? I have a zero nullspace. It has another space rather. Well, let's try. Let's say T is a three-by-three case. This is an upper triangular form of T. For what? What is T minus Lambda I? Well, that's going to be lambda one minus lambda, lambda along the diagonals I'll add or subtract minus Lambda. Doing business as usual, right, for this, the way we fall for eigenvalues. So for what value of lambda will I have a nullspace? Try Lambda equals Lambda one. What happens at lambda equals lambda one? Well, the first column is all zeros, right? Lambda one minus lambda is not zero. This matrix have a null space. Yes. Because of z all zeros, you're not full rank. So you're going to have enough space for the first row is all zeros. That means you're going to be the nullspace. So lambda one equals lambda, lambda equals lambda one is an eigenvalue. Because when I plug in lambda equals lambda one, in this equation, when I plug in Lambda equals Lambda one, I get a null space which is required for an eigenvalue property. What about lambda two? Lambda equals lambda two. Then the second entry here will be zero. This have a nullspace. How do you check this? It says matrix full rank. First row is fine. Three entries, second row of zeros, zeros something. Third row is 00 something. Can it have full rank? No way. These two have to be. This has to be a scaled version of that, right? So that means this is also has a nullspace. So anytime I plug in any Lambdas that are on the diagonals, you can see that I'm going to have nullspace. That's the, that's the reason that clear. I mean, we haven't proved it, but so t equals Lambda to my lungs. Also, the lambda two is also an eigenvalue and general case. So what happens in the general case? The general case, let's say, let's take a five case, just four, without things blowing up. So you see that you have stuff here, zero here, or lambdas here. If you consider T minus Lambda three looks without loss of generality, Let's take Lambda equals Lambda three, so that will become zero, right? Lambda minus lambda three minus lambda three is zero. When lambda is lambda three and all the others will not be zeros. And then you will be left with this three by four case, which is going to be enough drank deficient. More explicit way of saying it. If harking back to your Gaussian elimination studies in linear algebra, is that no pivot in column three. That means you have a free variable. So the matrix is not invertible, so it must have been hospice. Go back to your basics that you studied. Probably when you came to Berkeley for the first time or maybe in high school. I don't remember where you did this. But clearly you can see the two takeaway facts are the eigenvalues of M and the eigenvalues of T are the same. Not only that, the eigenvalues of T are on the diagonals. So you can just read them off. Very convenient. You don't have to solve any equations. You don't have to solve characteristic equations. You just solve. Read off the diagonal entries and you will be there. Okay? Let's look at, let's kinda do this actually on. So BIBO stability. We know about BIBO stability. If systems with non diagonalizable matrices. We know about BIBO stability for diagonalizable systems. So what happens if x of I plus one is equal to lambda, lambda 10 times x of I plus alpha beta times U of I. So because of this, Has he ever seen this is not diagonalizable. So question, under what conditions is this system? Bibo stable? An important question because you're going to encounter non diagonalizable systems such as the RLC circuit in your homework problem, this week, you will have a non diagonalizable system that you want to solve. So what do you do? Well, we know that x of I plus one is lambda x two of i plus beta u of a. Second equation. So let's write this. We call it equation one, even though we took the second component of the matrix. Then this is scalar. So it says BIBO stable. Who can answer with a equation one corresponds to a baby's BIBO stable system or not. Yeah. Yeah. What's the statement? What's the less than not equal? Less than one? Yes. Exactly. So if Magnitude of lambda is less than one. Then bounded input results in bounded output, right? So pounded you gives you pounded x2. Now, let's use this in equation one. What is equation one? It's gonna be x one of i plus one is equal to lambda X1 plus X2 of high plus alpha u of y. Okay? So what do we know about x2 of I plus alpha u of i? Well, let's say that lambda, we want to check for stability and let's say that lambda was less than one in magnitude. And therefore, from equation one, what do we know that a bounded you will give you a bounded X2. This is now a general input equation one, yeah. There's no lambda. That's point. If it's from this equation, right? It's the sum of the first equation. C X X1, X1 of I plus one is lambda X1 plus X2 plus alpha u over Lambda only shows up in the second equation for x2. Okay? So there's a general input. So this is equation two. So how do we argue? We know that x one of i plus one is lambda of x one of i plus some input. If lambda is less than one, then it's BIBO stable. If Lambda is greater than one is unstable and you're done, right? But if it's less than one, in equation two, what do you know about the underlying part, the general input that's going to be bounded. That for bound, that for stability means. So the input in equation one is also bounded. So how do I know whether X one blows up or remains unbounded? By checking Lambda again, if lambda is less than one, then X1 is also stable. So what is the conclusion? So basically let me quickly, right? So plus some input here, if lambda is less than one, it is BIBO stable. So if I were to do lambda one, lambda two, and so on lambda n. And you have some stuff here, just stuff. What is the condition? And this is all zeros. So this is where I do back substitution. I start from the last equation. Do I check my stable or not? If I am, I bring it to the second-last equation. As a general input, I asked on my stable or not. So what is the general condition for stability for a non diagonalizable system? All the eigenvalues have to be less than one in magnitude. Exactly like in the diagonal case. Wow. So just because you were not diagonalizable, then mean anything, it still means the condition for checking stability is the same. So if all the Lambda i's are less than one, for all i, then BIBO stable. So lambda i magnitude less than one for all i is true for all BIBO stable systems matrices. And we are done. So NEMA, you want to announce anything? Yeah, I wrote it on the front page. It's a hauled away in the beginning there this follow links that no. The password is periodic. Yeah, Thank you. It's really nice to see all of you here. I hope this is a trend. I guess, as long as the extra credit continuous, you will continue, right? Looking good, We are done. So we will meet on Thursday. The link isn't working apparently. Oh, okay. Thank you. Lecture. What do you see? 