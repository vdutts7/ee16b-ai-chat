Okay, Let's start the class. Welcome to Thursday. Close to the weekend, but homework is due tomorrow. So how many people have started the homework? Okay, Good. So today we are going to be discussing the SVD. Of course, we started last time. But by the way, I forgot to mention that we are officially in module three, whatever that means, okay, that means SVD and so on are topics that are at the heart of the machine learning part of the class. So you can see that next week I think you will be doing labs on PCA, which we will also talk about. So we are sort of Module tool was controls and nominally this is more applications of controls or linear algebra tool, machine learning. Okay? So, um, we'll talk about the applications next week. So the usual announcements, I leave it up for the link for the lectures for you get your attendance. Last time we had done minimum energy control. We recapped the spectral theorem for symmetric matrices. Then we introduced the SVD, the Singular Value Decomposition. So today I'm going to try to use a combination of my iPad on the board. Okay, so we'll see how that goes. And to fill in the gaps in your details of the lectures. Definitely look at note 14. Okay, that's a lot of good information. And hopefully the notation is not too inconsistent. But even if it is, I'm sure that we can you can make the adjustment. So what are you going to do today? Today, it's all about the SVD. We'll do it in detail. Ah, we will talk about what's known as the full SVD, which is what we've been discussing, will tell you that there are differences. Will show the construction and justify it. And show you the SVD algorithm. How do you find this decomposition from scratch? And we'll show examples. And then we'll talk about two other forms of SVD that are more practical for implementation sick. One is the compact SVD, which is throwing out all the extraneous stuff that you don't need. And the older product, SVD, which is actually the way you should implement things. Okay. Any questions? So am I going too fast or too slow? Can you give me a general feedback because I want to adjust. I don't want to make sure I take everybody with me. I don't want to lose you behind. So anyway, if I'm going too fast, please raise your hand. Okay. But hopefully I'm erring on the other direction. I'm purposely doing that because SVD is something, it takes a little time to digest it. Okay, recap. And this is all from, this is from two lectures ago, but it's kinda topical for this. So you don't have to write this down. This is all in the notes. We know for symmetric matrices always can be diagonalized. This is just a review. And not only that, the diagonalizing matrix V is the eigenvectors of a symmetric matrix S, and we know them to be orthogonal, right? So you get orthogonality and eigenvectors in one swoop. And not only that, the eigenvalues are all real, right? When we do this two lectures ago. And the meaning of that is that if I decompose, if I decompose, what this does, the whole of the tail. Oh, nice. Okay. I hadn't, I hadn't experiment with this feature. Okay? So we know that this is the decomposition of v transpose S V and lambda is the diagonal matrix. So the decomposition of the symmetric matrix S is given as V Lambda V transpose. And just as a reminder, the lambdas are the eigenvalues of S and all the zeros, eigenvalues. Eigenvectors corresponding to the zero eigenvalues are shown in pink here, and the non-zero ones are shown in black. And we know that as a reminder, S times all the pink vector. So we return the corresponding zero eigenvalues by definition, the satisfy that because the lambdas are zero. Another way of saying that is that Vr plus one through Vn. This guy here in the nullspace of S, right? So just a reminder about the notation. It'd be useful later. Then we discussed motivating the SVD. We said, if I have a square matrix and we're gonna make em to be smaller than n, meaning it's a white fat matrix. What's a good way of decomposing a general matrix like this, which we call a. We know, let's count our blessings. We love orthonormal basis. We love diagonalization. But we don't want to rely on special structures like symmetry or even square, right? So can we get both diagonalization and orthogonal and orthonormality without needing to resort to symmetric square matrices. And we said, basically it's a question of how do you generalize the concept of Eigen decomposition or eigenvalues and eigenvectors to the case where you have rectangular matrices. So there's no such thing as an eigen stuff, right? I didn't only applies to square matrices. And the key insight is that was that we have to give up on the pipe dream of having one orthonormal matrix and instead resorted to the instant you go to, life opens up, okay? And good things start to happen. We need one orthonormal basis that spans the column space of a. And another one, separate one that spans the row space of a. Clearly, these two spaces need not even have the same dimension. Rectangular case the row space is different from the column space. So we want one for each. And for square matrices, of course we had Av equals Lambda v for all the eigenvector pair. And these of course, just to another reminder, the need not be, the eigenvectors need not be orthogonal. But We said, let's relax this condition here. That v1v on both sides of this equation. And instead, how about if we do UI and VI, where u's and v's are different and they need to be because a times v has a different dimension than v, because these multiplying a and a is a rectangular matrix, right? So in our example, V is a long matrix, but use a short metrics. Because using the column space. So that's exactly what we decided. We now have two orthonormal bases, UIs for the column space, the eyes for the row space. And in matrix form. If we assume, let's assume for the moment that your matrix has full rank. So what does that mean? In the case of a rectangular matrix? It means that the rank has to be m. Since m is smaller than n, it could be smaller. We'll discuss the case in a bit, but let's first see the full rank case. If it had been a tall matrix, the rank would have been, would have been n. Yeah, good. So this is the setup. So if you write our wish of having two different bases as E times, Hey, times v1 through vn equals one through m, sorry, eight times v. V is now an orthonormal matrix n by n. You are orthonormal matrix m by m. This is our racialist. This is what we would like. And we want to see if you can do that. And today we will show that this construction is indeed feasible. And so you want u1 times sigma one, u two times sigma two and so on, UM, times sigma n and the V m plus one through n. These matrix matrices, I'm sorry, these vectors will map into the nullspace zero. So then we can have, this is all still wishing, right? This is what we're hoping can happen. So we can have a times V equals U times sigma. The key thing we did was we didn't insist on v here. Rather we said no, let's have another one. You are, you can't, you can't even have v because the size doesn't even match. So that doesn't make sense. So the rank of a, as I said, is m. We are assuming it's M for, not for the moment. And this is exactly known as the SVD. Any questions on the recap? Okay? And this was the nice pretty picture of the SVD, which is further showing what we wanna do. Taking a concrete case of ten by 100, you know, it'll be decomposable. M by n will decompose into m by m, m by n, and n by n. The number of sadly the size matches. So the use of the orthonormal basis for our ten hard tenant, the column space, right? Because you have ten year and u, v rather v transpose, which means the rows of V are the orthonormal basis for R 100, right? Which is for the row space, because you have 100 rows dimension or has, has dimension 100 and Sigma. So if you have a rectangular matrix equals square matrix on the left and square matrix on the right. What has to go in the middle? It has to be the same size as the matrix that you're trying to decompose. Because if this was ten by 100, so this is ten by hundred. And you know that this has ten by ten and this is hundred by 100. This has to be ten by 100. By logic. Otherwise, you have illegal matrix multiplication. So the size of sigma will always be the same as the size of a. So whatever size is, that's what sigma will inherit. Because the u and v transpose have to be squares, because we are insisting on that based on orthonormality constraints. Okay, So these are known as the left Eigen singular vectors. I'm going to sometimes labs and say I again, you've got to correct yourself. These are not Eigen, singular. Singular is the new icon. So singular vector left, singular vector, right? And these are the singular values on the diagonal. And for the tall skinny case, where m is greater than N, then you will have 100 by ten, will be 100 by 100 times 100 by ten times a ten by ten. Okay? And, but everything still applies. So we will focus on the first case. But everything applies whether it's fat and short or tall and thin. So that's the summary of the SVD. And this is just recounting. Recounting that a is U Sigma V transpose. So keep drilling into your head until it gets in. These other 3D compositions. A into three different matrices. And this decomposition is known as the full SVD because of the square times diagonal, rectangular time square, it that's called the full SVD because u and v are expressed fully in terms of their of their basis, the entire basis is present in the decomposition. That's what caused them foot. And we will see that we can shorten that if you want. But conceptually this is the right rate of thing. Okay. I wanted to quickly recap the example we did last time. That's because examples are always useful. To recap, was that a is given by this four by 44433, two-by-two case. So let's, let's apply the SVD for the square setting. Squares are also rectangular. It's a special case. So if you do the decomposition and we will work out later today how exactly we get this particular decomposition. That's going to be, we'll revisit this example after we've studied how to do this construction. But for the moment, let's assume somebody gave you this answer and you can indeed verify, you can multiply the three matrices and you should get 4433. Okay, Let's check it out. I mean, you can easily do it. The key things to point out here that to note are you have to two bases here, the yellow ones and the purple ones. And they're both orthonormal basis for R2. The yellow ones are four fifth and three-fifths and three-fifths and minus four-fifths. That's orthonormal basis for R2 and 11.1 minus one with the normalization of one over root two is another orthonormal basis. So it turns out that the purple one and the violet one and the yellow ones are the right basis to use for this matrix. Okay? We'll see why later. But that's the first thing you know. So, so U1 is four-fifth, three-fifths. U2 is three-fifths, four-fifths. So there are two orthonormal bases. These are the basis for the column space. You can see that the column space is any multiple of the first thing, you notice that the rank of this matrix is deficient. It's rank one because the second column is the same as the first column, which means it will show up in the fact that there's only one singular value. See the singular value being zero is telling you that there's only one rank of one. And yeah, So v is going to be these two guys, use gonna be these two guys. And these span the row space for, for aerospace or any multiple of that. Indeed, you can see that 11.1 minus one as a basis can indeed span the row space. And 4/5 and 3/5 can indeed spend 4/3. Okay. I wanted to point out something. I don't know if I misspoke The last time. I said that singular val I may have said, I don't, I don't remember that singular values become eigenvalues in the square setting. That's wrong, okay? If I said that I was a lapse, should I take it back? What is the difference between singular value and eigenvalue? So what is this? What is the Eigen decomposition is a square matrix, right? I can also do an Eigen decomposition on this. How different would that be from what we have shown here? What would be the Eigen decomposition? Yeah. Yeah, you, you will not have any, you don't have u's and v's, right? There's only one in the interests of not being cluttered, let's call it Q. That's what I'm calling here. Q, right? Yeah, Only one. Q. The same queue for the row space and column space. And in the middle you have lambda, but lambda is diagonal, but it's not this diagonal. It's different. If you check the eigenvalue of this, the eigenvalues of this matrix are not the same as the singular values. They cannot be because the eigenvalues correspond to the basis which is very different from the two basis we are using here. That's a very important point, okay? You always have to just remember to SVD means to u and v. And u and v together combine to give you the sigma. So in the first eigenvalue case, you have Q Lambda Q inverse is our eigenvalue decomposition. You can see that why they are different as you have a single queue as opposed to two, u and v. And two is that the Eigen decomposition will not give you necessarily orthogonal stuff. Only in the symmetric case. Will the Eigen decomposition also be orthonormal? Whereas the SVD insists on orthonormality as the first principles. You must start with, we cannot compromise an orthonormality matters. Eigen decomposition doesn't start the registers, just give me something where Av equals Lambda v, right? Any questions on that? It's an important point to conceptually, once you understand this, you see that everything will fall in place. The normal questions. Yeah, let let let me also recount how we're going to approach the SVD construction for today. We started last time. I asked about how do you turn a rectangular matrix into a square matrix? And one of you correctly answered, you should take a transpose a. You can also do AA transpose. That also works, but let's focus on a transpose a. So what happens when you have a transpose a? In our example here, we know that this is a, this is a transpose. Let's exaggerate the dimensionality. This is a, this is a transpose and the product is of course going to be big one, right? So this is just to visually verify that a transpose a is square. But let's say that we use the SVD, that a is U Sigma V transpose. Then a transpose a becomes U Sigma V transpose transpose times U Sigma V transpose. Just let me, so this is a, this is a, a transpose a becomes transpose of U Sigma V transpose go in reverse order. You have V sigma transpose U transpose. And then again you have your U Sigma V transpose corresponding to a. You recognize that U transpose U if I, since you as orthonormal. And so this simplifies to a sigma t times sigma on the inside and V and V transpose on the outside. I. I also made the second mistake last time I call that sigma squared. That's nonsense because sigma is not a square matrix. Sigma has the same shape as a. So sigma times sigma transpose a is square, but sigma squared doesn't exist. You can multiply sigma by itself because things don't match. So I meant in spirit, you get sigma squared. In fact, you can see here that sigma transpose times Sigma is gonna be what? It's gonna be this times this, right? And it's gonna be squared. So the eigenvalues are, now, these are eigenvalues because we are doing B and V transpose are the same. So these are the eigenvalues of a transpose a. Likewise, you can show that a transpose, everything gets flipped and not something nice happens when I do a transpose, a kind of sifts out the v and the EU is gone. There's no You, even though the SVD had a u, a transpose a has no trace of view. Likewise, aa transpose has no trace of V. Filters out the V when you do a transpose. And, but the eigenvalues that go into sigma transpose sigma and sigma sigma transpose. There are different dimensions, right? Sigma, sigma transpose is, in our example, it's gonna be a short one. It's m by m. M is the smaller dimension, sigma transpose sigma is the tall one, n by n. Well, you will have lots of zeros on the diagonal of sigma transpose sigma. This will be in the case where it's full rank. This will be m by m with diagonal values lambda one through lambda m. What are the lambdas? They're going to be Sigma squared. Sigma x times sigma transpose is going to be Sigma squared on the diagonals, lambda squared. We'll, we'll get to them. But just to note that sigma transpose sigma and sigma sigma transpose, although they are of different dimensions, the non-zero entries of those guys are identical. They're the same. Okay? This suggests that since there seems to be an intimate connection between the SVD and looking at a transpose a or AA transpose, it sounds, it seems that the key to understanding how to do this SVD decomposition is to study the properties of this matrix a transpose a. So let's kinda delve into that and that's what we did last time. And I'm going to interchangeably, you'll see an a in part because C is sort of object of interests coming from the controllability matrix application. So the first is we're going to consider C transpose C. And the fact is that C transpose C is symmetric. And we said last time the proof has one line. Literally, just write C transpose a transpose a equals C transpose. Because if you transpose both sides. This comes to us, that comes second. So first we know it's symmetric. So we have a symmetric and square matrix. Good things happen. We know that has beautiful properties. So we can use our spectral theorem. So we know from the spectral theorem that the eigenvalues of C transpose C are going to be real. We showed that they have to be real valued. We proved it by saying that if lambda is lambda R plus j lambda or lambda star is equal to Lambda, the Lambda conjugate is the same as lambda, which means lambda has to be real, which means the eigenvalues of S are always real valued. But in this case, when S is C transpose C, not only are they real, but they're also positive or non-negative. They can, they can not be negative. They'll always be either zero or positive. And this will translate into positive semi singular values. We'll see that. Okay, so let's do some work. Okay, I'm going to write on the on the iPad for now. Okay, Let's, let's use the iPad for now. Let's prove this. In fact, you should have known that people have seen this from 16:00 A.M. I, right? Give me give me out a yellow yay or nay. Have you seen this before? I guess some of you haven't forgotten right now, you should have seen this from 16. So let's, let's do the proof. Let's, okay. Let's prove it. We know that S times v is Lambda times v. This is assuming Lambda and v are an eigenvalue eigenvector pair for S. So we start from that. But what are the S has the C transpose C. So let's write C transpose C times V is equal to lambda times v. Now, we know that all the eigenvalues of S equals c transpose c are real by the state that, because we already proved it. So we will say that all the eigenvalues of S equals C transpose C real. By what property or theorem? Spectral theorem. Okay? So half-mile thing is already been done before. So what needs to be done to show that they're all non-negative? That's the only thing that needs to be done. So how would you go about doing this? Any suggestions? How do I show that lambdas are going to be non-negative? Yeah, let's multiply it with yeah, whenever you see something like a bunch of matrix times v and you want to try to see what's in it. Try and make it into a scalar. How do you make it into a scalar multiplied by v transpose from the left, right, v transpose times C transpose C times V will become a scalar then, right? So that's just a hint whenever you see things. So as was suggested, Let's left multiply by v transpose. Okay? What do you get? You will get that v transpose times C transpose times C times V equals Lambda. V. Lambda. Lambda between yeah. Yeah. Oh yeah, yeah, sorry. You're right. I've been sleeping. Didn't have enough coffee today. You have to multiply both sides by Lambda v. So that's what we did here, right? Multiply both sides by v transpose on the left. And what is the left-hand side? What does v transpose? C transpose. What's another way of saying that? C times V, the whole thing transpose. So let's try that on C times V, the whole thing transpose is the same as v transpose C transpose. And then you have another CV. This is equal to v transpose Lambda v. Can. Let's pull the Lambda to the left. Let's write this as. Lambda is just a scalar. So what is cv transpose times CV? That's x transpose x for any x. What's another way of writing that? Yeah, norm squared. So this is just C times V norm squared, right? That's exactly x norm squared is x transpose x. What about this? Well, v transpose v, we just said was norm squared. So we norm squared. So CV norm squared is equal to Lambda v norm squared. So what does that mean? Lambda is what? It's Cv norm squared divided by v squared. So you have a positive over positive can be zero. Now, there is an eigenvector. Eigenvectors can never be zero. So this is always greater than or equal to zero. The eigenvalues of a symmetric matrix are always real. Now, this doesn't prove that. We have proved that earlier. When he proved the spectral theorem, two lectures ago, we did it. We inherit the real part from there. This doesn't prove that it is real. There's only proves that it's positive. It's greater than zero. Okay? So there's another name at the end of the proof. There's a name for this for those who are interested, but if it's only for your informational purposes, you don't have to know if any metrics which has eigenvalues that are greater than or equal to zero is called what? Do you know? What you call a matrix whose eigenvalues are greater than or equal to zero? Yeah. Yeah, positive semidefinite. Yeah, there is just for your knowledge, I don't worry about it, but just in case. Later on in your classes, when you take, you'll see that term and say, I saw that in 16 be, I remember that day when I was sleeping. And so say that C transpose C is called a positive semidefinite or PSD matrix, okay? Just for your information. Okay? So C transpose C, There's a symmetric matrix having non-negative eigenvalues. So when, since it is customary, and you'll see that when we study the SVD in a minute, to order, the eigenvalues of C transpose C has lambda1 is greater than equal to lambda two. As you know, when you do Eigen decomposition, these Lambdas can come in any order you like. But we're going to form we are going to be picky and say, it's not any old order, we should do it in the following way. The largest eigenvalue of put it first. The second eigenvalue, you put a second and order the eigenvectors in that order, okay? So you do lambda one, lambda one greater than equal, lambda two, lambda three greater than, delta dot, greater than or equal to. Let's say that there are our eigenvalues that are non-zero. Then the rest, Let's use another pen. So these are the zero eigenvalues, and these are the positive eigenvalues. So we can write v as V1, V2 dot dot dot Vr, and followed by Vr plus one dot, dot, dot VN. So I didn't. I know it doesn't look square, but this is supposed to be a square matrix. It's this, it's an n by n. But the key point here is that these are the eigenvalues. These are the vectors of C transpose C corresponding to lambda I is greater than zero. And the others, the E vectors. C transpose C corresponding to lambda I equals zero. I think we saw that earlier too. Any questions on this? This is deaf, good old Eigen decomposition. Nothing new, right? Because S transpose S, sorry, C transpose C is a square and symmetric matrix. The key identification is that the eigenvalues are all positive or zero. There's no negative guys. Now that's gonna be important. Okay, So let's come to another fact that is also important. Let's start a new page. Okay, here's another fact. C and C transpose C have the same nullspace. Have you seen this and 16? You should have. I see, Yes and no. One of you is right? Both cannot be right. But I know that you don't remember it, so let's do it. Okay. Let's, let's, let's see why. But in general, always, when you have a and a transpose a or C and C transpose C, the null spaces will be the same. And again, see 60 day. Well that's kind of not nice. I don't want to be that I don't expect you to go and review it, although you should. But let's kind of do the proof. So the nullspace of a is what? Is all the vectors x such that C x equals zero. That's the definition of not anything that takes a vector to zero is in the null space of C. We know that we'll have two bullets, if you will. There are two sides to this, is to show that if Cx equals zero, then C transpose cx equals zero. That's a dark statement, right? All I did was multiply on the left by c transpose. C transpose times zero is going to be zero on the right and the left hand side your future MOOC. So it's, it's, it's kind of trivial to say that if x is in the nullspace of C, it has to be in the null space of C transpose C intact. You can multiply anything you want. On the left. It will be in the null space of that as long as it's attached to see, right? X and C, C is the first thing that exits and that's already zeroing it out zero times anything is gonna be zero. So this proof is trivial, okay? I'm not even going to qualify that. Prove obvious. One of the few times you can say obvious and actually mean it. Usually you have some, I'm always annoyed when I read some complicated expression, the author says, it is obvious that what do you mean it's obvious? I don't see it at all. But in this case, it is obvious. So this is one of those times and the author is allowed to say this is obvious. You can write that in your homework. To write the TAs can not give you any grief. This is obvious. Okay? If C transpose cx equals zero, then x equals zero. This is less obvious. But how do you, how do you show that? What did I say? Anytime you see c transpose c x, what is the first thing you should do? What did we do in the last proof? Yeah. Yeah, multiply on the left by c transpose. I told you you can't go wrong. So proof of, so let's do the proof. So left multiply as we did earlier. So you'd have x transpose times. So we want to prove this. So let's start with C. C transpose cx equals zero. So x transpose C transpose C x equals zero. And like before, we'll skip a step, since we did this last time. So this means that the norm squared of dx is zero. This means that x has to be zero only the zero vector has zero norm. So this is the statement that if c transpose c x is zero, then C x is also zero. And we're done. So we have revisited the 16 a proof that C and C transpose C have the same nullspace. Everybody, good. Okay. So now let's actually, let's, let me skip ahead because I want to, there's no need to delete this guy. Keep going. Okay, so let's look at what we have done. So we know that c times. So remember our goal. So let me write it down here. Our goal is to show these are the same things, right? So, nice job. So you see that the board is there, this is still here, so you can see both. Okay, So let's look at CV. We want to show CV is U times sigma, right? Now. What is cv? Now we have accumulated a bunch of knowledge. We have shown that the path to showing this is true C transpose C, right? We said that we know the eigenvalues are real and non-negative. And we also know that the nullspace of v and v transpose v are the same facts that we have in our hands. So again, you should imagine C to B m by n. If it's concrete, think of it as ten by 100, and assume it's full row rank M for them for the moment. So c times v is going to be c times v1, v2 all the way up to c times m, followed by C times V m plus one through n because the rank is m, we know that all of these are going to be zero. So that means v m plus one to v n are in the null space of C transpose C, which is also the nullspace of a. Right? So we're gonna take the vectors that we got, the eigenvectors we got from the previous Eigen decomposition we did of what? Of c transpose c. So we take C transpose C, which is a square matrix, take its Eigen decomposition and the eigenvectors of v1 to vn, We know that C transpose C has to be zero eigenvalues. So we know that and they ordered them. Remember we ordered them. So lambda one corresponds to the largest one, lambda m to the next, and so on. And these are all zeros. So we know that c times v m plus one, C transpose C times V m plus one to v n are going to be zero because we know the eigenvalues are zero. But by the theorem we just proved, we reminded you from 16. These are also the ligand, the nullspace of c, which means that c times all these guys have to be zero. Is that clear? Okay? So now we have decomposed C into V rather into V1 to VM and VM plus v1 to vn. Now here's a fact that would be nice if it were true because then we'd be on our way to showing the eigentlich, the SVD, C1V1, C2V2 after CV m for m form an orthogonal but not necessarily orthonormal, meaning the scalings have to be taken care of basis set for RM. So obviously, these are going to span RM because the rank is m. So c1 is of length, is of dimension m. Now, why is this good? So if you go back to the board or we look here, this is the reason. This is reassuring because if you have c times v column with, let's call the non-zero eigenvalues to be the stack of V1 to VM. Let's just call them v column. And we'll call the others v null, right? The ones that are zeroing out. If c times v, We know that C times V null is zero. We know that from derived based on the nullspace property. If we can show that c times v column is u, where u is a orthonormal matrix. But possibly with some scaling here to make, to take care of the fact that it's not really orthonormal is actually orthogonal. How do you, how do you turn orthogonal and orthonormal you to take the vectors and make them unit length scale by the norm. And you put the scaling factors on, on the diagonal here. And then you will have an orthonormal basis here. So we will be done. It's like saying that you can be derived from V C. We start with c transpose c, which gives you V B. If the Eigen, Eigen decomposition for C transpose C from V, we have to derive u. We want to show that the U that is induced by the V is that we just derived are also orthonormal. Then we'll be done because I thought it is because Eigen decomposition SVD is nothing more than c times an orthonormal equals some orthonormal time some diagonal, right? So that's what would be nice. So let's, yeah, let's do this. Let's do it on the Okay. Let's do it on the iPad for now. So we said that fact ups on pen. So we know that c times v equals c times v1. This is capital V. And c is of course, m by n. We'll assume full rank. So here the fact C1V1, C2V2, see VM, which means these guys form an orthogonal. And I'm using orthogonal precisely in the sense that it is only orthogonal. It is not orthonormal orthogonal basis for R m. Okay? Why? This is the key? If you can, if you can show this, we are actually done. Well, let's call C1V1, C2V2, dot-dot-dot, see VM. Let's rename them as U1, U2 dot-dot-dot. Um, it's not by accident. I picked the notation. Here is exactly the value that we're going to end up with. For c times v is going to be u. Okay? Let's, why is this? So let's take the inner product between CVI and C v j for I not equal to j. So I is, concretely, you might have that m is ten. So doing a ten by 100 matrix decomposition and you're taking say, a beat, Let's take v1 and v2. If it's easier for you to comprehend, what will this be? This is going to be c v transpose times C. Vijay. I don't need this brackets here. I don't know why I put them. See CVI transpose. Transpose there. I'm just taking the inner product, which is the same as taking X transpose X. So this, There's vi, vi transpose C transpose C times V j transpose rules. Now, we have to use what we just derived. What is C transpose C? That's s. We just, we just did Eigen decomposition for C transpose CRS. So what is S times v j? What are the bees? The bees are the eigenvectors of S. What is S times v j lambda j times v j, right? It's the lambda is, the Bs are the eigenvectors for S. So this is just Lambda j times v j. Okay? Almost done. Because I can take the Lambda j, which is a scalar out. And then I have VI transpose v j, the inner product between VI and VJ. What is that? What are the VI is what are the PIs and VJs? Yeah, there's an orthonormal eigenvectors of S, right? Remember that's why the symmetry of S plays a key role. These are not any all eigenvectors. These are orthonormal eigenvectors. So what is v inner product v j zero? So this is just zero. So if I is not equal to j, it, so we have just established not only other V's, Right? The bees as in these V's, not only are these orthonormal, but I multiply C by V, I'm going to induce the structure. So we are a dot the I's now and we're done. The reason for that is, okay, let's now consider why things are orthonormal versus orthogonal. So to do that, we have to find the norm of CVI squared, right? That's how you find where the CVI is. We know that CVI, this set, this set we know is orthogonal. We just proved it. Ah, we also know that it's a basis because I and j span one through m, m vectors and it says full rank. So the only thing left to discounters, if we want ortho normality, we need to normalize C times V two unit length. So which means C V dot product with CVI. What is CVI dot-product with CVI? Well, we did the calculation right here. Just replace j by k. So this will just be lambda I times V i inner product VI, right? You're not redo the calculation. We just did it. We did it for J. And arbitrary J. J could be when j is not I at zero, when j is I will have lambda I times the inner product of v with itself. What's the inner product of v with itself? What are the eyes? The eyes are orthonormal, so it's one for this. So what is this? So what is the CVI inner product with CVI? That's gonna be lambda I times the inner product of v with v, which we know to be one. Therefore, this is just Lambda I. So, so C v squared norm, which is the left-hand side, is equal to lambda i. I want to highlight that ups. And this is called a singular value. This is the singular value. Does not, I shouldn't call it the singular value yet. Take that back. Therefore, what has the norm of CVI? Without the square, square root of lambda I. Lambda I is not equal to zero. If lambda is zero, then you're in the nullspace. So that doesn't matter. So this is the only considering the basis of all the v is not equal to zero. And we know from the real and non-negative property of the eigenvalues of S, that they have to be positive. These all have to be positive for. They have a square root, so you don't have any imaginary stuff happening. Okay? So it's all reals and all positive. Square root of positive is positive, all good. And we are done because you just piece everything together. Right? So what have we shown here? Let's look at this. So what have we shown? We have shown that UI. We'll just call them UI. So that's UI is the new name for CVI over root lambda I. So CBI over root lambda I is an orthonormal basis. That's the new basis in the SVD. Why? Well, that's what we derived. That UI forms an orthonormal basis for RM, right? We just, we just derived it. And these correspond to all the eigenvectors for which lambda is strictly positive. And for the other guys from v plus v1 to vn, they correspond to lambda equal to zero. So here's a decomposition, C times V. I'm writing all the dimensions out so we understand. So C is an m by n. You're going to hit it by v, which is N by N, what is V? V is the eigenbasis for C transpose C, Right? That's exactly how we derived it. That's going to be all the non-zero guys upfront placed an order followed by the zero guys here. So now CVI, we just sent this root lambda times ui, right? Because we just showed that C times V i, c times VI is root lambda I times VI. So c times v is root of lambda one times U1, C times U2 root lambda two times U2. So we just derived that these two are the same where the youth are now orthonormal, right? And these are zeros on the right. I can write this. Let's split root lambda I times UIS. Let's just write the UI is first you want through, UM, and use our matrix multiplication knowledge to put the Lambda i's on the diagonals. What's on the diagonal root of lambda i's root lambda one is the first entry, root lambda two is the second entry, and so on. So clearly when I do this multiplication, you get root lambda one here, root lambda two here, multiplying on the row, on the columns. And you will get the expression that C times V is equal to root lambda I times square root of lambda items UI. That clear is just matrix manipulation. And basically this is what we call sigma. The sigma on the board. Same sigma. And this guy are called the singular values, which we have already seen. So we saw that c times v is u times sigma. The second equation on the board. And multiplying both sides by v transpose on the right, D times V transpose is I. So C becomes U Sigma V transpose. Then end of story, You have just finished completely understanding hopefully the SVD construction. Again, the key is that you start with two basis, u and v. Once you start with that, and then you start with, try to look at C transpose C as the basis a starting point. Everything follows. Okay, So what is the algorithm? We have looked at it? So this is for summary. So think of the case where c is ten by 100. So what are we saying? You're saying that C is ten by 100. You're going to first multiply it. This is the CV, the second formula on the board. So v is gonna be an orthonormal basis for C transpose C. The purple guys are the non-zero ones ordered, ordered according to lambda1. The top one is a maximum, the last one is the smallest. And these are all the nullspace vectors, v, v 113100. So C and V go to zero here. So if you do multiplication, you see that Cv equals nu sigma. Okay? So here's the procedure. For as c equals SVD, SVD procedure. Let's recount the steps, the recipe. Now you can be completely in auto mode. You have understood the concepts. So now we can go Duck, Duck, Duck, Duck, Duck, step-by-step. Two teaspoons of this and stir and blah, blah, blah, right? Just a recipe. It should be mechanical. But this is only for CMS concept. Okay? So first thing you do is you compute S equals C transpose C. That's how we already start of everything, the symmetric matrix. Then you compute the eigenvectors of Hesse as V. V is the orthonormal basis, which is the Eigen basis for S. And it's orthonormal because it's symmetric. If it's symmetric, it's orthonormal to use that property everything is building up. Your knowledge has been building up lecture by lecture. Now, you use this V to populate the V matrix for the SVD. That's the V that we want. Okay? V1 to VM correspond to the positive eigenvalues which correspond to lambda one. V1 corresponds to the maximum singular value. V2 corresponds to the eigenvalue. Lambda is an eigenvalue. Sigmas are our singular values. So these are the Lambdas corresponding to the Eigen values for the symmetric matrix S, which is c transpose c. So lambda one is greater than lambda two is get an emblem order them. And the F1's are the corresponding eigenvectors that correspond to these. And of course, lambda m plus one to lambda n are all zeros. Okay? That's the first step. Second step. Exactly as per our, what we wanted to do. Say C times V over root lambda i or the UIs, right? That's what we derived. So make sure that we know that the C times V IL orthogonal to scale them by root lambda i. They are also orthonormal. So that orthonormal set they use. So you start with the B's and you derive the US from the bees using exactly this expression. So that's the use. And then the sigmas are just diagonals of square root of lambda i's on the diagonal is ordered from largest to smallest. And zeros because of the size incompatibility. It's diagonals wherever you can be diagonal. End of story. Is that clear? Okay, good. So this is the singular value decomposition. Then you will ask what happens as C is not full rank. I assume that c was full rank. Which means that in the rectangular case where m is less than n, I assumed that the rank was the smaller dimension, which is m. Whatever it is, not that whatever it is smaller than the smallest, smaller dimension. Or do you do? Any ideas? We've shown you the full construction when the rank is m, right? In our example. What if Frank is not M? Suppose you are, it's m minus five or n minus two. What would you do? Here's the, here's the entire SVD construction. What would have to give where's the rank coming in in the construction? This is assuming that was full rank, right? Yeah. Louder. The rake is V1. V1 to VM, or capturing the fact that the rank was m. So if the rank is less than n, you would only have V1 through v r, where r is some number less than m, right? What do you, what will you do with r plus one or plus two and so on, up to Orem, up to m. What am I supposed to do? Yeah. Gram-schmidt, grower, Gram-Schmidt. So I only need exactly are vectors, eigenvectors to represent the Bs, the left Eigen singular vectors. To capture my, my, my matrix, all the rest of fluff. I don't even need them. But since I cannot constrained to using an orthonormal basis because it's the full SVD, It's a very redundant expression. I will have to populate, have to fill out the eigenbasis. How do you do? Well, Gram-Schmidt use r plus one, r plus two all the way up to RM BM will come by doing Gram-Schmidt, which you should be an expert by now, you know how to fill out an incomplete set into a full set. That's exactly right. So let's kind of maybe see what happens if the rank is r, is that you will have lambda one greater than equal to, greater than equal to lambda r subs. Which is greater than lambda or lambda r plus one equals lambda, r plus two equals lambda, n equals zero. So you just start only r of them are non-zero. That'll be reflected in the fact that square root of lambda i's are also going to be only r of them are non-zero. And so here you will have up to square root of lambda r. And all these will be zero on the diagonal. That's it. It's as simple as that. Okay, let's do an example. And in fact, let's revisit the example we did earlier. Remember, I plunked we did the SVD for that. I didn't tell you how we got there. Now I'm going to tell you how you get the effect. You should be able to do it yourself. Because we have recipe to follow the recipe. What are the recipe say? Well, firstly assess, okay, sorry, again, I'm going to interchange a and C. So the example had a, so let's use air, doesn't matter what, What's in a name. So a transpose a, let's formulate that foot 4343 times a transpose. This is a, this is a transpose, this is a. And if you multiply it out, you get a square matrix, 25, 25, 20, all the same entries. So first thing you notice is as clearly that jumped. Sorry. First thing you notice is that this has rank one, right? Because this is rank deficient. We discussed. So the eigenvalues of a transpose a are the roots of find the eigenvalues. By now we should know how to do that. And so that's exactly what I do. I set the determinant of Lambda I minus a minus a transpose a minus Lambda I is set to zero. Solve for the roots, you get 0.50. So these are the eigenvalues of a transpose a. We know that we have to put the non-zero first zeros at the end, right? That was our tradition. And the non-zeros have to be ordered, but there's only one guy, there's nothing to order, only one. And that's the, that's the 50 guy. So lambda one is 50. And V1, if you work it out, because it's gonna be one over root 211, that's the eigenvalue of the first eigenvalue corresponding to Lambda equals 50 of a transpose a, that's v1. Lambda two is zero and v2. It's of course, since it's an Eigen decomposition, these have to be orthogonal. So this is gonna be the orthogonal complement of 11, which is one minus one. So this is the second vector. So what do you know? What do we do now? So a transpose a is going to be 111 Eigen decomposition V Sigma V transpose. So this is not the singular value decomposition. This is the Eigen decomposition of a symmetric matrix. So you plunk V1 here, V2 here, and you plunk lambda1 and lambda2 here. And this is V transpose. Now second step from the VCE come the use, what do we use user AVI over root lambda i. That's exactly how we derive it. For lambda I is not zero. So what does that mean? So u is going to be a times V1. So a times V1 divided by root lambda one. And if you work it out, it comes to four-fifths, three-fifths. That's the other basis. What do you do for U2? Exactly what was suggested. I don't have any U2 in mind, but I know I need to be orthonormal basis. So I use Gram-Schmidt. So I use Gram-Schmidt to complete the orthonormal basis. And if you do that, if you use Gram-Schmidt here for three, you will have three minus four. Okay? So you see the U1 and U2 form a complete orthonormal basis set for R2. And that's exactly what you do. You plunk, you derive the VCE. Now you derive the US. And you already know the sigmas because they come out as in the process of doing the Eigen decomposition for a transpose K. Sigma was already root of lambda i's are brutal, 50 times zero. So SVD is exactly this. This is exactly what I claimed without proof. Now I'm showing you how to construct it. Can sit for a moment and pause. Are there any questions or things clear? Yeah. Yes. Yes. Yes. V2 was just found by the Eigen decomposition. I mean. Okay. Now you have two orthonormal basis. Again, they're separate. The yellow guys are the use for the column space. The purple guys are the bees for. Now. Let's move quickly to the compact form of the SVD. Oh, this was another matrix example that we did last time. And I will leave to you convinced, to convince yourself that you can find the SVD for this rectangular matrix which has all these crazy entries. Remember we did this last lecture. I also plunked this example on you. And in this case, u is two-by-two, sigma is two-by-four, v is four-by-two. The transpose is for Biden. And you go exactly the same way. You start with a transpose a, find its Eigen decomposition, plunk the values of v that way. And then you derive you from the V's and then the sigmas are from the Eigen decomposition to. Now, what is the compact form? In fact, you could also see in the compact form in the other case. But if you were to, if you look at this matrix, there's a lot of wasted stuff going on here, right? The fact that I had to use Gram-Schmidt to complete a basis is totally wasted. You work. I'm going to complete it and then not use it. What the hell are we doing? Why are we even doing that? Well, they're doing that because I wanted to conceptualize first that you have a basis here and a basis there. But now that I understand this, let's cut out all this crap of doing redundant stuff. We don't want any any extraneous work. So what are the extraneous work here? Well, you notice here first is that there's only, this has rank 12. And that is evident from the fact that you have only a single singular value. This other singular value is zero. Which means in order to do this, a equals U Sigma V transpose. The only thing that count, or the first column, the singular value and the first row. All the rest don't play any role in the computation of a. I don't care about all this. I don't care about this. It serves no purpose. So I might as well reduce my decomposition instead of having square times rectangular Times Square Tool, rectangular times squared times rectangular. So it's a reduced form where you are. It's now two by one. I don t need to buy to, I don't need the second because we derived the fact that the second was derived using Gram-Schmidt. But then I never used it because it was corresponding to a zero eigenvalue, right? So I don't need it. So I just use the first guy and then I take the singular value. It's a square matrix because it's one-by-one, which is square scalar. And the first row. That's it. You notice this times, this times this. If you care to multiply it out, will indeed give you this. And that is the most compact form of the SVD. It has its advantages and disadvantages. Disadvantage is that I don't see any orthonormality anyway, I don't see a full matrix. That's why we went through completing the matrix using the Gram-Schmidt. But you could also have done the same thing here. So what would be the what would be the compact form for this? Anyone? In our two-by-two example, we also had only rank one. What would be the reduced form? Anyone? Yeah. Louder, louder. Right. Exactly. So basically, all you need is exactly what we said earlier, which is, I need this guy, this guy, and that guy. Indeed, you can work it out. So you can write the Reduced, not a reduced. Here's a is equal to for over 53/5 times root 50 times one over root two times one over root two. Work it out. What do you get? Root two goes into root 50, root 25 times which is five. So this is just 554 fifth column vector times 55 row vector gives you 4433. That's exactly 4433 on the same matrix. So it's a much more compact form of doing that. The compact form will have, will only capture all the non-zero singular values. I'm not sure. I let let's talk offline. I think I know what you're saying. Yeah. So if NEMA here I don't know if NEMA came to oh, there he is. Okay. So I think yeah, I think we need to we'll continue next week with the SVD, but for now, the important business of your attendance. So what shall we make the password today? We did singular last time, right? Compact. Okay, let's do it. Compact. The password is compact. 