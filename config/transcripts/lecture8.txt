Alright, let's get started. So any questions before we start today's lecture? Okay, let me remind you of where we were last lecture. So last lecture we talked about change of basis. And the idea is that we can represent things in the standard basis, which is E1, E2, E3, and so on. But that's just one particular choice, right? We could also represent, we can pick any n linearly independent vectors in an n-dimensional space and use them as basis vectors. They don't need to be orthogonal, they don't need to be normalized. They can just be any old vectors, right? And what we found was we could always transform from the standard basis to any other basis by forming a matrix where the columns of that matrix are the basis vectors. And if we multiply by the inverse of that matrix, we actually go from standard basis to this new basis. So let's write that down as a reminder. So let's say we have a basis B. And let me actually, before doing this, just remind you. We have some basis vectors. We'll call them B1, B2, Bn. We have another, Let's just say standard basis vector x, right? In other words, just to be very explicit here, this is X1, Y1, X2, E2. And then we'll hat means that these are normalized vectors. Doesn't matter though, right? So what we wanna do is we wanna go from the standard basis to this other basis. And what we found lots lecture was if we form the matrix B, which has these basis vectors as columns, then we could. Basically go to basis B by multiplying by the inverse of this matrix. Okay? Any questions about this? Yes. Exactly. So yeah, very explosive, very good. X is in the standard basis. I want to represent it in this other basis B. All I need to do is form the matrix capital B, which has the basis vectors as columns. I invert that matrix and that takes me to basis B. Alright, That's important. Actually make sure you have that clear in your mind because we're going to use this a lot. Now the other thing is, how does it, how do we represent a transformation from one basis versus another? And this is where I got messed up last lecture, sorry to waste your time. So the idea here is, again, we have this basis B and we have this transformation B. But it should be also related to a transformation. Let's say we're going to stay in standard basis. I have a transformation, a is in the standard basis. Okay? Remember the way we find a is we apply the transformation to the columns of the basis vectors, in this case E1, E2, E3. So this is where the matrix a comes from. I'd like to find a matrix B. I'll call it a sub b, which means the trends. Let me call this t, Just so the a's and b's are not confusing. Notation is very important as I got messed up last time because a bad notation. So I'd like to represent this in transformation TB. I'd like to find the matrix TB. That's the same transformation, but in the basis B. Okay? So vectors XB, YB are now written in terms of the b basis vectors. And clearly t and t, I can maybe even say t and TB should be related. Right? There should be an equation that connects these two. Just intuitively, if you think about it, it's the same transformation. All we're doing is changing basis. So the matrix, there should be a relationship between the matrices. So again, let me write this equation down. Now, let me move everything from the standard basis. I did my notes a little bit differently. So I'll call this basis I. I is the matrix for the standard basis. So let me move everything to this other basis. So how do I do that? Well, I just take my basis vectors form a matrix and multiply. So I have B inverse y. B inverse a is equal to x. So what did I just do? I multiply both sides of the equation by the transformation matrix that takes me from standard basis I to the basis B. Therefore, now I have y b is equal to B inverse a X, X is still in the standard basis, right? But I know that and of course these matrices are invertible. Transformation matrices are invertible because we have the basis set. So that's how we did this. We know that I can write x. Or let's just say I'm calling an X here. I can say that I know that x be simply B inverse times x. Alright? I know I can transform any vector including x to basis B by just multiplying by B inverse. Well, if I multiply this equation by b on both sides, I get B x B. The matrix B is the opposite direction, right? It transforms you from basis, standard basis. Excuse me, it transforms you from the basis B into the standard basis. And so I am going to now substitute why B B inverse a. And I'm going to use this relationship here. And look what I have. I have a transformation that takes me from the basis vectors and B x B, two other vectors, y be also using the same basis. That's the same transformation now represented a new basis. So this oh, I call it T on the previous page. So let me again, I want to be consistent with my notation here so you guys don't get too confused. Right, I called it T over here. So basically what I've done is this is I'm going to call this t underscore B. This is the transformation in the B basis. It's the same transformation just with different basis vectors. So what I can say is that indeed the transformation t, This is the standard basis is related to so he and standard basis and this is t and basis B. Again, we see that these matrices are indeed related. This is what we were after. And there's a simple relationship that takes you from one transformation to the other. This is sometimes called a similarity transform. We say that matrices T and TB are similar. So in linear algebra, T and TB are similar matrices. Okay? And you can see that we have TB is B inverse t b. Alright? I can multiply both sides by b. And then I can write, of course the inverse is defined so I can multiply it left and right inverse are the same for an invertible matrix. I multiply it by a right inverse. And I can see I have a similar relationship going back-and-forth. So t and TB are similar matrices. These, this is called the similarity transform. And this is very important, extremely important. As we'll see in today's lecture. We're going to use this a lot. Questions. Pretty clear. Yeah. So that again, why should they cancel? Here? I'm right, multiplying by B inverse, right? No, they don't write because matrix multiplication you can't change the order of things, right? Yeah, good, good question. Yeah. In general, matrix multiply, multiplication does not commute, right? So you can't change the order of this. It's very important. Okay. Other questions. I noticed in the comments, somebody said that we go through the math a little bit quickly, so I will try to slow down and you guys can ask questions if, if it's not clear, some of the steps aren't clear. Alright? Now that there's another question which is, if you have a transformation, what's the best basis to use, right? In other words, is there some kind of natural basis that makes our transformation and very simple to understand, right? In general, our transformation is an n by n matrix. And an n by n matrix is as simple as possible when it takes what form? All right, What's the, what's your favorite matrix as you're solving a problem and you see that you're like a diagonal matrix. Yeah, if you've, if you're doing a complicated calculation and you could reduce your answer to a diagonal matrix are gonna be very happy because you can invert that very easily. You don't need to use a computer to invert it. You don't need to use fancy theory. It's simple. It's just a diagonal matrix. So what we're after is some way to represent a transformation T as a diagonal matrix, right? In other words, is it possible? So the question that we would like to answer is, can we take B inverse TB and make it some kinda diagonal matrix? So I'll call this, can we make T in this new basis just, I'll call it Lambda diagonal. I'll just call it D for diagonal. So what be, what churn our transformation into a diagonal matrix. So basically, what basis B turns our transformation key into a diagonal matrix. So that's what we'd like to answer. If we can find that basis, then you might like to call that the best basis, right? Because calculations in that basis get really simple. Of course, you've got to first get to that basis. So you've got to multiply by the inverse matrix to get to that basis. But once you're in that space, life is good. Calculations are simple. And it turns out that even though we won't do it in this class, this concept of working in a diagonal basis. Is very powerful and very general. And in fact, a lot of engineering tools that most people don't even think of it that this way. Like if you go ask one of your senior students who's taken one-twenty, who's learned about the Fourier transform. And you ask them, why do you work in the frequency domain? Let say, Well, everything is simpler in the frequency domain is, why is that? Well, it turns out that the frequency domain is the diagonal is basically the eigenvector basis. I just went ahead and told you that how we're gonna get there. But it is the best basis to use, which turns all linear operators into basically diagonal matrices. Okay? So I gave it away. The best basis, turns out to be the eigenspace. Right? So we take the mapping T, we find the eigenvectors, and we use that as a basis. Let's prove that. Okay? So let's, let's multiply t by an eigenvector, right? This is by definition. By definition, just to remind you that if you multiply a vector by, if you take a mapping and you pick a vector that happens to be an eigenvector and you map it, you get the eigenvector back multiplied by constant. In general, a complex constant lambda. We call that constant the eigenvalue. Okay? So now we'd like to try out our transformation on a vector u, which is represented by, let's say U1, V1, u2, v2, and so on. So here the eigenvectors are v1, v2, vn. And we're assuming that these n eigenvectors are all linearly independent. In other words, we're saying that transformation is invertible. So here we have TU times this sum. By superposition, we just multiply each term one by one. You want is just a constant times T applied to v, v1, u2 is just a constant times T applied to V2, and so on. And each transformation, by definition is very simple. Each transformation is simply u1 lambda one, v1. We get V1 back, U2 lambda two, v2. Okay. So this is nice and simple, right? Any questions? Yes. Do you have your hand up? Yeah. We're gonna show that in a minute. Exactly. Yeah. We haven't haven't shown that to yet, but we will show it. Yeah. Other questions? Yes. Can you say that again? Oh, yeah. So what are you on YouTube? I'm glad you guys are asking these questions. U1, U2 is basically I'm representing my vector u is some point represented with basis v1, v2, v3. So the constant U1 is how much distance I move in the direction of vector V1. Alright? So you can think of the, let me just write it this way. U is equal to u1, u2 UN. But it's understood that u is already in the eigenspace vector vector space, the eigenvector vector space. All right. Other questions. Yes. Good question. So here we're assuming that the eigenvectors do span the space, right? In other words, all the eigenvalues are non-zero. In other words, the matrix is invertible. Okay? Good questions. Okay? So we're going to do is we're going to rewrite. It looked like it looks like I did use the same notation. Good. We're going to rewrite this T you simply in some matrix form. So first thing we're going to do is we're going to form a matrix Q. Q is going to have our eigenvectors as columns. So Q inverse is the transformation matrix from standard basis to the eigenbasis. It's everybody should get are you getting familiar enough with the notation here that that statement makes sense that Q inverse is the transformation matrix that takes you from standard basis to the eigenspace basis. Yeah, exactly. It's by definition, remember, we found that for any basis, to find the transformation matrix from standard basis to that basis, we take the basis vectors, we form a matrix of columns of basis vectors. We invert that, that is your transformation matrix, right? So that's exactly what we're doing here, is we're forming Q in that way. Yeah. Why is it negative is not negative. Oh, this is a this is a bullet point. So not negative. Yeah, Thanks. Thanks. Good point. Other questions. Today everybody is paying attention. This is really nice. All right. Other questions. Alright? So we're basically just rewriting this. We're just inventing some simple notation here. So we could say that this is q times some matrix. I'll call lambda times the vector u. We know what q is. Lambda is simply a diagonal matrix of eigenvalues. And we know what U is. And you can basically multiply out these matrices and verify that this is indeed true. Lambda you simply lambda1, U1. Let me just write it as a vector. So let me just verify this. So, so lambda u is lambda one, lambda n times U1. And so this is simply lambda1, U1, lambda2, U2 lambda n, u n flips. And now when I multiply by Q, this is multiplying q by this lambda1, U1, lambda n un. And remember the definition of matrix vector product is a linear combinations of the columns of V. So this is the first column of Q is V1. By definition, that's how we defined Q. So we take a linear combination with coefficient lambda1, U1. The second column of Q is the second eigenvector V2, and we take lambda2, U2, and so on. So indeed, this form q lambda you write is exactly the same as what we wrote over here. So we can write this as Q lambda view. Okay? And we have this diagonal matrix Lambda. Now, TU is now in the standard basis. Let's push this into the eigenspace spaces. We have TU is Q Lambda u. So let me premultiply everything by key Q inverse. Actually, I don't wanna do that. I want to take you and just say that, yeah, you, if we had a standard basis vector x, q inverse would take us to you. Right? So I can write this as Q Lambda Q inverse times x. So we've shown that actually any mapping T is similar to another mapping. It's basically the same mapping with a different basis. In this case, the basis is Q by definition. So the matrices T and Q Lambda Q inverse are similar. The only difference is one is acting and standard basis t. And this other is acting on what basis? Well, whatever basis Q is, well, we know what Q of basis that is. By definition, it's the eigenbasis. And what is the transformation map to a diagonal matrix? Isn't that beautiful? I mean, that that's amazing. What we've done is we've taken an arbitrary transformation which is very convoluted, right? Every time you put on a basis vector. And you want to see what the output is, you get a linear combination of all the basis vectors, right? So as we every time you put in an arbitrary vector that has multiple coordinates on different bases, they all get mixed up. And that's why we get a big complicated matrix equation. But if we view the problem, we just change our perspective and say, in this eigenbasis, what is the transformation look like? It's a simple diagonal matrix, which means that in this basis nothing gets mixed, right? You just, every coordinate just gets scaled by constant, lambda1, lambda2, and so on. I think there was a hand up, Yeah. Q inverse one. That's exactly what we did, right? Yeah. So basically let me know. Like let's just follow what I did here, right? Step-by-step. So step one, this you agree with, right? That's from the previous page. Now I'm saying you as simply this. And I'm going to substitute u into here. Right? That's all I've done. And now this equation, let me just highlight it a little bit differently. No, because what I'm trying to do is write, I'm trying to find a relationship between the transformation T and write this new transformation which is diagonal. Oh, I see what your point is. I see your point. Okay. Yeah, Let me take a step back here. I think I went too fast and messed up. So basically TU. Yeah, thanks. I think you're right. So here at TU is basically t q inverse x, and that's equal to Q lambda u, which is Q inverse x. So basically, this is what we're basically saying that the transformation in this new basis as lambda. So let's do a quick example of how we would use this is actually once you know this, it makes your life a lot easier. Let's say you want to solve this problem. T is some matrix times x is equal to B. Okay? So of course you can solve this problem by doing all the things that you did in 16. A Gaussian eliminate, Gaussian elimination and all that. But for theoretical understanding and for solving problems like what we're going to solve today. What we're gonna do instead is use the knowledge that this t can be represented in this form. Okay? So what we're going to do is we're going to map this problem. We know that u, here's the representation of x in the eigenspace basis. So we can actually substitute here with you. But remember b is still in the normal basis. So we need to also map B into the eigenspace basis. So let's multiply both sides of this equation by q inverse. Of course, by definition, this is I. So we have lambda u is equal to B tilde. So b is now b. B tilde is B in the eigenbasis. So there we have it. We basically have mapped our problem to solve into an equivalent problem, which has really simple. Now you might be thinking, why did I even learned Gaussian elimination? Why can I just do this? Well, actually, Gaussian elimination is the most efficient way of solving a problem. To solve this problem first, you have to find the eigenvectors, right? That's not easy. Then you've got to map everything into the eigenbasis, right? That's not easy. That's multiplying by Q inverse. It's more work to do this from a compute perspective. But from a theoretical framework, the insight it gives us is amazing. And it will actually, if we're gonna be doing a lot of calculations, like let's say you need to apply this transformation matrix 1 million times. Then you might say, well maybe if I solve the problem in the basis of t, which is the eigenbasis. Those million transformations are just 1 million diagonal matrices. That's much easier than doing N cube matrix multiplies in the standard basis. So there are situations where it's gonna be much more efficient to work in this new basis. Okay. Any questions? Yes. Okay. The Motivation is what we want to do. We basically let me just back up and say, we first of all, ask ourselves, what basis, what does it mean to be in a different basis, right? We acknowledged that we've been working more or less than standard basis all along. Basically when we write a vector x, we're saying that it's x1 times x1 plus x2 times e2 and so on. That was just a particular choice. Last lecture we shuts, we showed that we could also work in any other basis, like we define another basis which was cream and Cory Hall, right? That was our other basis. Or northeast or pick any two points? Pick any two points, basically, pick any three points on a plane, right? One is gonna be your origin, the other two are going to be the point of a vector. As long as those three points are not collinear, you can map any point. Trivial, trivial for you guys. At this point. We're saying in an n-dimensional space, the same thing is true. We can pick any basis vectors and use them to represent arbitrary points in that vector space, not necessarily E1, E2, E3. So what we found was a relationship between going from one basis to another. Then we ask the question, if you have a mapping or a mapping is a linear operator, linear transformation, which we are represented as a matrix a in the standard basis. What's the corresponding mapping in another basis? We found how to do that, right? We take the columns of the new basis who form a vector q. Then we multiply out Q inverse, a queue or the other way around q, q inverse. And we're now have the same transformation in a new basis. Then we ask the question, okay, we can do this. Is there a best basis to use? Would you prefer one basis over another? Well, without going into the details of the transformation, you would say no, they're all equivalent. But then once you have a transformation, if you can somehow modify that, if you can find a similar transformation, what do I mean by similar transformation, meaning the same transformation in a new basis. If you can find a similar transformation that's diagonal, then you've won the lottery, right? Because that's much easier to work with. So the motivation to answer your question is to find the simplest representation of a transformation. What's the answer the question, what basis makes the transformation look as simple as possible? Which is the diagonal matrix. Okay? Yeah. Okay, How do you multiply if you want a good question, how many operations does it take to multiply two matrices? You have to take every row, multiply by every column, right? And you have to do this over and over again, right? Row times column is n multiplications, right? And then you have to do it n times n squared. Now you do the same thing with a diagonal matrix. How many times does it take? Just end, right? Just once. Diagonal matrix times column, you're done. So it's much more efficient. So that's just a computational perspective. But from a simplicity perspective, it's much easier for me to solve a problem if it doesn't involve ten variables. If at all it only involves one variable, I'm happy I can probably go and ask my son who's in sixth grade to solve the problem, right? If I can say, if I tell him what's phi of x equals three, What's x? He's going to say three-fifths, right? That's what we're doing here. We're actually taking these complicated problems and we'll see how this applies to a vector differential equations. Going to take these complicated vector differential equations and turn them into very simple differential equations that we've already solved. Your hands up. Okay. I think I got your question. Your first question, maybe it's going to take too long to answer because that was all last lecture. And the beginning of today's lecture, which was the Daphnia. We found that if you have a basis to transform into that basis, we form a matrix which has the eigenvectors, is not the eigenvectors which has the basis vectors as columns. Then we invert that matrix, and then we multiply matrices in the standard basis that maps us two matrices in the new basis. So that's all of last lecture and the beginning of today's lecture. The second question is, this new basis kinda seems abstract. It's not really in our original problem. So big deal, you can solve a problem in this new basis. How is it helping you? Is that a good rephrasing of your question? Yes. So maybe with a little more attitude, you know, why why did you do this? The reason we do it is that even though in this new basis we can solve problems very easily, you're absolutely right. This new basis is very abstract. When we look at real circuit examples, you're going to say, hi, my original basis was voltages and currents on capacitors and inductors. This new basis is some weird linear combination of voltages and currents. I don't even understand what it's doing. So it turns out that you can, if you're sufficiently inclined and clever, you could actually go and understand what these new basis is mean. So there is a way to understand why these new basis is. Simplify the problem. Again, to give you an example, sometimes these Eigen, Eigen vectors are called modes of a system. So I'll give you an example. You're beating a drum and very different problem, but the mathematics are similar. You're beating a drum and you're looking at the surface of the drum as it moves and generate sound waves. And you might ask, well, if I hit it with a hammer on this corner, what tones are going to come out, right? It turns out that you're going to hear all the natural frequencies that that drum can produce when you hit it with a hammer. But why did certain tones come out with certain amplitudes and others came out with other amplitudes. If you now transform the problem into the eigenbasis of this drum, it turns out that there's these really pretty sinusoidal waveforms that represent fundamental modes. If you want to vibrate at 10 hz, 20 hz, 30 hz, There's very well-defined modes in the structure. And when you hit it with a hammer, you're kind of setting up some initial conditions for which modes are allowed and which ones are then allowed to play. I know that's a very long-winded, weird answer to your question. But these modes are important. What I'm saying is that they're abstract, but they're important. They tell us something very fundamental about the system. Now that's something that you're going to spend. You can spend your whole career studying the modes of a drum and how they produce sound and the acoustic theory. You could spend a lot of time going into the depths there. The point I wanna make is that you're absolutely right. If you don't understand the underlying physics of a system, these modes can seem very abstract. A lot of physics and engineering is about studying these modes to understand what they mean because they actually give us a simpler picture of a problem. And then finally, just to be practical, when you, if you really just care about the voltage on a capacitor or not, all these weird modes. You can always convert back. How do you convert back? You multiply it by Q, right? Or B or whatever the transformation matrix is, if you're working in the eigenspace is Q inverse takes you into that space. Q inverse brings you back. So you always can come back to, back home if you'd like, into the standard basis by multiplying by. So basically if I have x over here, you overhear what we're saying is that Q inverse goes in this direction and Q brings you back. So what we can do is we can take our problem, which is complicated over here, solve it in this space, and then come back to the standard basis. Okay? So we pretty much set all of this already. I won't just just to be, just to show you how simple it is. Of course, this is just one over lambda one, right? This is the simplest matrix to invert. Ok? So this is how we invert a diagonal matrix. Obviously we can't do it. We just element-wise invert. We can only do this for a diagonal matrix, right? Otherwise, you know, that's not how you inverted matrix. Okay? So this summarizes what we've learned. I think I've said this enough. I'm not going to say it. One last thing is kind of an application of this matrix powers in many systems we talked about earlier. Like you need to take powers of a matrix, right? What is a power of a matrix mean? Well, it just means as long as n is an integer, It's very clear. It means apply a n times. They take an input, you transform it wants, then you transform it again and again and again and again, right? And of course, we can multiply those matrices out and just write that as a to the nth power. So how do we actually compute this efficiently? Well, if we represent a in this form, then we make a very simple observation that when we're doing this product, right, we can't change the order of the products, but we can certainly evaluate products in a nice order without changing their order. So what I'm gonna do is I'm going to evaluate this product, right? Because that product is just i. So then this becomes q lambda I, lambda q inverse, right? And so on. But of course this is just Lambda squared q inverse. And then here I have again Q Lambda Q inverse, and so on. And you can pretty quickly see now that all the internal cues cancel out. And what you end up with is Q lambda to the nth power. Q n lambda to the nth power is very easy because it's a diagonal matrix, right? Lambda to the nth power. We element-wise raise each element to a power piece of cake. Okay? Any questions on this? Okay, so now we'd like to apply all this good stuff to what we were talking about last time, which was a kind of a vector. How do you solve a vector differential equation? So let me just remind you of where we are. And then we'll come back. So last lecture, we found that if I have a system, I'll just draw an arbitrary system here. So last lecture, what we showed was you can basically form a vector x, which we call the state of the system. The state of the system is formed by the voltages and on the capacitors and the currents through the inductors. Okay? And the reason we call this the state of the system is we just observe that if we know what these four numbers are, right at any given instant in time, if I know what the state of the system is, I can solve for everything else. Because everything else is just simple resistors, dependent sources, independent sources. In other words, you can imagine replacing this circuit. Let me just replace one by one with IL-1, IL-2, VC1, VC2, right? Clearly you can solve this problem, right? Why did we pick the inductors and capacitors? Why are we picking on them? Well, they're the finicky ones. They're the ones that don't have an instantaneous input, output relationship. Their current values depend on the past. We have history, there's dynamics involved. So as long as we can track the dynamics of the state variables, we're in good shape. Okay? And then last lecture, we showed that you could actually write by using KVL and KCL, we can actually write. The dynamics of the system in the following form. Here. X again is the state of the system AS some matrix, right? It's going to have the number of rows and columns are equal to the number of states in the system. It's a square matrix. And B S is the sources that we have independent sources. The true independent sources, not the voltages on the capacitors and the currents and the inductors. So we need to solve this problem, right? I think before going through and convincing you that we can always, what I'm gonna do in today's lecture as well as convinced you that you can always do this. Okay? But before doing that, I want to just take advantage of all the machinery that we just developed and solve this problem. So let's solve this problem. So basically, how do we solve this vector differential equation? Let's see if what we learned can help us. Let's write a as Q Lambda Q inverse times x. So here we assume that a is an invertible matrix. And it kinda has to be right, because what we're doing is we're saying this is a physical circuit. And what we're trying to a, represents kind of a statement of KCL and KVL, which we know is true. Always write KVL and KCL are always true there. Basically, fundamental relationships that we get from electromagnetics. Voltage around the loop is if you go around any loop, the voltage is zero. If you look at charge is always conserved. So at any node in a circuit, current going in, it's current going out, right? So there's, those are fundamentals that tells you that if the matrix a is not invertible, then there's something wrong with the physics, right? So we can always represent it this way. And now what we're going to do is realize that this is you, right? This is the state x in the basis of the eigenvectors of a. Okay? And of course, if I pre-multiply by q inverse, I have lambda. I have Q inverse times Q times lambda times u plus q inverse B S. Okay? And simplifying this a little bit, we get lambda u is equal to b S tilde. So now what are these two vectors, u and b? Well, we know what u is. U is simply x, the state vector in a new basis. In a similar vein, b Tilda is Q inverse times B S. B S is a vector of sources. And so Q inverse times the sources is the source vector in the basis of the eigenspace. Okay? Alright, so what can we do with this equation? We have q inverse, x dot is lambda u plus b tilde vector. So this is a funny double tilde. I guess we put the tilde first or the vector for output that tilt first. Okay? Now we're going to take advantage of the fact that differentiation, right? X dot is simply d by d t times the vector x. And if I multiply both sides by Q inverse, this is Q inverse is just linear. I can interchange the order of differentiation and multiplication. And this is again you. So this is d by d t of u. In other words, this is u dot. Well maybe you don't like the dot notation, but you can also write this as, I'll write it both ways, just so it's clear what this is. This is d by d t u is Lambda u plus bs. Okay? So we've taken our system of differential equations and transform them into another system of differential equations. But everyone here already knows how to solve this new system of differential equations. Why is that? How do you solve this new system? Hit Solve it row by row. What is the first row? Say? The first row says d by d t of u1 is equal to lambda1, U1 plus bs one. Does that look familiar? Of course it does. This is a first-order constant coefficient differential equation. What's the homogeneous solution? Well, if we set the source equal to zero, then again, we're setting the source equal to zero in the eigenvectors space. So that's a little tricky. But if we could do, if we do that, we can say that, well, let me try e to the lambda1 you, I take a derivative, you know, you've seen this before. I get e to the lambda, e to the lambda one, e to the lambda one u is equal to lambda1 a, either lambda one, u, not u, t. And of course the a's cancel out. Lambdas cancel out. And indeed one is equal to one. So this is true. Of course, you know this, you've solved this problem many times before. So, homogeneous solution, we know what it is. What's the general solution? Well, remember the integrating factor method. We could use that to solve for any arbitrary source B S. We can use the integrating factor method and find an integral relationship between VS and the output. So this is cool. That means that in the eigenbasis, my problem is really easy to solve. I just need to solve in first-order differential equations. And then the actual solution, when I transform back to the real space that I'm interested in, which is voltages and currents on capacitors and inductors. What is that? What is the transformation? Well, it's just a linear combination of these guys. So I'm going to take a linear combination of the eigenmodes solutions. Notice I'm using the word mode here. I'm going to take the linear combination of the eigenmodes of the solutions. And that's going to give me the actual real solution that I'm interested in. Okay, so we'll do an example of that. If time permits at the end of the class, I'll pull out my computer and we'll have some fun with it. But let's now go back to last lecture, which is, I need to convince you that this is true. I need to convince you that you can always write the differential equation x dot equals Ax. And I need to give you a systematic way of finding the vector a matrix. A yes question. Very good question. Yeah. The state of the system is equal to the sum of the number of inductors and capacitors. So therefore, the dimensions of a or N by N, which is, let's say n plus n, where n is the number of capacitors, m is the number of inductors. Okay, Other questions. Yeah, in the back, x dot is a notation. It's Newton's notation for derivative. So d by d t is a Leibnitz notation, notation. X dot is Newton's notation. I wrote it both ways just to be clear. Let me go back to the derivation. Here. Let me just change everything by two, d by d t, right? So I have u over here, right? This is you. And I want to represent the left-hand side, not as x, but as you say, I want to transform X into the US space. Well, I know I can transform it by just multiplying my Q inverse, say multiply everything by q inverse. And I can change the order of integration, differentiation, and multiplication. So that actually gives me this term, which is in fact u dot. Again, I'll use the notation d by d t EU. That clear. Cool. Yeah. Alright. Any other questions? Okay, So next we're going to actually prove that you can do this. And I'll use a kind of a running example I'll use soon. Why not just use this circuit? Because this is the one I solved on my computer. We can use this as a running example. Okay, so the question is, how do we actually find that matrix a? You can do it by inspection. Of course what I mean is you just apply KVL and KCL until you find a. And I did that last lecture. Someone pointed out that I had a redundant equations. I had to go back and find a third equation. And we had three equations, three unknowns. And so I had solved the problem. The question is, how do you do this systematically? Like how do you tell the computer, hey, set up the matrix a, right? You can't tell the computer, Hey, do KVL and KCL and until you get enough equations, then you're done, right? It's not the right way to do it. So there are actually efficient ways of doing this. But beyond what we've learned in this class. If you look at a very classic circuit theory book, for instance, to Berkeley authors who are famous, deciliter and cool, and later on Tewa. So later editions of this book also include Leon Chewa was a giant in the field of circuit theory. Chaos. In their book, they basically tell you very nice algorithmic math, mathematical way of drawing a tree and finding the cut set and finding this matrix a. I wanted to come up with a way of convincing you guys because I'm the type of person I hate it when someone says, Trust me, you can always do this, right? I'm like No, Show me. I don't believe you. So I wanted to come up with a way of showing you guys without introducing any new extra lectures. Because there's not that in some ways, it's not that important is just to convince you guys that you can do this. And then I realized you actually have all the tools you need. All we need is Thevenin and Norton. And then we can show that the matrix a can always be formed. So let me show you how we do that. So this is the algorithm. You can use it. If you're on the exam and you're having trouble finding the matrix a, you can always come back to this algorithm and use it. And this algorithm says, Okay, for each inductor or capacitor in your circuit. So think of this like a for-each statement. We're gonna loop. Let's say C1, C2, all the way to cn, L1, L2, all the way to LM. For each state variable, what we're going to do is we're going to take the representation of the circuit where we've replaced the state variables with independent sources. And we're going to look into the terminals of the particular state we're at. Let's see where it capacitor C5. So we're going to take C5 out of the circuit and look inside the rest of the circuit and say what's the Thevenin equivalent circuit or Norton? At this point. And of course, it's well-defined because there's no inductors or capacitors in the circuit. It's just all independent sources and resistors and dependent sources for instance. So we can always do this. And then we can say, well, the dynamics of this capacitor are therefore determined by this Thevenin equivalent circuit. We'll do an example to show how this works. So again, this is just writing it down. We look into the circuit from the terminals of LRC. Using this modified schematic, we find the Thevenin equivalent voltage or current. And then we write a simple first-order differential equation that describes that particular equivalent circuit. So definitely, we have to do an example to see how to make this clear. It sounds complicated, but it's actually quite simple. So remember, the first step was to take our circuit and transform it into a new circuit. Well, I'll just redraw. It's probably just as fast. So this is a V S, Let's say V S or S. Let me define some polarities. Remember, these are just arbitrary, as long as I'm consistent, everything works out. So this is IL-1, this is IL-2. This is the direction. This is VC1. Better make this nice because I'm going to use this over again, over and over again. This is vc2 and this is RL. Okay? So this is the equivalent circuit. And what I'm gonna do now is I'm going to pick. The first element, the first state variable, which is VC1. So again, I have the advantage here of copying and pasting. If you don't, you might have trouble keeping up. Okay, so I'm gonna take this circuit and let's consider VC1. Maybe I should duplicate this four times. Let's say duplicate. A duplicate at page after. No, I don't want blank. Never mind. Okay. So let's take VC1 out. And then here I'll say it, this is C1. And I'm going to say, what's the Thevenin or Norton equivalent circuit looking into here? Okay. Well, you know what doing southern and Norton's, a lot of times it just pays off to take a deep breath and look at the circuit and say, do I actually need to do any calculations? Or is it obvious? In this particular case? It may be obvious, right. If you look at it carefully, you realize that I have two current sources meeting at this point. So remember, for Norton equivalent circuit, if you short circuit the black box, What's this current flowing? Just I Norton, right? So this circuit, if I were to short-circuit these two terminals, What's the current flowing through here? It has to be iL1 plus iL2, iL1, iL2. Otherwise we get conservation of charge violated, right? So that's it, That's INR, I don't need to do any calculations. What's our Norton? Will think of our Norton as if I were to vary the voltage at the terminals. How much is the current going to change? If you vary the voltage across IN IL-1, IL-2, how much is the current going to change? Not at all. Their current sources, right? So I'm, our Norton is infinity because the circuit will always provide I-naught no matter what you do to it, right? No matter what you hook up here, doesn't matter that you shorted it. You could put a resistor, you can put a kangaroo, you can put anything between those two terminals. And if those current sources are true to their character, they're going to provide IL-1, IL-2. Is that clear to everybody? So as far as the dynamics of C1 are concerned, we can say that the circuit looks like this. That's it. So if I wanna know how VC1 is changing, I can just say D VC1, d t times c1 is equal to IL-1, IL-2. Notice that this is exactly what I want. This is telling me the dynamics of state variables c1, how they evolve. Only in terms of state variables and sources. In this case, it doesn't matter what VS does. What matters is what the two inductors are doing. So the charging and discharging of this capacitor only depends on what's happening in these two inductors. If I know the state of those inductors, I can tell you how this capacitor is going to evolve, right? Any questions on this? Yes. So why is our Norton infinity? Well, our Norton is infinity because it doesn't matter. What I hook up here. Like let me tell you. Let me give you an example. Let's say you have a black box and you're in the lab, you short-circuit the black box you get IN. Now you hook up, I don't know, 50 ω and you still get IN. What can you say about R n? Well, you can say that our n is infinity because otherwise there would be a current divider between RN and you're 50 ω. Alright? So normally if I put a short circuit here, I'm always gonna get the current IN to flow, right? But the moment I changed from short-circuit to some arbitrary RL, how much current is going to flow? Well, this is a current divider. So i out here is going to be basically r n, r n plus RL times I end. This is remember the current divider. But I'm telling you that it's still IN right. So what is our N needs to be to make that true? Well, if our n goes to infinity, then this is true. Okay? So step one, we found the dynamics for C1. Let's do the same thing for C2. Let's see what it has in store for us. So now we're, we're considering C2. So we're gonna do is we're going to take CO2 out. And we're going to look into here. Okay, so for C2, what is the Norton current? Notice that I've got I Norton coming through here. And this current I1 Norton is gonna divide between these two paths. If I short circuit this path, then certainly all the current is going to flow here. So I, Norton is simply IL-2. That was easy, right? Likewise, our Norton is really easy because if I look into the terminals of an independent current source, What's its output impedance? Infinity, right? By definition of current source has infinite output impedance. It will not change its mind no matter what you do to it, right? It's always going to give you the same current. So if I look to the left, I see a current source. So the path to the left is infinity. And of course the path to the right is just RL. Put them in parallel, I just get RL. So the equivalent circuit is IL-2. That's my eye, Norton RL. That's my our Norton connected to a capacitor, C2. So let me write a differential equation for this. I can say that C2 DVC two d t, That's the current going into a capacitor, is equal to IL-2 minus VC2 over RL. Okay? That's it. That's exactly what I wanted. This tells me how does my second state variable evolve? It evolves according to this differential equation. Yeah, current source. So remember here's another way of looking at it graphically. This is a, this is I, this is V, right? So this is something with some slope. G is one over r, right? What happens if this is a flatline? Slope? Slope is zero, R is infinity. Okay? So notice that on the right-hand side, I only have state variables. On the left-hand side, I just have how my second state variable evolves in time. Great, That looks good. Let's go to our first inductor now. L1. Let's see how L1 state evolves. Again, I'm going to take this circuit, redraw it, or cheat, copy and paste and take L1 out. And then ask, what's the Thevenin equivalent or Norton equivalent? Again, I like to take a deep breath because it helps me focus and simplify and don't have to do too much algebra. My trainer and my soccer trainer told me, whenever you're about to shoot, take a deep breath, right? Take a look around, you. Look where the goal is. Take a shot. Similar thing we wanna do over here. Don't rush into it. Yeah. Okay, so yeah, Why did the inductor becomes an open circuit? So it's not that the inductor became an open circuit. Because remember in steady-state, we found inductors or capacitors to open. What we're doing is we're taking the inductor out of the circuit and looking in to say, how do the dynamics of the inductor evolve. So if you'd like, I'm going to put the inductor over here. And I want to model the rest of my circuit. That's really what I'm doing. Yeah. Good question. So what I'm saying is that at a time t at a given time, I don't care what the inductor and capacitor are doing at any given time. They have a certain value of current and voltage on them, right? In my state variables are the currents through inductors and voltages across capacitor. So I'm going to freeze time and say in the future how a state variable three, which happens to be the current or the inductor one evolving based on the other state variables. Now in this case, if I take a deep breath, I, I observe a look. The voltage here is fixed by VS. If there's no current flowing, then this voltage here is just V S. Also the voltage over here is just VC1. So if I open circuit my system, the open-circuit, That's V Thevenin, and that's equal to VS minus VC1. What about R Thevenin? R Thevenin. If I look over here, let me use a laser pointer. If I look in this direction, I see a short circuit in parallel with an open circuit. So short-circuit wins, it's just short. If I look over here, I just see RS. So the overall Thevenin resistance is R S. And so then the equivalent circuit is V Thevenin, which is VS minus VC1. Notice now VS is appearing for the first time in C it before our S L1. And we know that the voltage across inductor L1 is given by L1 times the derivative of the current. And here we're saying that the voltage in terms of state variables and source terms is VS minus VC1 minus IL-1 times RS. There we have it. We have the evolution of the third state variable. In terms of the other state variables and in terms of source terms. And do the similar calculation for L2, I won't basically we're running out of time. So let me just tell you what L2 is. For L2. I have it's pretty easy actually. I'll let you guys do this at home to verify that you understand what's going on. And now I'm going to put all this together. I'm going to have x dot. Okay? You guys don't like the dot notation. Some of you say you don't. So let's do DX DT. That's d by d t of VC1, VC2, IL-1, IL-2. Now I have a big matrix, four-by-four, not so big. Zero-zero, one over C1 minus one over c10 minus one over r, l, C2, 01 over C2 minus 10 minus r. Can even read my own handwriting. I think that's our S over L1 and one over L2 minus one over L2 zeros zero times x. This is x plus the only place that the source term appeared once here. Let's just call this V. S is a scalar. So this is our matrix a. And each row of this matrix is just the calculation we did look at the last row. Does that look familiar? Right? The last row here is this equation. And every row is just one of these equations. Write this equation is the third row, right? This equation is the second row. And all the way back here, this equation is the first row. Of course the ordering is arbitrary. So there you have it. We took our circuit and hopefully I've convinced you that you can do this for any circuit. And form. Put the circuit into the standard form, which we call vector differential equation. And let's see if we can pull off a minor miracle. Turn on my computer and time and plug it in and just look at a quick example calculation with this. Okay? So I think it's fun to go through this calculation with a, you know, on your own, on Matlab or Python or whatever you like. Because it gives you kind of forces you to really understand everything, all the details. It's kinda like doing homework. Did that pop up? Yeah, hey, it worked almost. Computer is not happy. I think it's trying to find a license. Alright, so what I did is I put that matrix into Mathematica. And then I put in some values for L1, L2, C2, because that's a four-by-four system. You don't want to deal with a four-by-four system. Why are you upset? Computer, come back to life. You're a Mac, not a PC. You're not supposed to do this. I have a Mac and a PC, and PC's have gotten a lot better, but they tend to do this stuff more than max. Okay? Alright, here we go. There's the matrix. Might look familiar, right? Zoom in, window, zoom. That's not what I wanted. Command Plus, you're supposed to zoom in. I don't know what's going on. Alright. There's the matrix, there are some numbers. And then I just did the steps here are all really trivial. It's exactly what we did in lecture. We, we find the eigenvectors, right? Q is equal to eigenvectors of a transpose it because Mathematica gives you the eigenvectors. 