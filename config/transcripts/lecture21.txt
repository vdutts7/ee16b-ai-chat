Okay. Let's get started. Welcome to first Thursday after spring break. Some announcements. These are repeated announcements from last time. A reminder that you have students support hours on Mondays, one-two-three. Sign up on the course calendar for interviews, for appointments. And of course, the fact that you are here means you know that you have in another 0.25 extra credit for every lecture. So we will give you the password when NEMA comes. Okay? So I hope people got the credit for last time there was there were some hang-ups, but for the most part, it went smoothly, right. Did you get your quarter point from last time, if you can. Yeah. Okay. Alright, so last time we had done upper triangular realisation, also known as the shore decomposition. And we had done a few examples, but most of the lecture was focused on giving you a proof by induction. Just for your extra knowledge. We have some extra notes on the class webpage along with lecture 20 from Tuesday. That will go rigorously through the proofs because I kinda did it informally, for the most part. Not that I didn't do it formally, but I didn't dot the I's and cross the T's because this is a lecture and you want to read that on your own. Maybe I'll flash that a few minutes. Would people be interested in seeing the fool-proof? Okay. I see some nods, so I'm not going to go faster it okay. I'm going to go faster it I'm just going to run through it. And before we do that, let me tell you the agenda for today depends on how far we go, but we're going to recap some of the implications of the upper triangular realisation, which we did at the end of last lecture. But I want to recap that because other than the proof, which is fine, it's a math proof, it's an important result, but we are more interested in what does the resultant do for me? Why is it important? What's the implications? And then today's main topic is gonna be about symmetric matrices. And something called the spectral theorem. Okay, it's a really important theorem because it will set the stage for the rest of the lectures, including the SVD. You must have heard of the SVD, the Singular Value Decomposition. So the spectral theorem forms the crux of why the SVD works, okay? So make sure you understand that it's pretty straightforward, but hopefully we'll get through. If we have time, we'll look at minimum energy control, but if we don't, we'll do it next time. Okay. So any questions before we start? Okay. So let me first run through what I mentioned earlier that will summarize the proof by induction, which is on your class web page. So don't take notes, it's already out there. So the theorem, as we said, as we did last time, any real matrix n by n, we'll abbreviate it by M sub n, n to indicate the length of the matrix. So we did two-by-two, three-by-three examples, right, to go through the proof. But the statement is that any square matrix can having real eigenvalues, which I said for convenience. It really doesn't matter if it's real or complex valued. The complex valued more cumbersome, but it's the same concept. Can be upper triangular rice using an orthonormal matrix UN. And the key idea was it for convenience only. So the key is, if you look at the proof by induction in case you are confused about how we did it. Here's the way induction proofs work. Suppose you have a statement, S sub n, right? That depends on the value of n. In order to prove induction in general, those who took, who are taking or have taken CS 70 would know this pad. But for those who haven't, How many of you have not seen induction before? There are a few. Okay. Good. I'm mostly addressing you, but it's also a reminder for those who know it. The way you prove by induction is that you first show that for n equal to one. So you want to show that a statement, S sub n is true for all integer n, right? S1 is true as true is true if three is true and so on. That's the induction premise. You first show that it's true for n equal to one as S1 is true, then you have to say that for k greater than one, greater than or equal to one, if you assume that S sub k is true, then you want, you can prove that S sub k plus one is also true. Okay? And so that totally completes the story because you know, S1 is true and we have a generic statement for any k that for any FK, if it's true, it's also true for k plus one. So if it's true for S1, it must be true for S2 and therefore true for S3, and so on all the way down, right, for there's no value of k for which it's not true. So that's the proof by induction. And what we did last time was we showed that S1 is true because it's trivially true. Because the scalars, upper triangular or diagonal, anything you want it to be. And then the inductive hypothesis is that if you assume any k by k matrix has real eigenvalues, then it can be orthogonalized or upper triangular, right? Using an ortho normal, u sub k. For k is the index for the size of the matrix. And I'm not gonna go through the proof, but we went through all of this and be shorter. We first had our first try where we just completed the basis based on an eigenvector and the rest using Gram-Schmidt, the generic proof. And how do you find these use Gram-Schmidt by extending the basis. So you tell that k plus one is a first try at trying to orthogonalize. And we saw that I'm not gonna go through the details. But when you do u tilde transpose M utilitarian you tilda, you will get a form like this, which is almost upper triangular in the sense that the first row and the first column are respecting the upper triangular structure. The inside K by K part is not. But you now reduce the problem of a k plus one by k plus one into a k by k. So you then, by inductive hypothesis assumption, you know that there is x, there exists a u sub k, which will upper triangular is m sub k, the inner MK. So we did this for 2.3, right? We started with a three-by-three and muted for two, shorted for two. And then if you tweak the orthonormal matrix, you to include a u sub k on the right of r k, which is what we did using the example. You will indeed, you can check that it's orthonormal. And you will find that after you put, put things through you indeed, this becomes also upper triangular with the new choice. And therefore, we are done. Only thing you have to check is that the eigenvalues of M sub k by our inductive hypothesis have to have real eigenvalues. But that follows from the fact that if you have a k by k plus one by k plus one matrix with all zeros on the first column. The, the characteristic polynomial for the k plus one by k plus one matrix is just lambda one times the characteristic polynomial for MK. And since we know that by assumption, all the eigenvalues of the k plus one matrix are real and lambda one is real. So these also have to be dotting the i's and crossing the t's. Okay. This is the complete proof of if you're so inclined, you can go read it. It's quote unquote rigorous, but don't be too bothered by it. More important is to understand the idea of the proof, which we did last time using two-by-two, three-by-three. That's the same. So same principle. Any questions? Okay, if not, lets go back to the main story. Let's recap the implications of having an upper triangular system. So x k plus one is a x k plus b UK. This is our traditional state-space equation, vector equation. And we just proved that any square matrix can be transformed into an upper triangular matrix, which means that u transpose m mu is t. I'm saying the same thing. It's also known as the shortly composition. And the proof is by induction that we just saw. It's also on the class webpage. Now, let's take stock of things. So we know we have studied two different ways of doing of changing basis, right? The, the most recent thing we studied was u transpose m mu equals t. And t is upper triangular. And you happens to be orthonormal. That's by construction, right? Because we use Gram-Schmidt in the process of constructing the you. Have, you also proved last time that M and T, which are also sometimes called similar transforms, similar matrices rather have the same eigenvalues, right? We did. We prove that I'm not gonna do it again. But the eigenvalues of M and T are the same. Not only that, the eigenvalues of T, The upper triangular guy, are exactly the entries on the diagonals right? Before that last time. So it's very easy to get the eigenvalues of M by first upper triangular arising it and reading off the diagonal entries of t, That's exactly the eigenvalues of f. That's another way of getting eigenvalues of m. So now what we're going to want, we then showed is that for any t that is upper triangular, since it can be upper triangular rise to T. That's the same story. If you want to consider stability. It's BIBO stable if and only if all the eigenvalues of a matrix that the system matrix that you start with lie inside the unit circle. We certainly knew this to be true from way back when a was diagonalizable. But now we have a more complete story. Whether or not a is diagonalizable. The eigenvalues of a are less than one or greater than one in magnitude will determine whether you are stable or not. But much more powerful and more general statement. Okay? And the way we did this for, that's what I'm saying here. Even if a is not diagonalizable, this condition holds for checking for stability. So we have kind of flexor muscles have a bit. Alright. So the proof for this, the reason for this is very easy to see. Because if you consider the two-by-two case, again, this is a repeat from last time. X sub I plus one is given by this upper triangular guy times x i plus this. You, clearly when you have lambda and lambda, this is not diagonalizable. So under what conditions can I still stabilize x sub I? Because you might have a system which has these dynamics, right? And I think if you look at your current homework, the circuits problem has a form like that. So you cannot diagonalize it, but you still have to do analysis of it. So what do you do? Well, you look at the second equation, which is x two of i plus one, x two of i plus one, this lambda x two of i plus beta u of i. Now this is just a scalar equation involving only x2. We know how to do that. We've done it all all semester. So what do you check? Well, again, you check whether if I have a bounded input, will my output be bounded. You check by checking whether lambda is less than one in magnitude, right? And it's bounded input will give you bounded output. So now you plug, you consider the first equation, which is x one of i plus one. Yes, lambda X1 plus X2 of I plus alpha, you provide which I've written here. Now you can consider x2 of y plus alpha U of I as some other general input involving, because everything else is X1 and X2 here. But x2 is really an input to the system. It's no longer part of the state when you, when you, when you write it that way. Because you know, if lambda is less than one in magnitude, you know that x2 will be bounded and therefore your general input will also be bounded. So it goes back to questioning the same scalar equation that we saw earlier. If I have a bounded input, do I have a bounded output? And we know the answer to that, you just check whether lambda is less than one or greater than one in magnitude. So clear. So that all the story is. So yes, because x2 of I is bounded, input is bounded. And so we can make the generic statement. If lambda is less than one for all i, then your BIBO stable. So this is just a recap of things we did last time. So let's move on to today's program is going to be. So we have seen that t is u transpose MU. The short form t is upper triangular and use orthonormal. Keep that in mind. So whenever you upper triangular rise, the matrix that upper triangular rises it is an orthonormal matrix, right, by construction. Now, the topic for today is what happens if we have special structure on the matrix that we need to upper triangular is in particular what happens if it is symmetric? We will see where symmetry comes in later. But for now, if you're given a symmetric matrix, what is the upper triangular realisation of that symmetric matrix? So that's the topic for today. So when, so before we do that, let's recap what we did earlier. This has more stuff we did earlier or in 16 a, when a is diagonalizable, we can find a V such that V inverse AV is lambda, where lambda is diagonal, right? And that's when the lambdas or die or the, what, what is the V that diagonalized as the wake you up? You're showing up for your quarter point credit, but I want to make sure you're really awake too. So what is the V that that diagonalized this? The a, yeah. Yes. So the columns of B or B inverse will be the eigenvectors of a, right? This is stuff we did earlier. But here's the key point. When you did this. Many times, did you notice what the eigenvectors look like? Well, not really. You just calculated them. But if you looked at them more carefully, you would see that there was nothing special about them. They were just vectors and they will form the independent basis. But they weren't orthogonal. Orthogonality, something we like because it makes computation simpler. But the eigenbasis is not in general an orthonormal basis, so that's not true. So now, on the other hand, suppose, leave aside the fact that we wanted to diagonalize this using eigenbasis. Suppose we wanted to upper triangular is a. We know that two upper triangular is any matrix. We need to have orthogonal or orthonormal you because that's what does the trick. So basically, you know that U inverse AU is going to be u transpose AU, namely U inverse equals U transpose, meaning you is orthonormal. And the T is going to have the ship. So if you look at it, it says that you can either have an eigenbasis, which gives you a nice diagonal form here, which is beautiful. But bad news is the eigenbasis is not orthogonal or you don't care about eigenbasis. Let's go with upper triangular region. And let's have a nice, beautiful orthonormal basis that diagonalized, that upper triangular arises you. But the bad news is, you're no longer diagonal because your upper triangular, right? I would rather, what would I want to, what is the best thing you could hope for? What are the best of both worlds? Yeah. Yes. You want an orthonormal basis? That is also the eigenbasis wouldn't likely sweet. Life is sweet if you're symmetric. Okay, So that's the punchline. For the rest of the lecture. You'll see the details of why that is true. But the punchline is that if you want orthonormality and you want the eigenvectors of your matrix both at the same time. You need a special structure and that special structure is symmetric. Okay? So we'll see why that is true. Any questions? Okay? So what if we had a real symmetric matrix S? So here are some examples. So this is a two-by-two. So by symmetry, I just mean that you can have an arbitrary diagonal and then that'll act like a mirror. So all the entries are gonna be mirrored on the upper right is the same as the lower left. Likewise, a three-by-three, the d will be reflected, the E will be reflected in the FLP refer. So these are all scalars, right? These are real numbers. So you notice that when S is S transpose, the ijth entry is the same as the jth entry. That's what symmetry means. And as I mentioned earlier, symmetric matrices allow you to get the best of both worlds that we have seen. We explore two worlds. And there's a way of kind of getting the best of everything. That is symmetric matrices are diagonalizable and the eigenvectors of a symmetric matrix matrices are orthogonal. So here's an example. I just picked a symmetric s can see that you're, That's my diagonal. It can be arbitrary. And then you have symmetric entries. Let's find the eigenbasis for S. So if you compute, I'm not gonna do it here. In fact, I just wanna give you the answer. So you form the characteristic polynomial for S by solving the determinant of S minus lambda I equals zero and solve for Lambda, that gives you the eigenvalues. When you do that, you'll see that the eigenvalues are six minus three and minus one for the symmetric matrix. So let's call them lambda1 and green, lambda2 in blue and lambda three in purple or violet. And if you find the corresponding eigenvectors, of course, you'll have to work to do this or I can get the computer to do it. You will find that the eigenvectors corresponding to these are 111, minus one, minus 12, and minus 110. Okay? So these are the three eigenvectors corresponding to the three eigenvalues for this matrix S. If you notice. Well, the eigenvalues of S are all real. Now, it may appear to you that what's the big deal in that. But that's not true if in general, because you have to solve the determinant of this guy equal to zero. So that's a polynomial equation. And in general, the roots are going to be complex. They're not gonna be real, not guaranteed to be real. So it's a special property that they are real. Okay? And that only happens when S is symmetric. I wish I shouldn't say it only happens. It happens when S is symmetric. It might, there might be other cases where it happened Smith. So lambda one, lambda two and lambda three are all real. That's the first observation. Second observation is that these eigenvectors are indeed orthogonal 90 degrees apart. Check, check it out. The inner product of v1 and v2 is one times minus one plus one times minus one plus one times two, minus one plus minus one plus two equals zero. Likewise, you can check the other V2, V3, and V2 and V3, okay? You'll see that they're all zeros, meaning the inner product dot products are zeros. That means they are orthogonal. This is indeed the best of both worlds example that we're going to talk about. Okay? Any questions? Okay, good. So let's, from now we start to write, there's no more slaves. So let's write the spectral theorem. So I, by the way, I was very suggestive when I use S instead of a, because S is symmetric efforts for symmetric. So in case you are confused why I changed the notation? I'm being amusing S to indicate that whenever you see S, Well, hopefully it's a symmetric setting. So let S be an n by n matrix. We're going to assume it's always real in terms of the. So even if I don't say real every time and I just use symmetric, it's implied that I mean real, okay? Because I don't want to keep dragging real around. Then the following statements are true. S can be diagonalized. Whether or not the eigenvectors of S form a basis. Meaning it falls under the same category as being upper triangular. If you have symmetric upper triangular realisation is equivalent to diagonalization. So we know that if Eigenvectors of S form a basis, then you can diagonalize them using the eigenvector basis. But even if the eigenvectors do not form a basis, the statement is still true. Okay? Second point is that the eigenvectors of S form an orthonormal basis, which is the diagonalization, diagonalizing basis for S. And the third statement is that the eigenvalues of S are all real. These are all things that we observed in the example, right? So we saw that S can be diagonalized. We saw that the eigenvectors form an orthonormal basis. And we saw that the eigenvalues are all real. But this is not just true for that example, if true in general. So we will prove that starting now. So any questions on the statement? Yeah. Question. I couldn't hear the second part. Eigenvalues louder. Ah, that's a good question. I think if it has, if it is, if it, if it has complex conjugate symmetry, then it's true. In other words, the generalization of the real to complex value this, if this is a, oops, let's go back. If this is c, then this has to be c star. C star, meaning the conjugate of z. So that's called conjugate symmetry. When your conjugate symmetry, then also these properties hold. But let's not get into that. We will keep lives easier, just real. Although we will have to deal with the conjugate symmetry and all the stuff in the proof of the reals. So stay tuned. Okay. Any other questions? Yeah. Yeah. Yeah, I'm trying to see. So the eigenvectors of S form an orthonormal basis. Yeah, yeah, yeah. Yeah. What I mean is that if if in the second statement, the sort of maybe the correct statement is to say that the eigenvectors corresponding to distinct eigenvalues are orthogonal. So you might have some repeated eigenvalues, which therefore you cannot. But it's still possible to find, complete the basis using the Gram-Schmidt procedure? No, no, they'll, there'll be an orthonormal set. Right? It's not a basis. Yeah. You have to complete it to form a basis. But good question. Yeah. I mean, anything that's orthogonal can be made orthonormal by just scaling, right? So, yeah, in general, yeah, we don't make too much distinction between orthogonal and orthonormal. Because if two vectors are orthogonal, then I can always shrink them to have unit length. And now the orthonormal. So that's less important. But these are good questions. Yeah. Okay, good. So let's continue. So let's do the proof. Let's prove one first. So upper triangular realisation. So remember that U transpose U is equal to T. So T is upper triangular, right? So this is up from upper triangular authorization method. We know that I can take any U, that's orthonormal and upper triangular eyes it. And this implies that S is equal to U t, U transpose U S in terms of t and p in terms of S. Since use orthonormal, this should be true, right? In other words, you can just multiply on the left by you and on the right by U transpose. And you get from the first equation to the second equation because U transpose U inverse. Okay? So these two statements are always true. So let's box them. So now, upper triangular S implies that S, S U t U transpose. Fact, let's write the t in red. Now, taking a transpose S transpose U, t U transpose, transpose. So what, how do you do that? You go in reverse order, right? Whenever you're transpose a times B or B transpose times a transpose. This is gonna be u transpose transpose, which is u times t transpose, which I will write in red, times u transpose. Right? What do we know about symmetric matrices with regard to if S is a symmetric matrix matrix, how is it related to X transpose? Same. So we know that S is equal to S transpose. That implies u t u transpose u equals u d transpose U transpose. So you have T, T transpose. What does that imply? Yeah, same mu times something can do transpose in both equations. So they must be the same. So that must mean, this implies that. Okay? So D is an upper triangular matrix, right? In general, what is T transpose? If T is upper triangular, what is P transpose? Lower triangular. So what are we saying? The upper triangular and lower triangular or the same? What does that imply? It has to be diagonal because upper triangular means you have zeros in the lower part. Upper or lower triangular means you have zeros for the upper part. The only way you'll have equality if there's only stuff in the middle. All the rest are zeros. Simple proof, right? Okay, so here's a picture for you. This is my diagonal and stuff. And this is zero. This is t. And because non-zero, and this is zero, and this is T transpose, since this is equal to this. So the conclusion is that all non diagonal entries of T must be zero. Namely t must be diagonal. Okay, So that proves statement one. Because that's what we were after. Okay, so let's now look at the second proof. This is proof of one, right? What was the second statement? We want to show that you have an orthonormal basis. If S is symmetric metrics, you use now symmetric matrices. The basis for now, you, sorry, if it's you, the basis for upper triangular realisation. Actually diagonalize this S. Okay? So this is the, so S is equal to U t U inverse, or U transpose S equals u times. This is the diagonalization. So in 16, we saw that if the matrix has distinct eigenvalues, it can be diagonalized using the eigenbasis, but symmetric matrix can be diagonalized always. Okay, so that's, uh, so we know that if a matrix has distinct eigenvalues, it can be diagonalized. Using the eigenvectors. E vectors form a basis. You studied that in 168 also. But symmetric matrices are symmetric matrix can be diagonalized always. Okay? So that's the first statement. Um, okay, so now to show that the Eigen, sorry, the EU comprises the eigenbasis. This is the proof of two. So S is equal to u d u transpose d is to emphasize that t equals d t for upper triangular, D for diagonal. So we're going to always be suggested in the way we use these terms because then the stick in your head. So SUD as a symmetric D is diagonal. So S times U is equal to u d u transpose times U. I'm just multiplying both sides by u. So u is equal to what is u transpose u. Use orthonormal, right? Just UD. Fu is just u times d. So let's write it out. What does that mean? So basically, we know that S times. Let's write out the u in terms of its columns. You have new one. You too, a.un is equal to u1, u2.dot.un times d. D is lambda one, lambda two, lambda m zeros zero. This is the SU equals UD, written out where I expressed the matrix in terms of its columns. Let's read out equation by equation from start to finish. One of the first equation, S times u1. That's the first column on the right-hand side, right? As time t1 will be the first column of S times you. What is the first column on the right-hand side? Yeah, lambda1, U1. What about the second equation? Has times u2 equals lambda two times u to the nth equation as f times q n equals lambda n times u. And what are these equations? These are exactly Eigenvector equations. Write a x equals lambda x. The a and u is x. So these are exactly, so u1, u2, un must be the E vectors S because they're satisfied the eigenvector equation. So what is the conclusion? Let's, let's write a one here. Just to emphasize the takeaways are. The proof for first one says symmetric matrices can always be diagonalized. The takeaway from the second statement that'd be proved is that the diagonalizing basis matrix U is made up of the eigenvectors of S. Now, because here's the right pen. Because you was orthonormal. U1, U2, Un are orthonormal. So the eigenvectors of f are orthonormal. That's the takeaway of the second statement. Okay? I mean, we've been through this, but it's, this should be fairly straightforward. If you know what eigenvectors are. If you don't, then you may be in a bit of trouble, but you should review your eigenvector treatment. Okay? So that proves the second statement, right? That's the eigenvectors form an orthonormal basis. Now let's move on and show that the eigenvalues are all real. This will involve a little bit of conjugacy and complex numbers stuff because you have to show that in general, a complex scalar is actually a real scalar. So let's do that. So proof of routes. So let's prove of three. So F times V equals lambda times v. So remark is that Lambda and v are the eigenvalue and the eigenvector pair, Hen pair for S, okay? And let lambda equals lambda real plus lambda lambda imaginary J. Lambda in general will have a real component and imaginary component, right? So let's write it. In general. Let's assume that it's complex. We want to show that Lambda is going to be actually turning out to be real. Which means, what do we need to show? Yeah, so you have to show that Lambda I is zero. Because then lambda, which in general is lambda R plus j lambda i is just lambda, are always. Another way of showing it is that you can show that Lambda equals Lambda conjugate. If lambda is equal to Lambda Lambda conjugate, what is Lambda conjugate? Lambda r minus j lambda i. If they are the same, then lambda I must be zero. That's the only case where you can either show that Lambda I is zero or equivalently we can show that lambda is equal to lambda star. So I'm going to use star to denote conjugacy. I think people use other bar notation, but I'm totally used to doing stars. Okay? So let's try. To show that Lambda equals Lambda star. That is lambda I equals zero. I be good. Okay, good. Alright, so now unfortunately we have to do some kind of roll up our sleeves and do some algebra, okay? It's not the most insightful stuff, but it's necessary, so let's get it done. So let's take conjugates of, Let's box this equation. Call this equation one. So let's take conjugates. So let's write this in green. So let's take conjugates of both sides of one. What do we get? You again? What is the conjugate of a times B? So what is the conjugate of the product of off two elements? In terms of, yeah, it's just a conjugate B conjugate the people know that. You want to prove. Okay? Blank faces. I don't know what to say. Well, F conjugate the conjugate equals Lambda conjugate the conjugate. This is because write it in green. Since E B conjugate equals a conjugate times B conjugate. Is that not everyone? Give me a thumbs up. Okay. Good. So I mean, if you're if you're if you have to, if you're doubtful about this, just write a as a complex number in polar form, right? Think of it as the magnitude of a e to the j theta a and B as magnitude of v e to the j theta B. And you can see that what are the conjugate of that is e to the minus j theta, e to the minus d Theta B, and then exactly becomes a conjugate, the conjugate. So that's, that's the way to show, convince yourself. So when you're doing multiplication, always go to polar form. When you're doing addition, always go to rectangular form, okay? So usual tricks of the trade. So if you want to add complex numbers, express them as real plus j imaginary. If you want to multiply complex numbers, express them as magnitude times the face. Okay? These are the tricks for quick computation. Okay, So let's, okay, so let's continue our, our proof that things are real. So f star V star is equal to lambda star v star. Now, what about S? What does that start? What did we start with? What did we assume S was real, right? What happens if S is real? How that star, star is just S. So this is just S times Vi configuring real matrices. So S is equal to S star. Okay? So if f V star is lambda star v star, now let's take transpose. And what do you get? You will get that V star transpose times a transpose equals lambda star lambda is just okay. I want to be careful about it. For the first time. Is V star transpose times Lambda star transpose. What is lambda star transpose? Lambda is a scalar. What is the, what is it? What is the transpose of a scalar? It's the same thing. Only matrices have distinct, non-distinct scalar. So Lambda is a scalar, so I can bring it to the front. So we have this equation that V star transpose X transpose equals lambda star transpose B transpose. Let's box it. Not, not quite yet. We're almost there. We can simplify the left-hand side. What a symmetry mean. S transpose is the same as S. So V star transpose times S is equal to lambda star times V star. Let's box that. And this is because S transpose equals S. So I replace x transpose pass. Okay? Now, let's multiply both sides. My on both sides, multiply on the right. Multiply on the right of both sides by b. What do you get? You will get that V star transpose S times v. At the left-hand side of the equation, I'm multiplying and multiplying both sides of this by V will be equal to lambda star times V star transpose times B. Right? So let's box that and call that equation a. Now, we also have equation one here, right? Let's kinda carry it through. I wish I had the board, so it'll all be in one place. So recall that this is also true. So let's multiply. So we have a and we have one. And I want to equate these two. So what would you like to try to build out? I know this is kinda dry algebra we are doing on the board, on the iPad. But if I want to make the two equations look similar, so I can do comparisons. What should I do? I multiply one by V star transpose on the left. So what do you get? B star transpose times S times v. We didn't come out very well. Is equal to Lambda. I did something. Oh yeah. Lambda V star transpose times v. So I multiply one on the left by V star, lambda is just a scalar, so lambda can go anywhere it wants. So I will have this equation. And if I box this, call it be. I think now we have, we have accumulated to equations a and B that are telling me what equation ASS, after doing some algebra, I showed that V star transpose SV. Equal to lambda star v star v be after some manipulation, just multiplying and so on is V star transpose SD is Lambda V star V. What are the left-hand sides of a and B? Same thing. So what does the right-hand side have to imply? They have to be the same thing. So what does that mean? Lambda star v star v is equal to Lambda v starving. You're almost there. Because you know that lambda star must be equal to lambda. Remember, we're after showing that lambda star is equal to lambda, that will prove that there is no imaginary component to lambda. There is a slight complication that we have to, a complication. There's a slight extra nuisance that we need to deal with, which is what? Anyone Yeah. That's right. You have to check that the inner product between V star and v is not zero. Because if they are zero, then cancellation is not allowed. It's illegal. You cannot divide by zero. So putting everything together, Let's write down our parts. So a and b that we have accumulated have the same left-hand side. Therefore, they must have the same right hand side. Which means that lambda star times V star transpose v equal to Lambda v star transpose. This has to be true. Now, what is V star transpose V? Well, we know, let's do that in green. So we know that. So let's, let's do this here. V is components of V. V is an n vector and V star transpose. It's gonna be a row vector which has v1 star, v2 star dot, dot, dot VN star. So what is V star transpose times v? That's gonna be a scalar, which is going to be v0, v1 star, V1 plus V2 star v2 plus dot-dot-dot plus V n star v n, which is V1 magnitude squared plus v2 magnitude squared plus dot, dot, dot plus Vn magnitude squared. What S, what can we say about the v vector? Which is an n-dimensional vector of the square of the magnitude of the entries of d can be, be zero. Can be zero. Remember what is V? V is an eigenvector. Can you have a zero eigenvector? Know all eigenvectors are non-zero. So that means this quantity can never be zero. This is not zero. Since v is an eigenvector, right? Therefore, I'm allowed to cancel. Since I'm not canceling by, since I'm not dividing by zero, I'm allowed to cancel is legally allowed. And therefore, lambda star equals lambda, that is lambda r plus g Lambda I is equal to Lambda r minus j Lambda I, which implies that lambda I equals zero, which means lambda equals lambda r. And we're done. That's the proof. The proof is a bit messy, but it's all algebraic. And if you put things together, more important is the implication of the proof, right? So let's take stock of what we did. We went through some algebra and the third part of the proof of the statement number three, because we have to go through this, because we have to earn it. But we have now proved that. Going back to our statement, we have succeeded in proving that. One, S can be diagonalized to the eigenvectors form an orthonormal basis. Three, the eigenvalues of S are all real, right? So that's the statement that was two. And the third statement we will just write here. Okay? Now, Okay, let's take a step back and look at what we have done so far. So we are in the control module. We've considered stability, which in English means that your state blow up or doesn't. No question. Do they run away? Run away States. Second, we considered controllability. Can I get to where I want? That's what controllability is. Reachability. Where can I go from particular point to another point? Controllability is can I go from any point to any point? So controllability is, can I go and I get wet wherever I want? Now there's a third component, which is extremely important. So this was stability. This is controllability. The third is efficiency. So suppose I know how to control this, and I know I can go from a to b. What is the most efficient way of doing it? There are engineers. We need to put on our most efficient hat to make sure that if you want to move a car from a point to a point in certain number of steps. And there are many ways to do it. What is the best way of doing it? So you could e.g. zoom all the way from a to B, right? I can go full speed and get there before I need to get there. The other end of the spectrum as I go, I sleep. And then at the last second I dash. Third one is 0. I'll be slow and steady like the hair. The tortoise will go slowly, slowly and go around. Go to, go to my point. So which of these is the most efficient? And how do we measure efficiency? Well, somewhat control energy efficiency, okay? So there are analogies too. The torque on your car that you have to put on your car in order to go than you are when you're raising how fast you want to go. So there are connections to that. So the control system that we are going to be measuring are analyzing is the same old, same old thing we've been doing forever. And we know how to solve this by now. You can do it in your sleep, which you are doing now. So sorry, let me make this a scalar system for the control. So I don't want a matrix for B, I just want a vector just to make life easy. A to the k minus one, b u of zero plus dot, dot, dot plus b u of k minus one. Now, this is my initial state. So consider k equal to 100. Suppose at time 100, I want to get somewhere. And I know that at time zero, I'm at the origin. And we know how to write x 100 in terms of my matrix, which is here to the 99 times. So since x of zero is zero, I just have a 299 times B times u of zero plus all the other things. Plus B times u of 99. Hey, and if you write this in equation form, you will see that we will get our equation that x of 100 is equal to. Remember our controllability matrix sends shivers down a spina. So you will have B, AB and 0 dot here to the 99 baby. Let's write a to the 98 Oslo times. You have 99 all the way down to u of zero. And if you recall, this was our controllability matrix Z. And we will call this u. And what do we want to do? So what we would like is that there are 100 to be equal to c times u. We know that can be true. But there may be many, many ways of making this equation true. Because for one thing we know that if you want to attain any x 100 on the left-hand side, if n is 100, you know that c has to be full rank, right? Only then I can invert. I can find the u for which X 100 will be. Ah, I can solve for this equation. Yeah. Oh, did I switched the order? I think you might be right. Let's see. Yeah, you zero go with goals with you or zero goes with 89. Yeah. You're absolutely right. Sorry about that. Good catch. So you're not sleeping. Thank you. So we want to exert some kind of control, which is the US use what we have control over so that I can make x 100, whatever I want it to be. I cannot always do that. We know that basically x 100 has to be in the span of c, right? Basically, my vector x has to be in the span of C. So, I mean, you've gone through this in earlier lectures. So e.g. there is, in general, if I want to make x 100 to be the span of C, We would like to C to B. Full rank matrix has rank n. But suppose I'm alone, I'm giving you a more leeway. So I'm allowing you to take many, many steps to go somewhere where you didn't need that many steps. How should I use my extra degrees of freedom? So that's kinda the big picture. So the idea is that you will like to minimize u squared because the square of u is kind of a proxy for the amount of energy that you're using in your input control. Okay? The square of the quantity, right? So the square of u is exactly how much effort I'm expanding to create my objective in order to get from where I want, where I want. So I would like u squared to be as small as possible. Subject to that, I get where I get. I would like x 100 to be see you. But I want the minimum energy control. So this is called Minimum. So in general, so we will have, let's try to be concrete. Let's say C is ten by 100. You ten by one. And D is ten by one. So this is now d, d, f and destination. So the goal. So you can imagine, let's use blue. This is C. So all you want, the rank of C to B is ten. But I'm giving you a width of 100, okay, in this example, so C is ten by 100. Let's move this over to the left. You is one by 100. So off the, how many solutions will the system of equations have? When you have dealt in 16 and earlier in this course, where C was a tall matrix, right? When we have a tall matrix, what is the solution we're after? If you want AX equals B and a is a tall matrix, then what is a good objective to strive for, for, for solving for a and B. And I want to solve for x. Squares, exactly the stuff that you've been doing any asleep, least-squares all over the place. We have a slightly reverse situation. Now, what do we have? C is a fat matrix, not a tall matrix. So what does that mean? That means how many equations do you have? Each of these is a scalar, right? Yeah, you have, basically in this example here, you have ten equations. How many unknowns? 100. How many solutions? One, not one. Yeah. Infinite. The answer is infinite. You have infinite number of solutions to solve this equation. So of all these equations, I want to pick the one that is most efficient In the fence that it minimizes the norm of U, because U is how much effort you expanding to make C u equal to d. And I would love to expand the minimum energy needed to make C u equal to d. And this is called a minimum norm solution in linear algebra. This is called the solid solution to minimize u squared subject to C u equals d. C is short and fat. It's called the solution. The solution u star to this optimization problem. This is an optimization problem. U star, which is called minimum norm solution. Okay? We will name I hear. Oh, yes. So I think yeah, we have reached the end of the road, so let's take your attendance and we will see you on Tuesday. But you need the magic password, right? So go to your, go to the link, everybody have the link. You don't. Okay, let's go back to the, let's go back to the front. That's the link, link that each.org slash lecture that EC. And the password is spectrum for spectral theorem, right? Password a spectrum. Spectrum. Okay. Have a great weekend and see you on Tuesday. Yeah. That's pretty that's exactly what it is. Like an hour. 