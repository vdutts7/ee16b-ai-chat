Okay, let's get started. Happy Tuesday. So cool today, but I'm not going to complain about the weather after all the rain we've been having. So it looks like a tendon attendance very necessary in the extra credit points are not incentive enough, but nonetheless. So thank you for showing up those who did, you know, it's kinda early for you. So just a reminder that I have my office hours after, on Tuesdays after lecture. So I see a few people come show up, but it's not as dense as I would have imagined it would be. I know that it's earlier in the week and the homeworks are not due to a Friday or Saturday now, right. So they've been extended to Saturday. So nonetheless, if you want to ask any questions, conceptual problems, stopped by to 99 cord. Okay? So last time we had studied symmetric real matrices. And we looked at the spectral theorem, which basically we were identifying the eigenvalues and eigenvectors of symmetric matrices, right? And be motivated minimum energy control. And that's going to be the topic for today. But only a part of it. We're going to recap the spectral theorem for symmetric matrices very quickly. The most important aspect of what we're gonna do is what's known as the singular value decomposition. Which is, it's going to take more than one class to explain the concepts of SVD. And that is, I would argue the most beautiful decomposition and linear algebra. You've studied a lot of things. You've studied eigenvalues, eigenvectors, you studied diagonalization, you studied Gaussian elimination, all kinds of stuff, right? But this is the pinnacle of linear algebra because it allows you to express any matrix in terms of three decomposition, u Sigma, and V transpose. We'll talk about that. So that's the plan for today. Also, there is a note 14 that has been attached to the library webpage that you should be looking at. For details. So there might be some inconsistency in notation, but hopefully not too much there. Conceptually they are the same things. Okay, Any questions before we start? Okay, So, so today we're gonna do a little bit of a recap of the minimum energy control and of the spectral theorem which will lead to the singular value decomposition. Okay? So this is from last time, as we know, control systems we have looked at throughout the semester. And we have considered three aspects of control systems. We have looked at stability. When the system state blows up. Controllability and reachability. How can I get from any point I want to, any other point I want on state-space. And of course, states correspond to various physical attributes like position and velocity and so on. And efficiency. It's just a third bullet, which is what we started on last time, which has to do with it's not just enough for me to get from a to B to tell me that it is possible. Can you tell me the most efficient way of doing that? So it's going one up on, of course, you have to be controllable and reachable. But life is more than just controllability. It's also about efficiency. So this is the canonical problem we've been looking at. And this is where the control action is a scalar and the state is a vector of length n. And as you know, we have done this recursion ad nauseum by now, where we know how to write x of k in comes off x of zero and x of k minus one and u of k and so on. So if you consider e.g. concretely, I want to get at K equals 100, starting at zero. Then we know that x of 100 is, you just plug into the recursive equation here, 99 BU of zero plus dot, dot plus BU of 99 in matrix form. This is exactly given by four x of zero equal to zero. At time 100. We will call that the vector d or destination, is gonna be given by c times u, where c is the controllability matrix. And use our action vector, right? So you've zero, if the action at time zero, you have one is action at time one and so on. And by time 99, ah, you will be, you will exert this control over this control matrix to provide this target destination. All this should be review, right? Give me a heads up if you're good with this, okay? Alright. So x of 100, which is b, is equal to c times u. What is the minimum energy control problem? It's to minimize the norm squared, which is a proxy for energy. So we know that the square of certain objects reflect their energy, right? In terms of currents, I squared R, in terms of voltages, CV squared, and so on, right? So the square of the EU is going to represent the control energy that I'm going to expand in order to do my control task. And I want to make that as small as possible. Maintaining the fact that at time 100, I need to get to 100, I need to satisfy both. So clearly, if that is, if this constraint is not possible, then the optimization problem is meaningless. It has nothing to do. So you have to make sure that you satisfy this constraint. While doing that simultaneously. I would like to minimize the energy that I expand in doing so. So that's the concept clear. Okay? So this boils down to linear algebra. Why? Because everything is, because we started with a linear control system. So c times u equals D, and that's gonna be concrete. Let's assume that C is a ten by 100 matrix, meaning that the state dimension is ten, right? And U is of course going to be of size 100. And this is the target state is also going to be of dimension ten, since all states are ten-dimensional. So how do we solve this? Well, that's what we started last time. So you want to solve the problem here. It's actually an optimization problem. So if you study, if you take 127, you will study various techniques to solve these kinds of equations using Lagrange multipliers and so on. But we don't need to do that here. How many people are, have taken 127, you're not likely to have because there's a prereq for that, I guess. Yeah. Okay, sorry, bad question. Solve. Minimum of u squared squared. This is the norm of u squared subject to C0 equals D. Same problem that we studied earlier. This solution is called the minimum norm solution for obvious reasons because that's a solution which minimizes the norm on you or the square of the same thing. Also, as a matter of notation, the u star that solves this equation is often written in this form. So it's called the arg Min of you have used squared such that u equals d. How many of you have seen the argument notation? Okay, good. So some of you have not. But if you haven't, what it means is it's that value of u for which this function is minimized. This function happens to be u squared, right? So this is the notation for that. So minimize the norm on you while satisfying field equals d. Okay, so let's do an example. I want to give you the intuition for how you solve this problem. There's always a math way of doing it and hopefully an intuitive and graphical way of seeing it under small, for small examples. So what is the smallest example I can think of that will be non trivial and still meet the point. Well, kinda get much simpler than this. I'm going to say that x of k plus one. So x has, everything is one-dimensional. So everything is killer, right? So x of k plus one is one times X of k plus one times u of k. My a and B matrices are just a scalar and one can get much easier than that. So if you want to plug in x k equal to one, you'll see that X of two is x of one plus u of one. X of one is x of zero plus 00. So this is the recursion. And if you write it in matrix form, in the form of CuI equals d that we saw earlier. You get a very simple equation. C is just, remember, b goes on the right and AB goes adjacent neighbor. B is one, a is one. So this is 11. This is u of zero. You have one. I'm going to abbreviate by subscripts, use 0.1, and that's the equals. And let's say that I want the target to be two. I am too. So in other words, I want that, I want to start at the origin x of zero equals zero and go to the state two at time two. So this might be a distance. So if there's a distance I wanna go. I start at the origin and I go, it's a 1D thing because this is 1D example. I want to go two away from the origin in two steps. So very, very simple setup. And while doing that, I want to minimize how much energy I spent by controlling u of zero. And you have one, I have only two actions. You have 0.01 and then there'll be an x of two. So I want to minimize the energy I expand in, in controlling my setup. Okay? So this is the problem. So u-star is now argument of u squared subject to u zero plus u1 equals two. How do you solve this? You can guess, right? So simple, you could guess. Who wants to guess? Yeah. U zero equals u1 equals one is actually the minimum norm solution. If you didn't get it, don't worry, we're going to go through it. And I'm not gonna do it mathematically, graphically. We'll, we'll do it both ways actually. Alright, so given you a zero plus u1 has to be equal to two. What is the solution? U0u1, such that u zero squared plus u1 squared is minimized. This is the problem at hand, more generally. So how many? So this is one equation and two unknowns, u zero and U1. I have two degrees of freedom. I can control either one any which way. But I have to make sure that there are some ads to two. But that's it. I have no other constraints. This is it. This is the only constraint I have to satisfy two equations and one unknown. How many solutions are there? Many happen, infinitely many. Like uncountably infinitely many. Okay? So that it's exactly, yeah, some examples. You can do use zero equals to u1 equals zero. You can do use zero equals 1.5, U2 equals 0.5, and so on and so forth. Then you can straddle the entire spectrum, 20-0 to the straight line that connects them are all solutions. Which one is the best one? Well, suggested 11 is. Well, let's see. So before we go into that, I'm going to take a little detour timeout and remind you because there's also going to be useful in our study of the SVD, which is coming up. In preparation for that, I want to prepare you to look at the fundamental theorem of linear algebra, the four fundamental spaces. So this should be a review. How many of you have seen this? Should have had this right? Is it in 16 a or is it in math prereqs for that? I'm not sure. Can you tell me where you saw this first? How many have seen it? First of all. Okay. So some of you have not seen it. Is that right? Do I take from have you seen it? You haven't seen? Oh, okay. Well, I was assuming that you have already seen it. So it's going to flash through it. But I mean, we don't need too much at this. This is more of a reminder. This is something you should know that these are fundamental concept in linear algebra. So much linear algebra in this course. It behooves you to know this stuff. Okay? So what is the big picture? So what I've shown here, so today I'm not gonna do much writing. I'm just going to, I'm going to show the slide and I'm going to explain. So next class we'll do more writing. So shown here is the matrix a is m by n, m rows and n columns. And let's say the rank of a is r, Okay? And as you know, the rank cannot be more than the number of rows, number of columns. So when the rank of full rank, it says, if you're a rectangular matrix, m by n and m is smaller than n, what is the rank of a if it is full rank? Five or ten by 100 matrix. And I say it's full rank. What's the rank of a ten, right? Can be more than ten because they're only ten rows. So in general, min of m and n, 500 by ten, what is the rank is 210. So the rank of an m by n matrix is always know more than the minimum of m and n. That's always true. But sometimes it could be less. Maybe you have rank deficiency. So r could be smaller than MR. min of m and that's what I'm denoting by car. So these are the equations. There are four fundamental subspaces in, in terms of linear algebra. What are they? Well, there is a column space that's shown in green, and I marked the columns in green. So that's a space span by the columns. The columns will live in RM and the column space. Now, what is the row space and column space of this matrix? Well, if the rank is r, then both the column space and row space have to be our column space. And the dimension of the column space and dimensional row space are always the same for any matrix. And in this example there are the nullspace. Here's what, that's all the vectors that take a into zero, right? Sure. If you're, if you're, if you're a vector in the null space of a, if x is a vector in the null space of a, then x is zero. So all the vectors that take a to zero live here. And the dimension of this is going to be n minus the dimension of the row space. So you see that I put the column space and so-called left null space on the left and the row space and the null space on the right to indicate that these spaces are actually orthogonal to each other and these two are orthogonal to each other. This is the fundamental theorem of linear algebra. Okay? So that's all I'm showing here. Column space of a is all the columns that span the columns. That's all omega whole x, which can be written as a times w, where w is any vector in R n, that's the column space of a. The null space of a is everything that sends a to zero, right? W act a acting on w goes to zero, then w is in the null space. And there's a similar story for the row space, which has to do with the column space of a transpose, which is also all the w's that can be written as for any w in RAM. If you write x as a transpose w, Then x is in the row space. And likewise, the left nullspace is when you hit not a but a transpose. So these are fundamental result of linear algebra. And they were actually, if you are, if you want to know more about this, I would suggest lookup Gilbert strengths book. He actually has video. So because there's a lot of this stuff out on YouTube, you can go look and look them up if you are confused about it. But this is a fundamental basics to have in order to embark on future studies, okay, so in general, to prepare you well for everything you do in your career. So make sure you're on top of this. So it's the treatment. Okay. Can you can you see it? Okay, good. Alright, so one of the facts here is that the nullspace of c is orthogonal to the row space of C, right? So we saw that here, the nullspace of C is orthogonal to the row space of z. That's what this right angle means. Now, just to give you the hint of how to show that, because we're going to use this, you're going to use this fact to go back to our example. Remember we have an example with two, we put it on hold to come and give you a little bit cite information on this stuff. So why is the nullspace of c orthogonal to the row space of C? Well, the argument is actually almost by definition. Okay? So what is the null space? The null space of C. Here's all the w's for which c times w is zero. That's the definition of nullspace, right? So c times w, Let's write it as c1 transpose. So let's, let, let's write out all the rules of c0, c1, c2, all the way through. I'm David Rose, times W, which we can also write as C1 inner product w, C2 inner product w, all the way up through C, m inner product w. So C is an m by n matrix. We know this to be zero because that's what nullspace means. So what does this mean? C1 is orthogonal to W, C2 is orthogonal to w, C m is orthogonal to w, and w is in w and the nullspace. So if w's and the nullspace, then w is orthogonal to every row of c. So if you're, if you're orthogonal to every row of c, You are also orthogonal to every weighted combination of rows of three, right? If I put brakes on these roles, doesn't matter. If the rows are orthogonal to W. And the combinations of the rows are also going to be orthogonal to W, non-linearity. But what is the definition of any weighted combination of rows of C? That's the meaning of row space. For growth phases. Row space is any linear combination of the rows. So we've just shown, we have proved that the null space is orthogonal to the row space, giving you a hint of why the fundamental theorem of linear algebra holds. Likewise, you can do the same for the other pair, exact same treatment. So that's how you get. These are the four spaces and they have structure and that leads to space orthogonal. The other two spaces are orthogonal and so on. Any questions? This is all background stuff, okay? So this is okay. Now let's use the stuff that we have just reviewed and see where we go with it. This was our problem. We had left this problem on hold. We wanted to argmin, use u norm squared subject to zero plus U1 equal to two. And this was our matrix C was 11 user on yuan or were you? And we said that the solution is the minimum norm solution. And in words, the minimum norm solution is a solution that has smallest and length in. The smallest norm. Norm is just units of length and Euclidean length. So what is the smallest Euclidean norm vector u such that you zero plus the two components add up to two. Now, we just studied that the row space, we just proved that the row space and the null space are orthogonal for all matrices. Well, our matrix is very simple. What is the row space of see? What's aerospace? There's only one row. So what is the row space? All scaling, all scales of that row, right? That's it. This is the row space, very simple. Row space is a line passing through 11. That's row space of see, what is the nullspace of z? Let's put the nullspace definition. So c times u has to be equal to zero. That's meaning of nullspace. So that means u zero plus one equals zero. So it means you zero and you won't have to be negatives of each other. But they can have any skin in particular user or u1 can be any scaling factor times one minus one. Everybody see that? Okay? These are very simple example, but it makes the point. So what is the nullspace? Is the line going through one minus one, which is indeed orthogonal to the line going through 11. We'll see that in a figure very soon, but you should mentally be prepared for them. Okay, so we will get to this in a minute. So the nullspace is this. And we will see that the u star, the minimum norm solution, should be such that it has no projection on the null space. If you're not, if you're not following, hold on until we go to this figure. So I drew a nice figure for you. Hopefully that illustrates the point. Can you see it? Right? I've written the I have written the problem we are solving over here. Okay. Let me explain the figure. In black. You don't have to copy it because it's so it'd be available for you. So this is the youth zero plus u1 equals two lines. So the axes are u of 0.1. This is the amount of control I have to exert a time zero and time one. So you can visualize them to D. That's why I made it a very simple example. So, you know that this has to be satisfied because i'm, I'm starting at the origin, right at zero. And at time two, I want to be two away from the origin. And the way to do that is to say that use 00 plus one equals two. It's exactly this line. Which means that my solution, that is the control action solution, should be such that I land on the black line starting at the origin at time zero. Is that clear? I need to be somewhere on that line. Anywhere on that line. So here's an example, the blue arrow that's on the line. The red arrow is also underlying. So these are two particular ways of looking at it. Now, let's look at the nullspace of C we just calculated, we just did that. The nullspace is all scalings of one minus 11 minus 10 plus one equals zero. That's this pink line going right through the origin with slope minus one, because zero plus u1 is equal zero. So this is the nullspace. So what is the row space? Everything going through 11, the green line. So the row space is the green line, the nullspace is the pink line. And indeed the orthogonal, right? They need to be orthogonal by linear algebra and they are even for the special case. So the problem is I want to start here and I want to end, I want to end on the black line. And I can go any which way I can, I can, I can go anywhere I want. So what is the solution? The 11 solution is the red arrow. That's indeed the optimal solution. Why? Because the length of the vector, of the red vector is the smallest of all the vectors you can throw that still touch the tips on the black line, right? E.g. the blue vector has a component which is on the pink line, which is this brown arrow I've drawn. Whereas the red solution, which is the minimum norm solution, what is the projection of that onto the nullspace? Zero. It has no component because any component that is on the nullspace is a waste, wasted energy. So this, while both of these satisfy the constraints, this guy wastes energy by first moving left and then going there. This guy doesn't bother to overlap because he goes directly. So that's the intuition that clear. Okay, good. So this is exactly the, keep this picture in mind when you're doing many complicated matrices and doing minimum norms. Okay? So this is as simple as this. So this is all I'm saying here that the minimum norm solution, the red vector v star, has no component in the nullspace of C. In other words, the projection of the red line on the pink axis is zero. That's what it means. That the brown vector is wasted energy. So you shouldn't be wasting energy in doing your control. Okay, that was a simple example, but it's an illustration of the power of how we think. So. What about the general setting? Okay, I gave you a Mickey Mouse problem, but it gave you, but it gives you some intuition, some graphical way of looking at it. The general minimums, minimum norm solution still involves this. And we saw that what we want is that you want the projection of u star on the nullspace should be zero because that's the minimum norm. Because any, any projection on the null space, as I said, was wasted energy. And here's kind of the formal proof of that right here in green. To just go through it, let's go through why it's true. I showed you graphically and intuitively why that's true. But here's kind of the quarter chord formal proof. If you have your solution, your solution has all the solutions that satisfy the constraint. Namely, in our example, in our simple example, if all the vectors that started at the origin and ended on the black line, right? Those are all valid solutions. And you solution is going to be U star plus u is a vector addition. If you solution as you start plus u null, then let's take the norm squared, which is what the bottom line is. So I take the norm squared of a US solution, which is the sum of the squares of these two vectors. You start a new null, you end a few null. Okay, Let's do our a plus b squared formula. So it's norm of u squared plus norm of n squared plus two times the inner product with the new star and UN. But by definition, we said that u star has zero projection on the null space. So the inner product between u star, a new null is what? Zero. It can't be zero because that's what I'm calling u star. U-star is that thing which has zero projection on unit. So this is zero. So therefore this is u star squared plus u squared length. So certainly greater than or equal to U1, U2 star squared. So if it's only equality when you nl squared is zero, namely u null is zero. So this is the proof. So it's kinda like saying that it freely Pythagoras, the hypotenuse of a right triangle is greater than length of its sides, right? That's all it is. Because this is a hypotenuse. This is one of the sides of the right triangle. But even in higher dimensions, this is still true. Okay? So that's all on that. Okay? So let's, we know this to be true. Now let's use this to form our minimum norm solution. So we know that the row space is orthogonal to the nullspace. So the way we do it is we know U star has to lie in the row space. Example. Here's u-star and as lying in the row space, right, in this example. But it's true in general, even in higher dimensions. So you know that u star disk in the row space of C. So which means we know that the row space of C is what is same as the column space of C transpose, right? If you want to find the span of the rules, I just flip it. If you've, if you transpose the matrix, it's the, it's the space of the columns in the transpose space, because I prefer to work with column spaces. So it's C transpose times w, all the w's for which C transpose w. U star has to be in the row space, which means you start a C T, C transpose times w for some doubling. We also know we have to satisfy our constraint that I need to reach to D in, in whatever 100 timesteps or whatever I said. So see you must equal d. So I have two constraints. So by the way, if you go here, we can also see that here. So the solution that both is in the row space and satisfies the constraint is why the tip of this arrow is here. That's why it's the 11 In general, solution will always satisfy that. That is, it must simultaneously satisfy C u equals d, the black line, and that you have to be in the row space of c. So all minimum non affiliations lie in the row space of C and satisfy the constraint. And that's exactly what we're gonna do here. So we also want c u star equals d. So let's use the fact that C star is, we know that u star a CT times w. This is what puts you start in the row space. So c times c t times w equals d. Now, solve for this. And for the moment, we will assume that c, c transpose a is invertible. So WSCC transpose inverse times d and u star, which is C transpose times w, is given by that. That's our minimum norm solution, our general solution. Okay? So it's some metrics stuff. Now C transpose C, C transpose inverse times d. But don't worry about the formula. I never remember these things, but always remember how to get them. Just know how to get them, know what are the properties by which the minimum norm solution and the main thing you want to remember, for the minimum norm solution, you must lie in the row space. That's it. Once you know that you're like, there's only one solution that lies in the row space and satisfies the constraint. And that's the solution. And rest is algebra, but we can work it up to any questions. Yeah. More than one solution? No, I think there will be only one solution. So what is this solution reminiscent off, we went through a minimum norm setting, right? What is it you have learned again and again and again in your previous semesters? In particular, yeah, yeah, least-squares. This should remind you of least squares, except it's least-squares turned on its head. Why in least-squares? See is a tall and skinny matrix. Here is a short and fat matrix. It's just a transpose. So the minimum norm is kinda quote unquote, the transpose of the least-squares solution. But this is the math for it. And don't worry about knowing the understanding are following the actual notation. As long as you follow what properties it should obey, this is a solution. So let's do, let's go back to our example. We saw the eyeball it and said 11. You already had a hint that 11, and you can intuitively say it's 11. But if you just plugged for c equals 11 and d equals two, that was a special toy example. U-star is going to be C transpose times C. C transpose inverse times d. D is just to see, is 11 row vector, c transpose as 11 column vector, you work it out. You get 11. Yeah, good. Kind of verification or validation that the formula is correct. It's not a proof. If you didn't get 11, then you know, you did something wrong or if techno fix. Okay, So just as a note, if you contrast this with the least-squares setting which we just discussed in a minute, a minute ago. We know that the least-squares a C transpose C inverse times a transpose d, whereas this is C transpose C, C transpose inverse d. One is minimum norm and the other is least-squares. Now, as a heads up of what's coming, we will do it next time, next week actually, I don t think I'll get to it this week. That once we learn the SVD, we will see that there's one solution fits all. That's what's beautiful about the SVD. So we know that C u equals d is what we want to solve. If you had a square system, C was a square matrix and it was invertible, then you would say that u equals c inverse times d, right? When everything is square, nice. But C is not square. In this example, it's, it's short and wide, or short and fat. And in the least squares case it's tall and thin. It's not square. So it turns out that you can define what is known as a pseudo inverse of C, which is denoted a C with a cross on the top. And this is called the pseudo-inverse a C. And both the minimum norm solution and the least square solution will be the pseudoinverse of c times d, where pseudo-inverse and so on will be defined when we get there. Okay? So stay ahead to say that I don't need to remember these things because I know the SVD. That's another reason to not learn, not to know all the details of the other. You should know that the concept of the proof, but you shouldn't, should never memorize all of these CSI transferable C transpose C. I don't remember that. Okay. So if there are no questions, let's embark on our journey to towards the SVD. So the journey starts at 10:20. Okay, let's see how long it goes, but it's really the crown jewel of linear algebra. So you should pay attention to the SVD and it takes, it takes some effort to learn. And so you may have to do it a few times. So we will also try and repeat the concept again and again as much as possible. Go to discussion and do the homeworks. So it would be useful. Okay, let's do a brief recap of what we did last time. Last class. We studied symmetric matrices Hess. And we said that symmetric matrix S can always be diagonalized. And the big picture, but if I remember, is that when you have a symmetric matrix and you know that any matrix can be upper triangular raised, right? So a symmetric matrix is equal to the symmetric matrix transpose. So that means it's upper triangular and lower triangular are the same, which means it has to be diagonal, right? So that's what I remember from what we did last time. Not that you should go back and refresh yourself that any symmetric matrix S can always be diagonalized. That's point number one. Point number two. Is that the diagonalizing matrix D, the diagonalize this S consists of the eigenvectors of S, right? And they are also orthogonal eigenvectors of S. Orthogonal, it's not always the case that the eigenvectors are orthogonal. In fact, it's very rare and it's always the case when asymmetric the eigenvectors are orthogonal. And not only that, the eigenvalues, the eigenvalues are also real valued. And we proved that using complex conjugates and so on, right? But I think you have a proof in your homework that you have to do it by induction. The TAs wanted to give you some practice in induction. So they want to talk to you. So you can do it by induction too. But as part of your homework, actually this homework, problem one. So another way of saying this is v transpose S V equals lambda. This is exactly when S is, S is for symmetric. And when lambda is diagonal with the DAG, with the eigenvalues on the diagonal. So if you have only are non-zeros and the rest are zeros, you will have a picture like shown here. So in general, this is just to make the point that S, If you write V transpose S V S lambda S, the symmetric matrix as V Lambda V transpose. Another way of just take v over to the other side. If seen this many times, S v is equal to v, multiplying both sides by V. On the right, we transpose times v is high. So you have v lambda on the right and F times d on the left. So S V equals V Lambda V. If the eigenvectors, which are shown in blue and pink, these are the eigenvectors vectors corresponding to the non-zero eigenvalues. And these are the eigenvectors corresponding to the zero eigenvalues. So you can see that v1 through VR times lambda one through lambda r is exactly V lambda. Now, we know that the, these correspond to the zero eigenvalues. So S times Vr plus one, It's gonna be zero. Half times Vr plus two is going to be zero and so on. Those are these guys. And the language that we have just learned. To remind you that we have reminded you. So these Vr plus 13n form a basis for the null space of S, right? Because the here is S. The nullspace is everything that takes us to zero. And indeed Vr plus one plus two and v n all take S to zero because these are the eigenvectors corresponding to the zero eigenvalue. So these must be spanning there a basis for the null space of S. That's another way of saying it. Okay? So these are concepts I want you to build on because you'll need them to follow the SVD treatment more carefully. Any questions? This is all review of stuff we did last time. Okay. There a question there? No. Okay. Okay. It's time to warm up for the SVC. You can see that I'm easing into the SVD. They're doing stretching exercises. Touch your toes because SVD is a big thing. So we need to be ready for it. You can't just go print a minor, the first jog and then there's the warm-up. Okay? So this is a review of the stuff we have done. So V inverse a V equals lambda for square. So this is always true for square matrices. So this is also true for square matrices. So this is the Eigen decomposition. So how many ways have we all have we studied? Let's take a recall of stuff we have done in 16.16 be in particular. So for any square matrix we saw that, let me mark that. For any square matrix a, we have shown that it can be diagonalized using the eigenbasis. So a equals v sigma inverse is one decomposition of a. So when a is square, it's really nice, are easy to decompose it into three matrices, which are B lambda and B inverse, a inverse, where lambda is diagonal and V and V inverse V, V contains the eigenvector eigenbasis. And in general, the eigenbasis is not orthogonal. So this is not an orthogonal decomposition, but it is a diagonal decomposition for diagonalize this. On the other hand, for any matrix, any square matrix a, we showed that it can be upper triangular list. That means it can be written as u transpose u times t times u transpose u transpose a, u equals detail the same thing, where d is no upper triangular. What do we know about the US for an upper triangular basis? We studied that these are orthogonal as beautiful. So what's good, what's bad? What's good is that I have orthogonality or bad, I lost diagonalization because t is no upper triangle, right? Except for symmetric. So keep remembering symmetric. That's the pathway to SVD. Okay? Keep remembering symmetric is the best of both worlds. Keeps a symmetric is the best of both worlds. Chanted a few times. Because once you know that you'll see how the FED comfortable. Alright, so now it's a square matrix. Okay? This now back to the symmetric when S is not only scrapped, but also symmetric, then S is equal to X transpose. This is the best of both worlds, right? Not only do you have an Eigen decomposition that diagonalized as S, But the, the eigenvectors are orthogonal for symmetric. Okay. Give me a show of hands. Alright, great. Now we want more. What is a good decomposition for a general? It's time to graduate. You're done Latin squares. We want to remove the go away from the square world. What is a good decomposition for a general non-square matrix? Well, what is our wish list? What do we like? You like? Tell me more or to at least give me two features you'd like about decomposing a matrix. As we have studied, what would you like to the basis, the change of basis to be? Yeah. I remember eigenvalues hold for only square matrices. So be very careful about the kind of visual eigenvectors anymore. Why? Because eigenvectors and eigenvalues only go with square matrices. Now we are in non-square land. He's fat. He doesn't have any eigenvalues. Fat people don't have eigenvalues, only scraped the blue. Tall people don't have eigenvalues. Only Square people do. So. You can't have eigenvectors and eigenvalues on your wishlist. That's a pipe dream. That's not gonna happen. Yeah. Yes. You want orthonormality that was very convenient. These are all the good things that come from the orthonormality. What else would be nice? I don't want our upper triangular. Upper triangular if it's too much work, right? You have all the upper right quadrant or triangle is all populated the nonzeros. I don't want that. I want diagonal. So I one diagonal and I want orthonormality. But I also don't want symmetry. Because symmetry is associated with square matrices. Something's gotta give. You had a question. Symmetric middle. What does that mean? Symmetric in the middle. What does that mean? I don't know what symmetric in the middle. I don't know. I don t think maybe I'm not getting it, but that's not what I was looking for. I think we're done. I wish list is I need to be diagonal. Diagonal as possible. Maybe that's what you mean. Because if I have, if I have a rectangular matrix, what does the diagonal? The diagonal is just the first few rows and the rest have to be zero. So I mean, when I say diagonal, diagonal in quotes, diagonal appropriate for the shape, shapes and sizes will determine what's diagonal. But I want to retain diagonalization in quotes, and I want to retain orthonormality. It turns out you can have both. Wow, I taught he said only symmetric matrices can do it. What's the catch? Anyone want to guess? I am claiming you can have orthonormality and you can have diagonalization in the manner that as befitting the shape, something's got to give, right? Because we are moving away from the square matrix land. And we're claiming that this is possible. Okay? So we love orthonormal basis. We love diagonalization. But you don't want to rely on special things like symmetry or even square. Forget about symmetry. Even squares is too much to ask. Most matrices are not square. What do we do? Anyone? Just pick guess. Yeah. Yeah, you're kind of getting that we clearly need. Okay. Hint should be, no matter what we do, it, it has a strong connection with symmetric matrices, right? Clearly because it's symmetric matrices have what we want. We're not having a symmetric, maybe you have some rectangular thing. Somehow we should try hard to convert it into a symmetric form. That's the hint, right? But, but that's how to do it. But I'm asking about the wish list. I think you are kinda hinting at it. So basically, eigenvectors and eigenvalues of a square matrix can be generalized with rectangular matrices. While insisting we have orthonormal basis. And the key solution, instead, we have to give up having one orthonormal basis. We need to. Okay. That's the answer I was looking for. Now, in all the systems, there was one basis. There was an orthonormal basis, if you're lucky. Now, you're going to have two basis. But each is orthonormal in their respective space. And that's the key. That's the breakaway from the square land, where we now are going to have two orthonormal basis in order to help with our decomposition. So that's the big picture you want to keep in mind. So one for the column space and one for the row space. In the symmetric case, they are the same matrix because the column space is the row space. But in a rectangular case, that won't be the case. So column space, we'll have one in the row space will happen. Okay? So we started with square matrices AB. So this is my eigenvector equation, Av equals lambda IVI. Now lambda IVI or an eigenvalue eigenvector pair for a. We can't do that because we don't have eigenvectors anymore. This matrix is not squared. So we're gonna give up and say, I'm going to, my wish list is, I'm going to ask for a times d Phi to be not a scaled version of v, but a scaled version of UI. The UI is some other vector. So the hope is the v's are orthonormal among themselves and they use are orthonormal amongst themselves. And that's the best we can. And that's the start of the SVD. Okay? So you always start with the conceptual. Now, the SVD can be, I can point to you that decomposition and prove to you it works, but that'll be devoid of feeling. You got to feel your way through it. I don't know why this is how did we get this right? So that's my hope to instill that in you. So we now have two orthonormal basis for the column space columns. This is an m by n matrix, right? So let's remember that. So if you look at, let's do another color. This is beautiful. I can actually write, I can actually draw straight lines with this tool. So this is m, this is n, this is my matrix a. So the UI is what is the column space of a? The columns are going this way and they're living in RAM. So the column space of a lived in RM, and the row space lives in RN, the other dimension. So UIs are the column space Eigen basis. I shouldn't say eigenbasis know Eigen anymore. Slip of time. Okay, UIs are an orthonormal basis for R, n and b. I's are an orthonormal basis for R n. Okay? And so let's write this in matrix form. So a times V1 equals u1 times sigma one, right? So let's, so he times v1 is sigma one times one. So this is the equation that we are after, the ABI equals sigma IUIE. And we're going to show that it can be done. Okay, so that, that will pick more than this lecture today. We'll continue it next week, next class. So ABI equals UI sigma i, V2 equals sigma two u2, right? When you multiply a matrix with a diagonal on the right, just these diagonal entries weight the columns of the u, right? Does a matrix vector, matrix-matrix multiplication. And so we get V1 equals one sigma 1.1. And there is a V n here. So let me, let's get to you in a minute. So remember, a is m by n, so it can have maximum rank m. M is smaller than hemisphere smaller dimension. So V1 to VM mapped to u1 through, UM, so this is an m by n matrix times an n by n matrix. So just by pure numbers, the first dimension has to be m since this whole thing is m by n. So you have to be m by m and sigma is m by n. And this is the fence in which I'm calling still, this is still a diagonal. What do you, what do you expect diagonal when you have rectangular is that you will populate. You will go as far as you can go. And when you run out of room, you'd give up. Everything else is zeros. So there's a similar story. When you're tall, you come down the diagonal and then you run out of room, so your zeros, so I'm still going to call this a diagonal. Yeah, There was a question. No, no, we're not connecting. They are we are saying that each, the column space and the row space need their own individual representation. And each, each of those spaces is gonna be spanned by an orthonormal basis. And those orthonormal basis of the youth and the VCE. Okay, you need two separate bases, one for the columns and one for the rows. That's the way to get around our dilemma of how do I maintain orthonormality and getting diagonalization? Yeah. Yeah, we have M rows. Yes. Oh, hold on. When we get to that, you're jumping the gun. I told you I need to warm up. This is a slow and there's a slow and hopefully exciting story. Because we don't want to play Max too early. You want to go slow. Alright? Here, m by n. Any other questions? Okay? So if you write like this, you can see that a times V equals U times sigma, right? I'm not proven anything yet. I'm just giving you the layout of the land and saying, We started with a wishlist and they say, wow, it would be good if we could do this. Well, we have to have one orthonormal, basically to, when we say it's too, this is a story. So a times v is u times sigma. Now, this is called the singular value decomposition. When you write, take the V over to the right-hand side. So we saw how a times v, E times V equals U times sigma. Sigma is diagonal and u is orthonormal. So each of these, this is an n by n orthonormal basis. This is an N by M orthonormal basis. This is diagonal and a is rectangular. So take v to the right-hand side. You'll have here equals U sigma V transpose because V transpose and B inverse are the same because there's an orthonormal basis. And in this example, rank of a is m. The m is a smaller dimension. Okay? I'm just concretely giving you an example when you have a rectangular matrix that are short and wide. This is the SVD or singular value decomposition. And here's a picture of this that will be more. Expanding on that a little more. You can see that Let's take the concrete case where a is ten by 100. So a is ten by 100. So that is going to be, since this is ten by 100, you have to start. You always have orthonormality in the, in the decomposition. So let's write it here that I'm going to start using the board. Okay? So the diagonal that is in courts based on the ship get at all. I mean, so this is the SVD and which I've hopefully shown here. You switch back. Yeah, thank you. So you can see here that this is an orthonormal matrix which is ten by ten because the number of rolls of a is ten. And then here's my diagonal stuff with the sigmas. And this is 100 by hundred orthonormal matrix D, but it's the transpose form. But meaning these are the orthonormal guys to the rows of B then are lying down rather than standing upright. Rows are orthogonal and the blues are orthogonal, orthonormal. And the u's and v's are two separate bases. They have nothing to do with each other. In fact, they don't even find the same space because one has four rows and four columns. These are called the singular values. At the singular value decomposition comes from the columns of U, the orthonormal basis for R ten in this example, are known as the left singular vectors because they are on the left. And b are called the right singular vectors. So this is the nomenclature for the SVD. And here's a picture. We were kind of ignoring the other fitting of the rectangular, which is tall and thin. And this is a tolerant and this is the model's ace. And now you isn't the fat guy or the big square guy. And v If the tiny guy, because the dimensions are swapped. So this is 100 by hundred orthonormal matrix Sigma, or this should be, should have. So sigma is now going to run along the diagonals, like so. And then it ends in zeros because we ran out of room. And I'm V transpose, the green vectors are also orthogonal, orthogonal. So I'm just setting the stage for U Sigma V transpose. Such a decomposition is always possible for any matrix will give me, okay? So that's the power of the SVD. You give me any, that's like we said in the symmetric, in the upper triangular case, was kinda the best thing you could hope for given you have a square matrix, right? Any square matrix can be decomposed into an upper triangular form. I was great. We thought that was great. Way to see this. This says, give me any matrix gray or otherwise. And I can always decompose it into orthonormal times diagonal times orthonormal. Okay? So this is cool. And this is, I'm just saying the same thing. This is sometimes known as the full SVD because I am looking at the entire square matrices for u and v, we will see that we can truncate them to get rid of the extraneous zeros. Zeros floating around. You don't want to compute them. This is the conceptual story, okay? This is not the way you would compute the SVD. This is the way you would understand the SVD. Once you understand it, you can compute using a tricks. You are doing change of basis, you're doing change of two basis. So there are two bases involved. I'm saying that there's no one basis that can, you can change a matrix two if it is rectangular, you need to think of two basis. Okay, so that's the key. Just remember 12 instead of one. But all the other stuff still remains. But I don't know if that answers your question. Yeah. Okay. So, right, singular vectors. Singular vectors. Okay, let's do an example. I want to show you. I was bragging that default for any matrix. So let's put it to the test and we'll start out with a square matrix. A square matrices can also be a 3D because a square is a special case of rectangular. So here's an example for 4433. Here if the decomposition, so here's a decomposition where this is the orthonormal basis for the column space. So what are the columns? The columns are for 3.43. By the way, you can see here, this is what is the rank of this matrix without even looking at anything. One, because second column is the same as the first column. So this is not fully independent, is not independent. So this is a rank one. And that is manifested in the fact that there's one singular value and this other singular value is zero. So now what we're going to talk about singular values, even though there is also an Eigen decomposition for the square case, it's the same, but we cannot talk about eigenvalues anymore. These are called singular values. There's only one singular value, which means the rank is one. In general, the number of singular values is the rank of that matrix. Okay? Now, you can see here that the, you are the columns, the U1. So in this example here, you can see here, what did we do? Okay? So in our U Sigma V transpose on the board, we have done the u basis or four-fifths and three-fifths times three-fifths and minus four-fifths. Verify that they are orthonormal. Well, four-fifths and three-fifths, four-fifths squared plus three-fifths squared is indeed one. That's good. And indeed, if I do the dot product between U1 and U2, I will get zero. That's four times three plus three times minus four will give you zero. So U1 and U2 are indeed an orthonormal basis pair. These are the eigenvalues singular values that are associated with hair. And this is the orthonormal basis that spans the row space. What is the row space? You can see that the row space or multiples of 1,144.3, three-year-old scales of each other, right? And indeed, 11.1 minus one are the complete orthonormal basis for the row space of g of eta. So you showed here has a basis or column space and V for the row space. And you should be able to, if you do this multiplication, verify for yourself. If you multiply out U Sigma V transpose U, indeed get a. Well, sounds like magic. How did you get it? Well, that's what we will do next class. I'll show you the construction one-by-one, step-by-step. Kind of done a lot of it today already. So hopefully you'll be in good shape. Here's another example, because the whole point was to do it for rectangular matrices. Why you're giving me a square matrix? That's kinda lame, right? Yeah, indeed it is slim. So let's do a non-linear example. So let's do 124-52-4810 For two-by-four matrix. That says have an SVD. Yes, every square root heavy rectangular matrix as an SVD. And if you work it out, you will get this. So what is this? Well, this is my u, so now it's two-by-four. So U1 and U2. And you should be able to verify that indeed they are orthonormal. On the other side, you have, you have four vectors spanning the row space. Because it's two-by-four. Write the rules are for an R4 and the four orthonormal basis vectors for our four according to my SVD are given by these four. And you should be able to verify that. The norm is one who take the sum of the squares of each entry of the roles, it's one and they are pairwise orthogonal. Let's check the first two. Yeah, one times minus five, and then 00 plus five times minus one, all with the same denominator. So maybe I got something. No, I think he's correct. You'll get zero. Again. Every pair is still zero. So it's almost like magic, right? How the hell did they come up with this set, set your Plunkett and I wanted to first show you the example to motivate you to study it, right? If I first gave you the proof is it'll be falling asleep. And here's some examples, but if I gave you the examples to show the power than you might be, oh, let me, I'll be motivated to learn how the hell it works. Alright, so how did we get this decomposition? The key insight is, okay, Now, let's cut. It's nighttime to unravel the hell we get there. And again, I've been throwing hints all around that our wishlist involved trying to emulate what symmetric matrices, right? Symmetric matrix has all the good properties we want. So question for you. How do I convert my he, like given here and E such as given here into a first, how do I convert it into a square form? What operator, what operation can I do on a that will make it from rectangular to square? Yeah. Exactly. Multiplied by its transpose, or a transpose times a squared. A times a transpose is also squared. So if I, if I were to do a transpose a, that gives me a square matrix. And not only is it square, it's also symmetric, which we'll look at that now. Okay? So from a transpose a, Indeed, you're going to get a square matrix. A transpose ten by 100. He is ten, sorry, a is ten by 100, a transpose is 100 by ten. A transpose a is 100, 100. Now let's consider a transpose a, and let's assume that we know how to do the SVD. We haven't shown it yet, but this is also a mental prep. So if a is U Sigma V transpose right here is U Sigma V transpose. So what is a transpose? A? It's nu sigma v transpose transpose times U Sigma V transpose. Complicated late I have six matrices now. But v transpose transpose is v transpose means to go in the reverse order. So sigma transpose times U transpose times U Sigma V transpose, right? There's exactly a transpose a. U transpose u simplifies because u is orthonormal. So U transpose U is I gone? Sigma is, I shouldn't say sigma squared, I should say diagonal. Yeah. I don't want to write sigma transpose times Sigma. He's going to be Sigma squared because this is a square matrix. So sigma and sigma transpose are the same. Sigma is diagonal. So you see that what did I do? A transpose a tells you everything about V, the right singular vectors. So it is deeply connected. So notice you is gone. When I do a transpose a, there's no use anymore. You've got canceled out. When I do a V because they said muddy. A similar story happens. When you do a transpose. Now the bees will be gone. So aa transpose of U Sigma squared UT. Yeah, Question. Yes, good question. So she's asking if I do AA transpose versus a transpose a, that they both give you the same singular values. Our, our derivation shows the answer is yes, because the singular values, in this case, AA transpose is actually square. So you have eigenvalues also, because they're the same thing for square matrices. Singular values degenerate eigenvalues, right? Now, it turns out that they will have the same eigenvalues. It's just the size will be different, but the zeros will be different. In other words, if you have an m by n, If m is smaller than n, then if you do use sigma, if you do a transpose, then this is a smaller matrix. So this Sigma squared, sigma squared is bigger, but the eigenvalues, the first m eigenvalues are the same. Yes, they repaired by zeros because you have to pad, you have to account for the size difference, right? Because u and v are not in general the same size. But so this is exactly what the show's. So you see that the eigenvalues of Sigma, rather the eigenvalues of a transpose a are actually given by Sigma squared. The eigenvalues of a transpose are also given by sigma squared, then the non-zero. But these two sizes are not the same. I mean, they're getting a little ahead of ourselves. So if we can hold on, we will get to it. Yeah. Sigma squared is square minus a square matrix because a transpose a is square. This example. But I mean, it's easy. In fact, if you don't, if you just look at this part, sigma transpose sigma is what we're actually after. Maybe I shouldn't even have written sigma squared. Yeah. V here. Yeah, yeah, it should be transpose. Yeah, you're right. Let's put it in. It's always the somethings, something transpose. Okay, so simple. This suggests the hint that the key to understanding the SVD of a equals U sigma V transpose. What I've written on the board is to study the square matrix a transpose a. That seems to hold the key to unlocking the mystery of the SVD. Okay, so that's what we want to look at. Alright, so back to the control land. So I'm gonna be using a and C interchangeably for the final. I'll be using C predominantly for the rest of our treatment. Because remember we started out with C times V equals d or something, right? The first thing that we started with C was the sort of rectangular matrix you are interested in. So I'm gonna go back to C language, but you should keep in mind that linear algebra is, but control land is our seats. But hopefully that doesn't confuse you. All right? First fact that C transpose C is symmetric. It's beautiful. Not only is it square, but C transpose C is also symmetric. The proof is by one line, C transpose C transpose, C transpose C. Done. The easiest proof. Okay? Now there is another property which is more useful and which is why we can get the singular values. And singular values are always non-negative, the non-zero ones, so are always positive. The eigenvalues of S equals C transpose C are always real and non-negative. So either they are positive or zero. This is a nice property. Why is this? Well, how much time do I have? I think we can go through, I'll run through this proof and then we'll do it again next time. So we know that S v equals Lambda v. Since Lambda v or Eigenvalue Eigenvector pair for the symmetric matrix S equals C transpose C. We also know that. I think, yeah, let me, let me, let me stop there. Actually, it's a little more involved. I start next time with showing that the eigenvalues of S are always real and non-negative. Okay? Nima, you want your thing? Singular? Alright. Today's password a singular. Yeah. Let me let me show you the page. Yeah, it's right there. 